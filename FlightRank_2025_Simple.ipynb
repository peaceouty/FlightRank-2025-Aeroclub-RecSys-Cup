{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d4d5a0",
   "metadata": {},
   "source": [
    "# FlightRank 2025 - ç²¾ç®€ç‰ˆè§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "å•†åŠ¡æ—…è¡Œè€…ä¸ªæ€§åŒ–èˆªç­æ¨èç³»ç»Ÿ - ç›´æ¥è¾“å‡ºsubmissionæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877817d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒè®¾ç½®å’Œåº“å¯¼å…¥\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# TPUæ”¯æŒ\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    HAS_TPU = True\n",
    "    print(f\"âœ… TPUå·²è¿æ¥ï¼Œå‰¯æœ¬æ•°: {strategy.num_replicas_in_sync}\")\n",
    "except:\n",
    "    HAS_TPU = False\n",
    "    strategy = None\n",
    "    print(\"âš ï¸ TPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU/GPU\")\n",
    "\n",
    "# ç¯å¢ƒæ£€æµ‹\n",
    "IN_KAGGLE = '/kaggle/' in os.getcwd()\n",
    "DATA_PATH = '/kaggle/input/aeroclub-recsys-2025/' if IN_KAGGLE else './'\n",
    "OUTPUT_PATH = '/kaggle/working/' if IN_KAGGLE else './'\n",
    "\n",
    "print(f\"ğŸŒ è¿è¡Œç¯å¢ƒ: {'Kaggle' if IN_KAGGLE else 'æœ¬åœ°'}\")\n",
    "print(f\"ğŸ“ æ•°æ®è·¯å¾„: {DATA_PATH}\")\n",
    "print(f\"ğŸ“ è¾“å‡ºè·¯å¾„: {OUTPUT_PATH}\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åŠ è½½\n",
    "def load_data():\n",
    "    try:\n",
    "        train_df = pd.read_parquet(f'{DATA_PATH}train.parquet')\n",
    "        test_df = pd.read_parquet(f'{DATA_PATH}test.parquet')\n",
    "        sample_submission = pd.read_parquet(f'{DATA_PATH}sample_submission.parquet')\n",
    "        print(f\"è®­ç»ƒæ•°æ®: {train_df.shape}, æµ‹è¯•æ•°æ®: {test_df.shape}\")\n",
    "        return train_df, test_df, sample_submission\n",
    "    except:\n",
    "        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®\n",
    "        print(\"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®\")\n",
    "        np.random.seed(42)\n",
    "        n_sessions = 50\n",
    "        data = []\n",
    "        flight_id = 1\n",
    "        \n",
    "        for session in range(1, n_sessions + 1):\n",
    "            n_flights = np.random.randint(5, 15)\n",
    "            selected_idx = np.random.randint(0, n_flights)\n",
    "            \n",
    "            for i in range(n_flights):\n",
    "                data.append({\n",
    "                    'Id': flight_id,\n",
    "                    'ranker_id': session,\n",
    "                    'totalPrice': np.random.uniform(200, 1500),\n",
    "                    'total_flight_duration': np.random.uniform(60, 600),\n",
    "                    'airline': np.random.choice(['DL', 'UA', 'AA']),\n",
    "                    'selected': 1 if i == selected_idx else 0\n",
    "                })\n",
    "                flight_id += 1\n",
    "        \n",
    "        train_df = pd.DataFrame(data[:int(len(data)*0.75)])\n",
    "        test_data = []\n",
    "        for session in range(n_sessions + 1, n_sessions + 21):\n",
    "            n_flights = np.random.randint(5, 15)\n",
    "            for i in range(n_flights):\n",
    "                test_data.append({\n",
    "                    'Id': flight_id,\n",
    "                    'ranker_id': session,\n",
    "                    'totalPrice': np.random.uniform(200, 1500),\n",
    "                    'total_flight_duration': np.random.uniform(60, 600),\n",
    "                    'airline': np.random.choice(['DL', 'UA', 'AA'])\n",
    "                })\n",
    "                flight_id += 1\n",
    "        \n",
    "        test_df = pd.DataFrame(test_data)\n",
    "        sample_submission = pd.DataFrame({\n",
    "            'Id': test_df['Id'],\n",
    "            'rank': range(1, len(test_df) + 1)\n",
    "        })\n",
    "        \n",
    "        return train_df, test_df, sample_submission\n",
    "\n",
    "train_df, test_df, sample_submission = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾å·¥ç¨‹\n",
    "def create_features(df, is_train=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ç±»åˆ«ç¼–ç \n",
    "    if 'airline' in df.columns:\n",
    "        df['airline_encoded'] = pd.Categorical(df['airline']).codes\n",
    "        df = df.drop('airline', axis=1)\n",
    "    \n",
    "    # æ•°å€¼ç‰¹å¾\n",
    "    numeric_cols = ['totalPrice', 'total_flight_duration']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # ç»„å†…æ’å\n",
    "            df[f'{col}_rank'] = df.groupby('ranker_id')[col].rank().fillna(1).astype(int)\n",
    "            # ä¸æœ€å°å€¼çš„å·®å¼‚\n",
    "            df[f'{col}_diff'] = (df[col] - df.groupby('ranker_id')[col].transform('min')).fillna(0)\n",
    "    \n",
    "    # ä¼šè¯ç‰¹å¾\n",
    "    df['session_size'] = df.groupby('ranker_id')['Id'].transform('count')\n",
    "    df['position'] = df.groupby('ranker_id').cumcount() + 1\n",
    "    \n",
    "    # å¡«å……ç¼ºå¤±å€¼\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# å¤„ç†æ•°æ®\n",
    "train_features = create_features(train_df, is_train=True)\n",
    "test_features = create_features(test_df, is_train=False)\n",
    "\n",
    "print(f\"è®­ç»ƒç‰¹å¾: {train_features.shape}, æµ‹è¯•ç‰¹å¾: {test_features.shape}\")\n",
    "\n",
    "# æ¸…ç†å†…å­˜\n",
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPUç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "def create_tpu_model(input_dim):\n",
    "    if not HAS_TPU:\n",
    "        return None\n",
    "        \n",
    "    def model_fn():\n",
    "        inputs = tf.keras.Input(shape=(input_dim,))\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    with strategy.scope():\n",
    "        return model_fn()\n",
    "\n",
    "# æ¨¡å‹è®­ç»ƒ\n",
    "def train_model(train_features):\n",
    "    print(\"å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    feature_cols = [col for col in train_features.columns if col not in ['Id', 'ranker_id', 'selected']]\n",
    "    X = train_features[feature_cols].fillna(0)\n",
    "    y = train_features['selected']\n",
    "    \n",
    "    print(f\"ç‰¹å¾æ•°é‡: {len(feature_cols)}, æ ·æœ¬æ•°é‡: {len(X)}\")\n",
    "    print(f\"æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.4f}\")\n",
    "    \n",
    "    # æ•°æ®åˆ†å‰²\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    model_type = 'rf'\n",
    "    \n",
    "    # é¦–å…ˆå°è¯•Random Forestï¼ˆæ›´ç¨³å®šçš„åŸºçº¿ï¼‰\n",
    "    print(\"è®­ç»ƒRandom Forest...\")\n",
    "    try:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,  # å¢åŠ æ ‘çš„æ•°é‡\n",
    "            max_depth=15,      # å¢åŠ æ·±åº¦\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=SEED, \n",
    "            n_jobs=1           # Kaggleç¯å¢ƒä¸‹ä½¿ç”¨å•çº¿ç¨‹æ›´ç¨³å®š\n",
    "        )\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_val)\n",
    "        \n",
    "        # æ”¹è¿›è¯„ä¼°æ–¹æ³•ï¼šä½¿ç”¨AUC-likeè¯„ä¼°\n",
    "        try:\n",
    "            rf_score = roc_auc_score(y_val, rf_pred)\n",
    "        except:\n",
    "            # å¦‚æœAUCè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é˜ˆå€¼æ–¹æ³•\n",
    "            threshold = np.percentile(rf_pred, 80)  # å–80%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼\n",
    "            rf_score = np.mean((rf_pred > threshold) == y_val)\n",
    "        \n",
    "        best_model = rf_model\n",
    "        best_score = rf_score\n",
    "        model_type = 'rf'\n",
    "        print(f\"Random Forestå¾—åˆ†: {rf_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Random Forestè®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "        # åˆ›å»ºä¸€ä¸ªç®€å•çš„åŸºçº¿æ¨¡å‹\n",
    "        print(\"åˆ›å»ºåŸºçº¿æ¨¡å‹...\")\n",
    "        class SimpleModel:\n",
    "            def __init__(self):\n",
    "                self.mean_score = 0.5\n",
    "            def fit(self, X, y):\n",
    "                self.mean_score = y.mean()\n",
    "            def predict(self, X):\n",
    "                return np.full(len(X), self.mean_score)\n",
    "        \n",
    "        best_model = SimpleModel()\n",
    "        best_model.fit(X_train, y_train)\n",
    "        best_score = 0.5\n",
    "        model_type = 'baseline'\n",
    "    \n",
    "    # å¦‚æœæœ‰TPUï¼Œå°è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹\n",
    "    if HAS_TPU and best_score < 0.8:  # åªæœ‰å½“RFæ•ˆæœä¸å¥½æ—¶æ‰ç”¨TPU\n",
    "        try:\n",
    "            print(\"è®­ç»ƒTPUç¥ç»ç½‘ç»œ...\")\n",
    "            tpu_model = create_tpu_model(X_train.shape[1])\n",
    "            \n",
    "            if tpu_model is not None:\n",
    "                train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                    X_train.values.astype(np.float32), \n",
    "                    y_train.values.astype(np.float32)\n",
    "                )).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "                \n",
    "                val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                    X_val.values.astype(np.float32), \n",
    "                    y_val.values.astype(np.float32)\n",
    "                )).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "                \n",
    "                tpu_model.fit(train_dataset, validation_data=val_dataset, epochs=20, verbose=1)\n",
    "                \n",
    "                val_pred = tpu_model.predict(val_dataset)\n",
    "                try:\n",
    "                    tpu_score = roc_auc_score(y_val, val_pred.flatten())\n",
    "                except:\n",
    "                    tpu_score = np.mean(val_pred.flatten() > 0.5)\n",
    "                \n",
    "                if tpu_score > best_score:\n",
    "                    best_model = tpu_model\n",
    "                    best_score = tpu_score\n",
    "                    model_type = 'tpu'\n",
    "                    print(f\"TPUæ¨¡å‹å¾—åˆ†: {tpu_score:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"TPUè®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    print(f\"æœ€ä½³æ¨¡å‹: {model_type}, å¾—åˆ†: {best_score:.4f}\")\n",
    "    \n",
    "    # ç¡®ä¿è¿”å›çš„æ¨¡å‹ä¸ä¸ºNone\n",
    "    if best_model is None:\n",
    "        print(\"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œåˆ›å»ºéšæœºåŸºçº¿...\")\n",
    "        class RandomModel:\n",
    "            def predict(self, X):\n",
    "                return np.random.random(len(X))\n",
    "        best_model = RandomModel()\n",
    "        model_type = 'random'\n",
    "    \n",
    "    return best_model, model_type\n",
    "\n",
    "model, model_type = train_model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790adf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„æµ‹å’Œç”Ÿæˆsubmission\n",
    "def predict_and_submit(model, model_type, test_features, sample_submission):\n",
    "    print(f\"å¼€å§‹é¢„æµ‹ï¼Œä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºNone\n",
    "    if model is None:\n",
    "        print(\"âŒ æ¨¡å‹ä¸ºNoneï¼Œåˆ›å»ºéšæœºé¢„æµ‹...\")\n",
    "        predictions = np.random.random(len(test_features))\n",
    "    else:\n",
    "        # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "        feature_cols = [col for col in test_features.columns if col not in ['Id', 'ranker_id']]\n",
    "        X_test = test_features[feature_cols].fillna(0)\n",
    "        \n",
    "        print(f\"æµ‹è¯•ç‰¹å¾æ•°é‡: {len(feature_cols)}, æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}\")\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        try:\n",
    "            if model_type == 'tpu' and HAS_TPU:\n",
    "                print(\"ä½¿ç”¨TPUæ¨¡å‹é¢„æµ‹...\")\n",
    "                test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                    X_test.values.astype(np.float32)\n",
    "                ).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "                predictions = model.predict(test_dataset).flatten()\n",
    "            else:\n",
    "                print(\"ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹...\")\n",
    "                predictions = model.predict(X_test)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é¢„æµ‹å¤±è´¥: {str(e)}\")\n",
    "            print(\"ä½¿ç”¨éšæœºé¢„æµ‹ä½œä¸ºå¤‡é€‰...\")\n",
    "            predictions = np.random.random(len(X_test))\n",
    "    \n",
    "    # ç¡®ä¿predictionsæ˜¯numpyæ•°ç»„\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    \n",
    "    print(f\"é¢„æµ‹å€¼èŒƒå›´: {predictions.min():.4f} - {predictions.max():.4f}\")\n",
    "    \n",
    "    # ç”Ÿæˆæ’å\n",
    "    result_df = test_features[['Id', 'ranker_id']].copy()\n",
    "    result_df['score'] = predictions\n",
    "    \n",
    "    # æŒ‰ç»„æ’åï¼ˆåˆ†æ•°è¶Šé«˜æ’åè¶Šå‰ï¼‰\n",
    "    result_df['rank'] = result_df.groupby('ranker_id')['score'].rank(method='dense', ascending=False)\n",
    "    \n",
    "    # åˆ›å»ºæäº¤æ–‡ä»¶\n",
    "    submission = result_df[['Id', 'rank']].copy()\n",
    "    submission['rank'] = submission['rank'].astype(int)\n",
    "    \n",
    "    # éªŒè¯æ’åçš„åˆç†æ€§\n",
    "    rank_stats = submission.groupby('rank').size()\n",
    "    print(f\"æ’ååˆ†å¸ƒ: {dict(rank_stats.head())}\")\n",
    "    \n",
    "    # ä¿å­˜æ–‡ä»¶\n",
    "    submission_file = os.path.join(OUTPUT_PATH, 'submission.csv')\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    \n",
    "    print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_file}\")\n",
    "    print(f\"ğŸ“Š æäº¤æ–‡ä»¶å½¢çŠ¶: {submission.shape}\")\n",
    "    print(f\"ğŸ¯ å¤„ç†äº† {result_df['ranker_id'].nunique()} ä¸ªä¼šè¯\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶\n",
    "print(\"=\" * 50)\n",
    "print(\"å¼€å§‹ç”Ÿæˆæäº¤æ–‡ä»¶...\")\n",
    "\n",
    "submission = predict_and_submit(model, model_type, test_features, sample_submission)\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ è¡Œå’Œç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\nğŸ“‹ å‰10è¡Œé¢„æµ‹ç»“æœ:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(f\"\\nğŸ“Š æäº¤æ–‡ä»¶ç»Ÿè®¡:\")\n",
    "print(f\"  - æ€»è¡Œæ•°: {len(submission)}\")\n",
    "print(f\"  - IdèŒƒå›´: {submission['Id'].min()} - {submission['Id'].max()}\")\n",
    "print(f\"  - æ’åèŒƒå›´: {submission['rank'].min()} - {submission['rank'].max()}\")\n",
    "\n",
    "print(\"\\nğŸ‰ å®Œæˆï¼submission.csvå·²ç”Ÿæˆå¹¶ä¿å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ æ¨¡å‹é—®é¢˜è¯Šæ–­å’Œä¿®å¤\n",
    "print(\"ğŸ” è¯Šæ–­æ¨¡å‹é—®é¢˜...\")\n",
    "\n",
    "# æ£€æŸ¥å½“å‰æ¨¡å‹çŠ¶æ€\n",
    "print(f\"å½“å‰æ¨¡å‹ç±»å‹: {model_type}\")\n",
    "print(f\"æ¨¡å‹å¯¹è±¡: {type(model)}\")\n",
    "\n",
    "if model_type == 'baseline':\n",
    "    print(\"âŒ æ£€æµ‹åˆ°ä½¿ç”¨äº†baselineæ¨¡å‹ï¼Œè¿™ä¼šå¯¼è‡´æ‰€æœ‰é¢„æµ‹å€¼ç›¸åŒ\")\n",
    "    print(\"ğŸ”§ å°è¯•é‡æ–°è®­ç»ƒæ›´å¼ºçš„æ¨¡å‹...\")\n",
    "    \n",
    "    # é‡æ–°å‡†å¤‡æ•°æ®\n",
    "    feature_cols = [col for col in train_features.columns if col not in ['Id', 'ranker_id', 'selected']]\n",
    "    X = train_features[feature_cols].fillna(0)\n",
    "    y = train_features['selected']\n",
    "    \n",
    "    print(f\"ğŸ“Š æ•°æ®æ£€æŸ¥:\")\n",
    "    print(f\"  - ç‰¹å¾æ•°é‡: {len(feature_cols)}\")\n",
    "    print(f\"  - æ ·æœ¬æ•°é‡: {len(X)}\")\n",
    "    print(f\"  - æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.6f}\")\n",
    "    print(f\"  - ç‰¹å¾ç¼ºå¤±å€¼: {X.isnull().sum().sum()}\")\n",
    "    print(f\"  - ç‰¹å¾æ•°å€¼èŒƒå›´æ£€æŸ¥:\")\n",
    "    \n",
    "    for col in feature_cols[:5]:  # æ£€æŸ¥å‰5ä¸ªç‰¹å¾\n",
    "        print(f\"    {col}: {X[col].min():.2f} - {X[col].max():.2f}\")\n",
    "    \n",
    "    # æ•°æ®åˆ†å‰²\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "    \n",
    "    print(f\"è®­ç»ƒé›†: {X_train.shape}, éªŒè¯é›†: {X_val.shape}\")\n",
    "    \n",
    "    # å°è¯•å¤šç§æ¨¡å‹\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_model_type = 'baseline'\n",
    "    \n",
    "    models_to_try = []\n",
    "    \n",
    "    # 1. å°è¯•ç®€åŒ–çš„Random Forest\n",
    "    try:\n",
    "        print(\"\\nğŸŒ² å°è¯•ç®€åŒ–Random Forest...\")\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        simple_rf = RandomForestRegressor(\n",
    "            n_estimators=20,    # å‡å°‘æ ‘æ•°é‡\n",
    "            max_depth=8,        # å‡å°‘æ·±åº¦\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=SEED,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        simple_rf.fit(X_train, y_train)\n",
    "        rf_pred = simple_rf.predict(X_val)\n",
    "        \n",
    "        # æ£€æŸ¥é¢„æµ‹å¤šæ ·æ€§\n",
    "        rf_std = np.std(rf_pred)\n",
    "        print(f\"  é¢„æµ‹å€¼æ ‡å‡†å·®: {rf_std:.6f}\")\n",
    "        \n",
    "        if rf_std > 1e-6:  # å¦‚æœæœ‰è¶³å¤Ÿçš„é¢„æµ‹å¤šæ ·æ€§\n",
    "            models_to_try.append(('simple_rf', simple_rf, rf_pred))\n",
    "            print(f\"  âœ… ç®€åŒ–RFæˆåŠŸï¼Œé¢„æµ‹èŒƒå›´: {rf_pred.min():.6f} - {rf_pred.max():.6f}\")\n",
    "        else:\n",
    "            print(\"  âŒ ç®€åŒ–RFé¢„æµ‹å€¼ç¼ºä¹å¤šæ ·æ€§\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ç®€åŒ–RFå¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    # 2. å°è¯•é€»è¾‘å›å½’\n",
    "    try:\n",
    "        print(\"\\nğŸ“ˆ å°è¯•é€»è¾‘å›å½’...\")\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr_model = LogisticRegression(random_state=SEED, max_iter=1000, C=1.0)\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        lr_pred = lr_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        lr_std = np.std(lr_pred)\n",
    "        print(f\"  é¢„æµ‹å€¼æ ‡å‡†å·®: {lr_std:.6f}\")\n",
    "        \n",
    "        if lr_std > 1e-6:\n",
    "            models_to_try.append(('logistic', lr_model, lr_pred))\n",
    "            print(f\"  âœ… é€»è¾‘å›å½’æˆåŠŸï¼Œé¢„æµ‹èŒƒå›´: {lr_pred.min():.6f} - {lr_pred.max():.6f}\")\n",
    "        else:\n",
    "            print(\"  âŒ é€»è¾‘å›å½’é¢„æµ‹å€¼ç¼ºä¹å¤šæ ·æ€§\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ é€»è¾‘å›å½’å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    # 3. å°è¯•æ¢¯åº¦æå‡\n",
    "    try:\n",
    "        print(\"\\nâš¡ å°è¯•æ¢¯åº¦æå‡...\")\n",
    "        from sklearn.ensemble import GradientBoostingRegressor\n",
    "        gb_model = GradientBoostingRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        gb_pred = gb_model.predict(X_val)\n",
    "        \n",
    "        gb_std = np.std(gb_pred)\n",
    "        print(f\"  é¢„æµ‹å€¼æ ‡å‡†å·®: {gb_std:.6f}\")\n",
    "        \n",
    "        if gb_std > 1e-6:\n",
    "            models_to_try.append(('gradient_boosting', gb_model, gb_pred))\n",
    "            print(f\"  âœ… æ¢¯åº¦æå‡æˆåŠŸï¼Œé¢„æµ‹èŒƒå›´: {gb_pred.min():.6f} - {gb_pred.max():.6f}\")\n",
    "        else:\n",
    "            print(\"  âŒ æ¢¯åº¦æå‡é¢„æµ‹å€¼ç¼ºä¹å¤šæ ·æ€§\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ æ¢¯åº¦æå‡å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    # 4. åˆ›å»ºåŸºäºç‰¹å¾çš„ç®€å•æ¨¡å‹\n",
    "    try:\n",
    "        print(\"\\nğŸ¯ å°è¯•åŸºäºç‰¹å¾çš„ç®€å•æ¨¡å‹...\")\n",
    "        \n",
    "        # ä½¿ç”¨å…³é”®ç‰¹å¾åˆ›å»ºç®€å•å¾—åˆ†å‡½æ•°\n",
    "        class FeatureBasedModel:\n",
    "            def __init__(self):\n",
    "                self.feature_weights = {}\n",
    "                \n",
    "            def fit(self, X, y):\n",
    "                # è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§\n",
    "                for col in X.columns:\n",
    "                    try:\n",
    "                        corr = np.corrcoef(X[col], y)[0, 1]\n",
    "                        self.feature_weights[col] = corr if not np.isnan(corr) else 0\n",
    "                    except:\n",
    "                        self.feature_weights[col] = 0\n",
    "                        \n",
    "            def predict(self, X):\n",
    "                # åŸºäºç‰¹å¾æƒé‡è®¡ç®—å¾—åˆ†\n",
    "                scores = np.zeros(len(X))\n",
    "                for col, weight in self.feature_weights.items():\n",
    "                    if col in X.columns:\n",
    "                        # æ ‡å‡†åŒ–ç‰¹å¾å€¼\n",
    "                        col_values = X[col].values\n",
    "                        if np.std(col_values) > 0:\n",
    "                            normalized = (col_values - np.mean(col_values)) / np.std(col_values)\n",
    "                            scores += weight * normalized\n",
    "                \n",
    "                # æ·»åŠ ä¸€äº›éšæœºæ€§ä»¥å¢åŠ å¤šæ ·æ€§\n",
    "                scores += np.random.normal(0, 0.1, len(scores))\n",
    "                return scores\n",
    "        \n",
    "        feature_model = FeatureBasedModel()\n",
    "        feature_model.fit(X_train, y_train)\n",
    "        feature_pred = feature_model.predict(X_val)\n",
    "        \n",
    "        feature_std = np.std(feature_pred)\n",
    "        print(f\"  é¢„æµ‹å€¼æ ‡å‡†å·®: {feature_std:.6f}\")\n",
    "        \n",
    "        if feature_std > 1e-6:\n",
    "            models_to_try.append(('feature_based', feature_model, feature_pred))\n",
    "            print(f\"  âœ… ç‰¹å¾æ¨¡å‹æˆåŠŸï¼Œé¢„æµ‹èŒƒå›´: {feature_pred.min():.6f} - {feature_pred.max():.6f}\")\n",
    "        else:\n",
    "            print(\"  âŒ ç‰¹å¾æ¨¡å‹é¢„æµ‹å€¼ç¼ºä¹å¤šæ ·æ€§\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ç‰¹å¾æ¨¡å‹å¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    # é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "    if models_to_try:\n",
    "        print(f\"\\nğŸ† æˆåŠŸè®­ç»ƒäº† {len(models_to_try)} ä¸ªæ¨¡å‹\")\n",
    "        \n",
    "        # é€‰æ‹©é¢„æµ‹å¤šæ ·æ€§æœ€é«˜çš„æ¨¡å‹\n",
    "        best_std = 0\n",
    "        for model_name, model_obj, pred in models_to_try:\n",
    "            pred_std = np.std(pred)\n",
    "            print(f\"  {model_name}: æ ‡å‡†å·® = {pred_std:.6f}\")\n",
    "            if pred_std > best_std:\n",
    "                best_std = pred_std\n",
    "                best_model = model_obj\n",
    "                best_model_type = model_name\n",
    "        \n",
    "        print(f\"ğŸ¯ é€‰æ‹©æ¨¡å‹: {best_model_type} (æ ‡å‡†å·®: {best_std:.6f})\")\n",
    "        \n",
    "        # æ›´æ–°å…¨å±€æ¨¡å‹å˜é‡\n",
    "        model = best_model\n",
    "        model_type = best_model_type\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Œå°†ä½¿ç”¨æ”¹è¿›çš„éšæœºæ¨¡å‹\")\n",
    "        \n",
    "        class ImprovedRandomModel:\n",
    "            def __init__(self):\n",
    "                pass\n",
    "                \n",
    "            def predict(self, X):\n",
    "                # åŸºäºä½ç½®å’Œç®€å•ç‰¹å¾çš„æ”¹è¿›éšæœºæ¨¡å‹\n",
    "                scores = np.random.beta(2, 5, len(X))  # åå‘è¾ƒå°å€¼çš„åˆ†å¸ƒ\n",
    "                \n",
    "                # å¦‚æœæœ‰ä½ç½®ç‰¹å¾ï¼Œç»™å‰é¢ä½ç½®æ›´é«˜æƒé‡\n",
    "                if 'position' in X.columns:\n",
    "                    position_boost = 1.0 / (X['position'].values + 1)\n",
    "                    scores *= (1 + position_boost)\n",
    "                \n",
    "                return scores\n",
    "        \n",
    "        model = ImprovedRandomModel()\n",
    "        model_type = 'improved_random'\n",
    "        print(\"âœ… ä½¿ç”¨æ”¹è¿›éšæœºæ¨¡å‹\")\n",
    "\n",
    "else:\n",
    "    print(f\"âœ… å½“å‰æ¨¡å‹ {model_type} çŠ¶æ€æ­£å¸¸\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ç»ˆä½¿ç”¨æ¨¡å‹: {model_type}\")\n",
    "print(f\"æ¨¡å‹å¯¹è±¡ç±»å‹: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04173c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹é‡æ–°ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹é‡æ–°ç”Ÿæˆæäº¤æ–‡ä»¶...\")\n",
    "\n",
    "def predict_with_fixed_model(model, model_type, test_features):\n",
    "    \"\"\"ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”® ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}\")\n",
    "    \n",
    "    # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "    feature_cols = [col for col in test_features.columns if col not in ['Id', 'ranker_id']]\n",
    "    X_test = test_features[feature_cols].fillna(0)\n",
    "    \n",
    "    print(f\"ğŸ“Š æµ‹è¯•æ•°æ®: {len(X_test)} æ ·æœ¬, {len(feature_cols)} ç‰¹å¾\")\n",
    "    \n",
    "    # è¿›è¡Œé¢„æµ‹\n",
    "    try:\n",
    "        if model_type == 'logistic':\n",
    "            # é€»è¾‘å›å½’éœ€è¦ç”¨predict_proba\n",
    "            predictions = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # å…¶ä»–æ¨¡å‹ç”¨predict\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "        predictions = np.array(predictions).flatten()\n",
    "        \n",
    "        print(f\"âœ… é¢„æµ‹å®Œæˆ\")\n",
    "        print(f\"ğŸ“ˆ é¢„æµ‹å€¼ç»Ÿè®¡:\")\n",
    "        print(f\"  - èŒƒå›´: {predictions.min():.6f} - {predictions.max():.6f}\")\n",
    "        print(f\"  - å‡å€¼: {predictions.mean():.6f}\")\n",
    "        print(f\"  - æ ‡å‡†å·®: {predictions.std():.6f}\")\n",
    "        \n",
    "        # æ£€æŸ¥é¢„æµ‹å¤šæ ·æ€§\n",
    "        if predictions.std() < 1e-6:\n",
    "            print(\"âš ï¸ é¢„æµ‹å€¼ç¼ºä¹å¤šæ ·æ€§ï¼Œæ·»åŠ å™ªå£°...\")\n",
    "            predictions += np.random.normal(0, 0.001, len(predictions))\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é¢„æµ‹å¤±è´¥: {str(e)}\")\n",
    "        print(\"ğŸ² ä½¿ç”¨éšæœºé¢„æµ‹...\")\n",
    "        return np.random.beta(2, 5, len(X_test))\n",
    "\n",
    "# è¿›è¡Œé¢„æµ‹\n",
    "predictions = predict_with_fixed_model(model, model_type, test_features)\n",
    "\n",
    "# ç”Ÿæˆæ’å\n",
    "print(\"\\nğŸ† ç”Ÿæˆæ’å...\")\n",
    "result_df = test_features[['Id', 'ranker_id']].copy()\n",
    "result_df['score'] = predictions\n",
    "\n",
    "# æŒ‰ç»„æ’åï¼ˆåˆ†æ•°è¶Šé«˜æ’åè¶Šå‰ï¼‰\n",
    "result_df['rank'] = result_df.groupby('ranker_id')['score'].rank(method='dense', ascending=False)\n",
    "\n",
    "# åˆ›å»ºæœ€ç»ˆæäº¤æ–‡ä»¶\n",
    "submission_fixed = result_df[['Id', 'rank']].copy()\n",
    "submission_fixed['rank'] = submission_fixed['rank'].astype(int)\n",
    "\n",
    "# éªŒè¯æ’ååˆ†å¸ƒ\n",
    "print(\"ğŸ“Š æ’ååˆ†å¸ƒéªŒè¯:\")\n",
    "rank_distribution = submission_fixed['rank'].value_counts().sort_index().head(10)\n",
    "print(rank_distribution)\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªä¼šè¯çš„æ’å\n",
    "session_ranks = submission_fixed.groupby('ranker_id')['rank'].agg(['min', 'max', 'count']).head()\n",
    "print(\"\\nğŸ“‹ å‰å‡ ä¸ªä¼šè¯çš„æ’åæ£€æŸ¥:\")\n",
    "print(session_ranks)\n",
    "\n",
    "# ä¿å­˜ä¿®å¤åçš„æäº¤æ–‡ä»¶\n",
    "fixed_submission_file = os.path.join(OUTPUT_PATH, 'submission_fixed.csv')\n",
    "submission_fixed.to_csv(fixed_submission_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ä¿®å¤åçš„æäº¤æ–‡ä»¶å·²ä¿å­˜: {fixed_submission_file}\")\n",
    "print(f\"ğŸ“Š æ–‡ä»¶ç»Ÿè®¡:\")\n",
    "print(f\"  - æ€»è¡Œæ•°: {len(submission_fixed)}\")\n",
    "print(f\"  - ä¼šè¯æ•°: {submission_fixed['ranker_id'].nunique()}\")\n",
    "print(f\"  - IdèŒƒå›´: {submission_fixed['Id'].min()} - {submission_fixed['Id'].max()}\")\n",
    "print(f\"  - æ’åèŒƒå›´: {submission_fixed['rank'].min()} - {submission_fixed['rank'].max()}\")\n",
    "\n",
    "# æ˜¾ç¤ºä¿®å¤åçš„å‰å‡ è¡Œ\n",
    "print(\"\\nğŸ“‹ ä¿®å¤åçš„å‰10è¡Œ:\")\n",
    "print(submission_fixed.head(10))\n",
    "\n",
    "# ä¸åŸå§‹æäº¤æ–‡ä»¶å¯¹æ¯”\n",
    "if 'submission' in locals():\n",
    "    print(\"\\nğŸ”„ ä¸åŸå§‹æäº¤æ–‡ä»¶å¯¹æ¯”:\")\n",
    "    print(f\"åŸå§‹æ’åèŒƒå›´: {submission['rank'].min()} - {submission['rank'].max()}\")\n",
    "    print(f\"ä¿®å¤æ’åèŒƒå›´: {submission_fixed['rank'].min()} - {submission_fixed['rank'].max()}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›\n",
    "    original_unique_ranks = submission['rank'].nunique()\n",
    "    fixed_unique_ranks = submission_fixed['rank'].nunique()\n",
    "    \n",
    "    print(f\"åŸå§‹å”¯ä¸€æ’åæ•°: {original_unique_ranks}\")\n",
    "    print(f\"ä¿®å¤å”¯ä¸€æ’åæ•°: {fixed_unique_ranks}\")\n",
    "    \n",
    "    if fixed_unique_ranks > original_unique_ranks:\n",
    "        print(\"âœ… æ’åå¤šæ ·æ€§å¾—åˆ°æ”¹å–„ï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ’åå¤šæ ·æ€§ä»éœ€æ”¹è¿›\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ¨¡å‹ä¿®å¤å’Œé‡æ–°é¢„æµ‹å®Œæˆï¼\")\n",
    "print(\"ğŸ’¡ è¯·ä½¿ç”¨ submission_fixed.csv ä½œä¸ºæœ€ç»ˆæäº¤æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6972459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ ä¸‹è½½æäº¤æ–‡ä»¶\n",
    "print(\"ğŸ“¥ å‡†å¤‡ä¸‹è½½æäº¤æ–‡ä»¶...\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # åœ¨Kaggleç¯å¢ƒä¸­ï¼Œæä¾›ä¸‹è½½é“¾æ¥\n",
    "    from IPython.display import FileLink, display\n",
    "    \n",
    "    print(\"ğŸŒ Kaggleç¯å¢ƒ - ç‚¹å‡»ä¸‹è½½é“¾æ¥:\")\n",
    "    \n",
    "    # æ˜¾ç¤ºä¿®å¤åçš„æ–‡ä»¶ä¸‹è½½é“¾æ¥\n",
    "    if os.path.exists('/kaggle/working/submission_fixed.csv'):\n",
    "        print(\"âœ… ä¿®å¤åçš„æäº¤æ–‡ä»¶ (æ¨èä½¿ç”¨):\")\n",
    "        display(FileLink('/kaggle/working/submission_fixed.csv'))\n",
    "    \n",
    "    # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ä¸‹è½½é“¾æ¥\n",
    "    if os.path.exists('/kaggle/working/submission.csv'):\n",
    "        print(\"\\nğŸ“ åŸå§‹æäº¤æ–‡ä»¶ (ä»…ä¾›å¯¹æ¯”):\")\n",
    "        display(FileLink('/kaggle/working/submission.csv'))\n",
    "    \n",
    "    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡ä»¶\n",
    "    print(\"\\nğŸ“‚ /kaggle/working/ ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶:\")\n",
    "    try:\n",
    "        import os\n",
    "        files = os.listdir('/kaggle/working/')\n",
    "        for file in files:\n",
    "            file_path = f'/kaggle/working/{file}'\n",
    "            file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "            print(f\"  ğŸ“„ {file} ({file_size:.1f} MB)\")\n",
    "    except:\n",
    "        print(\"  æ— æ³•åˆ—å‡ºæ–‡ä»¶\")\n",
    "        \n",
    "else:\n",
    "    # æœ¬åœ°ç¯å¢ƒ\n",
    "    print(\"ğŸ’» æœ¬åœ°ç¯å¢ƒ - æ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰ç›®å½•:\")\n",
    "    \n",
    "    if os.path.exists('submission_fixed.csv'):\n",
    "        print(\"âœ… submission_fixed.csv (æ¨èä½¿ç”¨)\")\n",
    "        print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize('submission_fixed.csv')/1024:.1f} KB\")\n",
    "    \n",
    "    if os.path.exists('submission.csv'):\n",
    "        print(\"ğŸ“ submission.csv (åŸå§‹æ–‡ä»¶)\")\n",
    "        print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize('submission.csv')/1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nğŸ¯ æäº¤å»ºè®®:\")\n",
    "print(\"1. ä¼˜å…ˆä½¿ç”¨ 'submission_fixed.csv' æ–‡ä»¶\")\n",
    "print(\"2. ç¡®è®¤æ’åèŒƒå›´ä¸æ˜¯å…¨ä¸º1\")\n",
    "print(\"3. æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç† (åº”è¯¥å‡ åMB)\")\n",
    "print(\"4. æäº¤å‰å¯ä»¥æŸ¥çœ‹å‰å‡ è¡Œç¡®è®¤æ ¼å¼æ­£ç¡®\")\n",
    "\n",
    "print(\"\\nğŸ ç®—æ³•é¡¹ç›®å®Œæˆï¼Good luck! ğŸ€\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
