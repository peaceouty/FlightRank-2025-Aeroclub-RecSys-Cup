{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92202046",
   "metadata": {},
   "source": [
    "# FlightRank 2025: Aeroclub RecSys Cup \n",
    "## å•†åŠ¡æ—…è¡Œè€…ä¸ªæ€§åŒ–èˆªç­æ¨èç³»ç»Ÿ\n",
    "\n",
    "è¿™ä¸ªnotebookåŒ…å«äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºé¢„æµ‹å•†åŠ¡æ—…è¡Œè€…åœ¨èˆªç­æœç´¢ç»“æœä¸­ä¼šé€‰æ‹©å“ªä¸ªèˆªç­é€‰é¡¹ã€‚\n",
    "\n",
    "### ç«èµ›ç›®æ ‡\n",
    "- æ„å»ºæ™ºèƒ½èˆªç­æ’åºæ¨¡å‹ï¼Œé¢„æµ‹å•†åŠ¡æ—…è¡Œè€…çš„é€‰æ‹©\n",
    "- è¯„ä¼°æŒ‡æ ‡ï¼šHitRate@3ï¼ˆæ­£ç¡®èˆªç­åœ¨å‰3åçš„æ¯”ä¾‹ï¼‰\n",
    "- åªè€ƒè™‘è¶…è¿‡10ä¸ªèˆªç­é€‰é¡¹çš„æœç´¢ç»„\n",
    "\n",
    "### æ–¹æ¡ˆæ¦‚è¿°\n",
    "1. **æ•°æ®åˆ†æä¸æ¢ç´¢** - ç†è§£æ•°æ®åˆ†å¸ƒå’Œç‰¹å¾\n",
    "2. **ç‰¹å¾å·¥ç¨‹** - æ„å»ºæœ‰æ•ˆçš„ç‰¹å¾\n",
    "3. **æ¨¡å‹å»ºæ¨¡** - å¤šç§æ’åºæ¨¡å‹æ–¹æ¡ˆ\n",
    "4. **æ¨¡å‹èåˆ** - æå‡é¢„æµ‹æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ff095",
   "metadata": {},
   "source": [
    "## âš™ï¸ Kaggle Notebook è®¾ç½®å»ºè®®\n",
    "\n",
    "### Persistence è®¾ç½®\n",
    "åœ¨Kaggleä¸­è¿è¡Œæ­¤notebookæ—¶ï¼Œå»ºè®®å°†Persistenceè®¾ç½®ä¸º **\"Variables and Files\"** æˆ– **\"Variables only\"**ï¼Œè¿™æ ·å¯ä»¥ï¼š\n",
    "- ä¿æŒè®­ç»ƒå¥½çš„æ¨¡å‹åœ¨å†…å­˜ä¸­\n",
    "- é¿å…é‡å¤è®­ç»ƒæ¨¡å‹\n",
    "- èŠ‚çœè®¡ç®—æ—¶é—´\n",
    "- ä¿ç•™ä¸­é—´å˜é‡å’Œæ•°æ®\n",
    "\n",
    "### æ–‡ä»¶æ ¼å¼è¯´æ˜\n",
    "- è®­ç»ƒæ•°æ®: `train.parquet` \n",
    "- æµ‹è¯•æ•°æ®: `test.parquet`\n",
    "- æ ·æœ¬æäº¤: `sample_submission.parquet`\n",
    "- æ•°æ®è·¯å¾„: `/kaggle/input/aeroclub-recsys-2025/`\n",
    "\n",
    "### è¾“å‡ºæ–‡ä»¶\n",
    "- æœ€ç»ˆæäº¤æ–‡ä»¶å°†ä¿å­˜åœ¨ `/kaggle/working/submission.csv`\n",
    "- åŒæ—¶ä¹Ÿä¼šç”Ÿæˆ `submission.parquet` æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae72b9",
   "metadata": {},
   "source": [
    "## 1. æ£€æµ‹è¿è¡Œç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338cd191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ£€æµ‹è®¡ç®—ç¯å¢ƒ...\n",
      "âŒ TPU ä¸å¯ç”¨: Please provide a TPU Name to connect to.\n",
      "WARNING:tensorflow:From C:\\Users\\ShuaiZhiyu\\AppData\\Local\\Temp\\ipykernel_22588\\4218732011.py:34: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "âŒ GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨CPU\n",
      "âœ… è¿è¡Œåœ¨æœ¬åœ°ç¯å¢ƒ\n",
      "ğŸ“ æ•°æ®è·¯å¾„: ./\n",
      "ğŸ“ è¾“å‡ºè·¯å¾„: ./\n",
      "ğŸ“‚ Available data files:\n",
      "\n",
      "ğŸ” å½“å‰è¿è¡Œç¯å¢ƒ: Local Environment\n",
      "Pythonç‰ˆæœ¬: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)]\n",
      "å½“å‰å·¥ä½œç›®å½•: c:\\Users\\ShuaiZhiyu\\Desktop\\FlightRank_2025\n",
      "æœ¬åœ°ç¯å¢ƒ - è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®\n",
      "âŒ TPU ä¸å¯ç”¨: Please provide a TPU Name to connect to.\n",
      "WARNING:tensorflow:From C:\\Users\\ShuaiZhiyu\\AppData\\Local\\Temp\\ipykernel_22588\\4218732011.py:34: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "âŒ GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨CPU\n",
      "âœ… è¿è¡Œåœ¨æœ¬åœ°ç¯å¢ƒ\n",
      "ğŸ“ æ•°æ®è·¯å¾„: ./\n",
      "ğŸ“ è¾“å‡ºè·¯å¾„: ./\n",
      "ğŸ“‚ Available data files:\n",
      "\n",
      "ğŸ” å½“å‰è¿è¡Œç¯å¢ƒ: Local Environment\n",
      "Pythonç‰ˆæœ¬: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)]\n",
      "å½“å‰å·¥ä½œç›®å½•: c:\\Users\\ShuaiZhiyu\\Desktop\\FlightRank_2025\n",
      "æœ¬åœ°ç¯å¢ƒ - è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ğŸš€ TPU ç¯å¢ƒæ£€æµ‹å’Œåˆå§‹åŒ–\n",
    "print(\"ğŸ” æ£€æµ‹è®¡ç®—ç¯å¢ƒ...\")\n",
    "\n",
    "# æ£€æµ‹TPUå¯ç”¨æ€§\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # å°è¯•è¿æ¥TPU\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        \n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        HAS_TPU = True\n",
    "        TPU_REPLICAS = strategy.num_replicas_in_sync\n",
    "        print(f\"âœ… TPU å·²è¿æ¥! å‰¯æœ¬æ•°: {TPU_REPLICAS}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        HAS_TPU = False\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(f\"âŒ TPU ä¸å¯ç”¨: {str(e)}\")\n",
    "        \n",
    "        # æ£€æŸ¥GPUå¯ç”¨æ€§\n",
    "        if tf.test.is_gpu_available():\n",
    "            print(\"âœ… GPU å¯ç”¨\")\n",
    "            HAS_GPU = True\n",
    "        else:\n",
    "            print(\"âŒ GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨CPU\")\n",
    "            HAS_GPU = False\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorFlow æœªå®‰è£…ï¼Œå°†ä½¿ç”¨scikit-learnå’ŒLightGBM\")\n",
    "    HAS_TPU = False\n",
    "    HAS_GPU = False\n",
    "    strategy = None\n",
    "\n",
    "# æ£€æµ‹Kaggleç¯å¢ƒ\n",
    "IN_KAGGLE = 'KAGGLE_WORKING_DIR' in os.environ or '/kaggle/' in os.getcwd()\n",
    "if IN_KAGGLE:\n",
    "    print(\"âœ… è¿è¡Œåœ¨ Kaggle ç¯å¢ƒ\")\n",
    "    DATA_PATH = '/kaggle/input/aeroclub-recsys-2025/'\n",
    "    OUTPUT_PATH = '/kaggle/working/'\n",
    "else:\n",
    "    print(\"âœ… è¿è¡Œåœ¨æœ¬åœ°ç¯å¢ƒ\")\n",
    "    DATA_PATH = './'\n",
    "    OUTPUT_PATH = './'\n",
    "\n",
    "print(f\"ğŸ“ æ•°æ®è·¯å¾„: {DATA_PATH}\")\n",
    "print(f\"ğŸ“ è¾“å‡ºè·¯å¾„: {OUTPUT_PATH}\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "if 'tf' in locals():\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "print(\"ğŸ“‚ Available data files:\")\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# æ£€æµ‹æ˜¯å¦åœ¨Kaggleç¯å¢ƒä¸­è¿è¡Œ\n",
    "def is_kaggle_env():\n",
    "    \"\"\"æ£€æµ‹å½“å‰æ˜¯å¦åœ¨Kaggleç¯å¢ƒä¸­è¿è¡Œ\"\"\"\n",
    "    return os.path.exists('/kaggle/input') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "# æ£€æµ‹ç¯å¢ƒ\n",
    "IN_KAGGLE = is_kaggle_env()\n",
    "\n",
    "print(f\"\\nğŸ” å½“å‰è¿è¡Œç¯å¢ƒ: {'Kaggle Notebook' if IN_KAGGLE else 'Local Environment'}\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"Kaggleç¯å¢ƒæ£€æµ‹åˆ°ä»¥ä¸‹è·¯å¾„:\")\n",
    "    print(f\"- Inputç›®å½•: {os.path.exists('/kaggle/input')}\")\n",
    "    print(f\"- Workingç›®å½•: {os.path.exists('/kaggle/working')}\")\n",
    "    print(f\"- Tempç›®å½•: {os.path.exists('/kaggle/temp')}\")\n",
    "else:\n",
    "    print(\"æœ¬åœ°ç¯å¢ƒ - è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f94bf",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771b6440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åº“å¯¼å…¥çŠ¶æ€:\n",
      "- LightGBM: âœ…\n",
      "- XGBoost: âœ…\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 87.0% (13.2GB/15.2GB)\n",
      "âœ… åº“å¯¼å…¥å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# åŸºç¡€åº“å¯¼å…¥ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc  # åƒåœ¾å›æ”¶\n",
    "\n",
    "# æœºå™¨å­¦ä¹ åŸºç¡€åº“\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# å†…å­˜ä¼˜åŒ–è®¾ç½®\n",
    "pd.set_option('display.max_columns', 20)  # é™åˆ¶æ˜¾ç¤ºåˆ—æ•°\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# æ£€æŸ¥å¹¶å¯¼å…¥å¯é€‰åº“ - ç®€åŒ–ç‰ˆæœ¬\n",
    "def safe_import(module_name):\n",
    "    \"\"\"å®‰å…¨å¯¼å…¥åº“ï¼Œå¤±è´¥æ—¶è¿”å›None\"\"\"\n",
    "    try:\n",
    "        if module_name == 'lightgbm':\n",
    "            import lightgbm as lgb\n",
    "            return lgb, True\n",
    "        elif module_name == 'xgboost':\n",
    "            import xgboost as xgb\n",
    "            return xgb, True\n",
    "    except ImportError:\n",
    "        return None, False\n",
    "\n",
    "# åªå¯¼å…¥æ ¸å¿ƒåº“ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
    "lgb, HAS_LGB = safe_import('lightgbm')\n",
    "xgb, HAS_XGB = safe_import('xgboost')\n",
    "\n",
    "print(\"åº“å¯¼å…¥çŠ¶æ€:\")\n",
    "print(f\"- LightGBM: {'âœ…' if HAS_LGB else 'âŒ'}\")\n",
    "print(f\"- XGBoost: {'âœ…' if HAS_XGB else 'âŒ'}\")\n",
    "\n",
    "# è®¾ç½®\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# å†…å­˜ç›‘æ§å‡½æ•°\n",
    "def check_memory():\n",
    "    \"\"\"æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)\")\n",
    "\n",
    "try:\n",
    "    check_memory()\n",
    "except:\n",
    "    print(\"ğŸ’¾ å†…å­˜ç›‘æ§ä¸å¯ç”¨\")\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ee720",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®åŠ è½½ä¸é¢„è§ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36afa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒ\n",
      "ğŸ“‚ å°è¯•è·¯å¾„: .\n",
      "ğŸ“‚ å°è¯•è·¯å¾„: data\n",
      "ğŸ“‚ å°è¯•è·¯å¾„: ../data\n",
      "âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºå°å‹æ¨¡æ‹Ÿæ•°æ®\n",
      "ğŸ”§ åˆ›å»ºå°å‹æ¨¡æ‹Ÿæ•°æ®...\n",
      "ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...\n",
      "ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...\n",
      "ğŸ“Š å°å‹æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ:\n",
      "   è®­ç»ƒæ•°æ®: (492, 6)\n",
      "   æµ‹è¯•æ•°æ®: (168, 5)\n",
      "\n",
      "ğŸ“‹ æ•°æ®æ¦‚è§ˆ:\n",
      "è®­ç»ƒæ•°æ®: (492, 6)\n",
      "æµ‹è¯•æ•°æ®: (168, 5)\n",
      "æäº¤æ–‡ä»¶: (168, 2)\n",
      "\n",
      "ğŸ“‹ è®­ç»ƒæ•°æ®å‰5è¡Œ:\n",
      "   Id  ranker_id  totalPrice  total_flight_duration airline  selected\n",
      "0   1          0  927.456299             244.214386      DL         0\n",
      "1   2          0  540.124695             219.811996      DL         0\n",
      "2   3          0  225.050629             524.296692      DL         1\n",
      "3   4          0  516.120178             564.796265      UA         0\n",
      "4   5          0  937.499390             162.476395      UA         0\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®åŠ è½½ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬\n",
    "def load_data():\n",
    "    \"\"\"åŠ è½½æ•°æ® - å†…å­˜ä¼˜åŒ–ï¼Œåˆ†å—å¤„ç†\"\"\"\n",
    "    \n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        data_paths = ['/kaggle/input/aeroclub-recsys-2025', '/kaggle/input']\n",
    "        output_path = '/kaggle/working'\n",
    "        print(\"ğŸ” æ£€æµ‹åˆ°Kaggleç¯å¢ƒ\")\n",
    "    else:\n",
    "        data_paths = ['.', 'data', '../data']\n",
    "        output_path = '.'\n",
    "        print(\"ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒ\")\n",
    "    \n",
    "    train_df = test_df = sample_submission = None\n",
    "    \n",
    "    for data_path in data_paths:\n",
    "        try:\n",
    "            print(f\"ğŸ“‚ å°è¯•è·¯å¾„: {data_path}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ–‡ä»¶\n",
    "            train_file = os.path.join(data_path, 'train.parquet')\n",
    "            test_file = os.path.join(data_path, 'test.parquet')\n",
    "            sample_file = os.path.join(data_path, 'sample_submission.parquet')\n",
    "            \n",
    "            # åŠ è½½è®­ç»ƒæ•°æ® - å†…å­˜ä¼˜åŒ–\n",
    "            if os.path.exists(train_file):\n",
    "                print(\"ğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
    "                train_df = pd.read_parquet(train_file)\n",
    "                \n",
    "                # ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜\n",
    "                train_df = optimize_dtypes(train_df)\n",
    "                print(f\"âœ… è®­ç»ƒæ•°æ®: {train_df.shape}\")\n",
    "                print(f\"ğŸ’¾ å†…å­˜ä½¿ç”¨: {train_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "                \n",
    "            # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "            if os.path.exists(test_file):\n",
    "                print(\"ğŸ“Š æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...\")\n",
    "                test_df = pd.read_parquet(test_file)\n",
    "                test_df = optimize_dtypes(test_df)\n",
    "                print(f\"âœ… æµ‹è¯•æ•°æ®: {test_df.shape}\")\n",
    "                \n",
    "            # åŠ è½½æ ·æœ¬æäº¤æ–‡ä»¶\n",
    "            if os.path.exists(sample_file):\n",
    "                sample_submission = pd.read_parquet(sample_file)\n",
    "                print(f\"âœ… æ ·æœ¬æäº¤: {sample_submission.shape}\")\n",
    "                \n",
    "            if train_df is not None:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è·¯å¾„ {data_path} å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if train_df is None:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºå°å‹æ¨¡æ‹Ÿæ•°æ®\")\n",
    "        return create_small_mock_data()\n",
    "    \n",
    "    return train_df, test_df, sample_submission, output_path\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜\"\"\"\n",
    "    print(\"ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != 'object':\n",
    "            if col_type == 'int64':\n",
    "                # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºæ›´å°çš„æ•´æ•°ç±»å‹\n",
    "                if df[col].min() >= -128 and df[col].max() <= 127:\n",
    "                    df[col] = df[col].astype('int8')\n",
    "                elif df[col].min() >= -32768 and df[col].max() <= 32767:\n",
    "                    df[col] = df[col].astype('int16')\n",
    "                elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "                    \n",
    "            elif col_type == 'float64':\n",
    "                # è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜\n",
    "                df[col] = df[col].astype('float32')\n",
    "                \n",
    "        else:\n",
    "            # å¯¹äºå­—ç¬¦ä¸²ç±»å‹ï¼Œè½¬æ¢ä¸ºcategoryï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤šï¼‰\n",
    "            if df[col].nunique() < len(df) * 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_small_mock_data():\n",
    "    \"\"\"åˆ›å»ºå°å‹æ¨¡æ‹Ÿæ•°æ®\"\"\"\n",
    "    print(\"ğŸ”§ åˆ›å»ºå°å‹æ¨¡æ‹Ÿæ•°æ®...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_sessions = 50  # å‡å°‘ä¼šè¯æ•°\n",
    "    flights_per_session = np.random.randint(5, 15, n_sessions)\n",
    "    \n",
    "    data = []\n",
    "    flight_id = 1\n",
    "    \n",
    "    for session_id in range(n_sessions):\n",
    "        n_flights = flights_per_session[session_id]\n",
    "        selected_idx = np.random.randint(0, n_flights)\n",
    "        \n",
    "        for flight_idx in range(n_flights):\n",
    "            selected = 1 if flight_idx == selected_idx else 0\n",
    "            base_price = np.random.uniform(200, 1000)\n",
    "            \n",
    "            data.append({\n",
    "                'Id': flight_id,\n",
    "                'ranker_id': session_id,\n",
    "                'totalPrice': base_price,\n",
    "                'total_flight_duration': np.random.uniform(120, 600),\n",
    "                'airline': np.random.choice(['AA', 'DL', 'UA']),\n",
    "                'selected': selected\n",
    "            })\n",
    "            flight_id += 1\n",
    "    \n",
    "    train_df = pd.DataFrame(data)\n",
    "    train_df = optimize_dtypes(train_df)\n",
    "    \n",
    "    # å°å‹æµ‹è¯•æ•°æ®\n",
    "    test_data = []\n",
    "    for session_id in range(n_sessions, n_sessions + 20):\n",
    "        n_flights = np.random.randint(5, 12)\n",
    "        \n",
    "        for flight_idx in range(n_flights):\n",
    "            base_price = np.random.uniform(200, 1000)\n",
    "            test_data.append({\n",
    "                'Id': flight_id,\n",
    "                'ranker_id': session_id,\n",
    "                'totalPrice': base_price,\n",
    "                'total_flight_duration': np.random.uniform(120, 600),\n",
    "                'airline': np.random.choice(['AA', 'DL', 'UA'])\n",
    "            })\n",
    "            flight_id += 1\n",
    "    \n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    test_df = optimize_dtypes(test_df)\n",
    "    \n",
    "    sample_submission = pd.DataFrame({\n",
    "        'Id': test_df['Id'],\n",
    "        'rank': 1\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“Š å°å‹æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ:\")\n",
    "    print(f\"   è®­ç»ƒæ•°æ®: {train_df.shape}\")\n",
    "    print(f\"   æµ‹è¯•æ•°æ®: {test_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df, sample_submission, '.'\n",
    "\n",
    "def create_mock_data():\n",
    "    \"\"\"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºä»£ç æµ‹è¯•\"\"\"\n",
    "    print(\"ğŸ”§ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\n",
    "    np.random.seed(42)\n",
    "    n_sessions = 200  # å¢åŠ ä¼šè¯æ•°\n",
    "    flights_per_session = np.random.randint(8, 25, n_sessions)  # æ›´æ¥è¿‘çœŸå®åˆ†å¸ƒ\n",
    "    \n",
    "    data = []\n",
    "    flight_id = 1\n",
    "    \n",
    "    for session_id in range(n_sessions):\n",
    "        n_flights = flights_per_session[session_id]\n",
    "        \n",
    "        # æ¯ä¸ªä¼šè¯åªæœ‰ä¸€ä¸ªèˆªç­è¢«é€‰ä¸­\n",
    "        selected_idx = np.random.randint(0, n_flights)\n",
    "        \n",
    "        for flight_idx in range(n_flights):\n",
    "            selected = 1 if flight_idx == selected_idx else 0\n",
    "            \n",
    "            # åˆ›å»ºæ›´ä¸°å¯Œçš„ç‰¹å¾\n",
    "            base_price = np.random.uniform(200, 1500)\n",
    "            data.append({\n",
    "                'Id': flight_id,\n",
    "                'ranker_id': session_id,\n",
    "                'totalPrice': base_price + np.random.normal(0, 50),\n",
    "                'baseFare': base_price * 0.7 + np.random.normal(0, 20),\n",
    "                'totalTax': base_price * 0.3 + np.random.normal(0, 10),\n",
    "                'total_flight_duration': np.random.uniform(120, 800),  # åˆ†é’Ÿ\n",
    "                'airline': np.random.choice(['AA', 'DL', 'UA', 'BA', 'LH', 'AF']),\n",
    "                'cabinClass': np.random.choice(['Economy', 'Business', 'First']),\n",
    "                'stops': np.random.choice([0, 1, 2], p=[0.6, 0.3, 0.1]),\n",
    "                'isRefundable': np.random.choice([0, 1], p=[0.7, 0.3]),\n",
    "                'selected': selected\n",
    "            })\n",
    "            flight_id += 1\n",
    "    \n",
    "    train_df = pd.DataFrame(data)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®ï¼ˆæ— selectedåˆ—ï¼‰\n",
    "    test_data = []\n",
    "    for session_id in range(n_sessions, n_sessions + 100):\n",
    "        n_flights = np.random.randint(8, 20)\n",
    "        \n",
    "        for flight_idx in range(n_flights):\n",
    "            base_price = np.random.uniform(200, 1500)\n",
    "            test_data.append({\n",
    "                'Id': flight_id,\n",
    "                'ranker_id': session_id,\n",
    "                'totalPrice': base_price + np.random.normal(0, 50),\n",
    "                'baseFare': base_price * 0.7 + np.random.normal(0, 20),\n",
    "                'totalTax': base_price * 0.3 + np.random.normal(0, 10),\n",
    "                'total_flight_duration': np.random.uniform(120, 800),\n",
    "                'airline': np.random.choice(['AA', 'DL', 'UA', 'BA', 'LH', 'AF']),\n",
    "                'cabinClass': np.random.choice(['Economy', 'Business', 'First']),\n",
    "                'stops': np.random.choice([0, 1, 2], p=[0.6, 0.3, 0.1]),\n",
    "                'isRefundable': np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "            })\n",
    "            flight_id += 1\n",
    "    \n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæäº¤æ–‡ä»¶\n",
    "    sample_submission = pd.DataFrame({\n",
    "        'Id': test_df['Id'],\n",
    "        'rank': 1\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ:\")\n",
    "    print(f\"   è®­ç»ƒæ•°æ®: {train_df.shape}\")\n",
    "    print(f\"   æµ‹è¯•æ•°æ®: {test_df.shape}\")\n",
    "    print(f\"   è®­ç»ƒæ•°æ®åˆ—: {list(train_df.columns)}\")\n",
    "    \n",
    "    return train_df, test_df, sample_submission, '.'\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train_df, test_df, sample_submission, OUTPUT_PATH = load_data()\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®ä¿¡æ¯\n",
    "if train_df is not None:\n",
    "    print(f\"\\nğŸ“‹ æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"è®­ç»ƒæ•°æ®: {train_df.shape}\")\n",
    "    if test_df is not None:\n",
    "        print(f\"æµ‹è¯•æ•°æ®: {test_df.shape}\")\n",
    "    if sample_submission is not None:\n",
    "        print(f\"æäº¤æ–‡ä»¶: {sample_submission.shape}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ è®­ç»ƒæ•°æ®å‰5è¡Œ:\")\n",
    "    print(train_df.head())\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®åŠ è½½å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c7912",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®åˆ†æä¸æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2212912c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  å¤§ä¼šè¯ä¸­èˆªç­å æ¯”: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlarge_sessions\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# åˆ†æè®­ç»ƒæ•°æ®\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrain_df\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     analyze_data(train_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè®­ç»ƒæ•°æ®\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# åˆ†ææµ‹è¯•æ•°æ®\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_data(df, name=\"æ•°æ®\"):\n",
    "    \"\"\"å…¨é¢çš„æ•°æ®åˆ†æå‡½æ•°\"\"\"\n",
    "    print(f\"\\n=== {name}è¯¦ç»†åˆ†æ ===\")\n",
    "    \n",
    "    # åŸºæœ¬ä¿¡æ¯\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "    print(f\"å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # ç¼ºå¤±å€¼åˆ†æ\n",
    "    missing_info = df.isnull().sum()\n",
    "    missing_info = missing_info[missing_info > 0]\n",
    "    if len(missing_info) > 0:\n",
    "        print(f\"\\nç¼ºå¤±å€¼åˆ†æ:\")\n",
    "        for col, missing_count in missing_info.items():\n",
    "            print(f\"  {col}: {missing_count} ({missing_count/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\nâœ… æ— ç¼ºå¤±å€¼\")\n",
    "    \n",
    "    # æ•°æ®ç±»å‹åˆ†æ\n",
    "    print(f\"\\næ•°æ®ç±»å‹åˆ†æ:\")\n",
    "    for dtype in df.dtypes.unique():\n",
    "        cols = df.select_dtypes(include=[dtype]).columns.tolist()\n",
    "        print(f\"  {dtype}: {len(cols)} åˆ— - {cols[:5]}{'...' if len(cols) > 5 else ''}\")\n",
    "    \n",
    "    # å¦‚æœæ˜¯è®­ç»ƒæ•°æ®ï¼Œåˆ†æç›®æ ‡å˜é‡\n",
    "    if 'selected' in df.columns:\n",
    "        selected_stats = df['selected'].value_counts()\n",
    "        print(f\"\\nç›®æ ‡å˜é‡åˆ†å¸ƒ:\")\n",
    "        print(f\"  é€‰ä¸­çš„èˆªç­ (selected=1): {selected_stats.get(1, 0)}\")\n",
    "        print(f\"  æœªé€‰ä¸­çš„èˆªç­ (selected=0): {selected_stats.get(0, 0)}\")\n",
    "        print(f\"  é€‰ä¸­ç‡: {selected_stats.get(1, 0) / len(df) * 100:.3f}%\")\n",
    "    \n",
    "    # ranker_idåˆ†æ\n",
    "    if 'ranker_id' in df.columns:\n",
    "        ranker_stats = df['ranker_id'].value_counts()\n",
    "        print(f\"\\næœç´¢ä¼šè¯åˆ†æ:\")\n",
    "        print(f\"  æ€»æœç´¢ä¼šè¯æ•°: {df['ranker_id'].nunique()}\")\n",
    "        print(f\"  æ¯ä¸ªä¼šè¯å¹³å‡èˆªç­æ•°: {ranker_stats.mean():.2f}\")\n",
    "        print(f\"  æ¯ä¸ªä¼šè¯èˆªç­æ•°ä¸­ä½æ•°: {ranker_stats.median():.2f}\")\n",
    "        print(f\"  æœ€å¤§èˆªç­æ•°: {ranker_stats.max()}\")\n",
    "        print(f\"  æœ€å°èˆªç­æ•°: {ranker_stats.min()}\")\n",
    "        \n",
    "        # ä¼šè¯å¤§å°åˆ†å¸ƒ\n",
    "        print(f\"\\nä¼šè¯å¤§å°åˆ†å¸ƒ:\")\n",
    "        size_dist = ranker_stats.value_counts().sort_index()\n",
    "        print(\"  å‰10ä¸ªæœ€å¸¸è§çš„ä¼šè¯å¤§å°:\")\n",
    "        for size, count in size_dist.head(10).items():\n",
    "            print(f\"    {size} èˆªç­: {count} ä¼šè¯\")\n",
    "        \n",
    "        # å¤§äº10çš„ä¼šè¯ï¼ˆè¯„ä¼°é‡ç‚¹ï¼‰\n",
    "        large_sessions = ranker_stats[ranker_stats > 10]\n",
    "        print(f\"\\nå¤§ä¼šè¯åˆ†æ (>10 èˆªç­, ç”¨äºè¯„ä¼°):\")\n",
    "        print(f\"  å¤§ä¼šè¯æ•°é‡: {len(large_sessions)}\")\n",
    "        print(f\"  å¤§ä¼šè¯å æ¯”: {len(large_sessions) / len(ranker_stats) * 100:.2f}%\")\n",
    "        print(f\"  å¤§ä¼šè¯ä¸­çš„èˆªç­æ•°: {large_sessions.sum()}\")\n",
    "        print(f\"  å¤§ä¼šè¯ä¸­èˆªç­å æ¯”: {large_sessions.sum() / len(df) * 100:.2f}%\")\n",
    "\n",
    "# åˆ†æè®­ç»ƒæ•°æ®\n",
    "if train_df is not None:\n",
    "    analyze_data(train_df, \"è®­ç»ƒæ•°æ®\")\n",
    "\n",
    "# åˆ†ææµ‹è¯•æ•°æ®\n",
    "if test_df is not None:\n",
    "    analyze_data(test_df, \"æµ‹è¯•æ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–åˆ†æ\n",
    "def visualize_data(df, name=\"æ•°æ®\"):\n",
    "    \"\"\"æ•°æ®å¯è§†åŒ–åˆ†æ\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== {name}å¯è§†åŒ–åˆ†æ ===\")\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{name}åˆ†æ', fontsize=16)\n",
    "    \n",
    "    # 1. ä¼šè¯å¤§å°åˆ†å¸ƒ\n",
    "    if 'ranker_id' in df.columns:\n",
    "        session_sizes = df['ranker_id'].value_counts()\n",
    "        \n",
    "        # ä¼šè¯å¤§å°åˆ†å¸ƒï¼ˆå‰20ï¼‰\n",
    "        ax1 = axes[0, 0]\n",
    "        top_sizes = session_sizes.value_counts().sort_index().head(20)\n",
    "        ax1.bar(top_sizes.index, top_sizes.values, alpha=0.7)\n",
    "        ax1.set_title('ä¼šè¯å¤§å°åˆ†å¸ƒ (å‰20)')\n",
    "        ax1.set_xlabel('æ¯ä¸ªä¼šè¯çš„èˆªç­æ•°')\n",
    "        ax1.set_ylabel('ä¼šè¯æ•°é‡')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å¤§ä¼šè¯vså°ä¼šè¯\n",
    "        ax2 = axes[0, 1]\n",
    "        large_sessions = (session_sizes > 10).sum()\n",
    "        small_sessions = (session_sizes <= 10).sum()\n",
    "        ax2.pie([large_sessions, small_sessions], \n",
    "               labels=[f'å¤§ä¼šè¯ (>10)\\n{large_sessions}', f'å°ä¼šè¯ (â‰¤10)\\n{small_sessions}'],\n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('å¤§ä¼šè¯ vs å°ä¼šè¯åˆ†å¸ƒ')\n",
    "    \n",
    "    # 2. æ•°å€¼ç‰¹å¾åˆ†å¸ƒï¼ˆå¦‚æœæœ‰priceç›¸å…³ç‰¹å¾ï¼‰\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Id' in numeric_cols:\n",
    "        numeric_cols.remove('Id')\n",
    "    if 'selected' in numeric_cols:\n",
    "        numeric_cols.remove('selected')\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # é€‰æ‹©ä¸€ä¸ªæ•°å€¼ç‰¹å¾è¿›è¡Œåˆ†æ\n",
    "        feature_col = numeric_cols[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ•°å€¼ç‰¹å¾æ˜¯ä»·æ ¼\n",
    "        \n",
    "        ax3 = axes[1, 0]\n",
    "        if 'selected' in df.columns:\n",
    "            # æŒ‰é€‰ä¸­çŠ¶æ€åˆ†ç»„\n",
    "            selected_data = df[df['selected'] == 1][feature_col].dropna()\n",
    "            not_selected_data = df[df['selected'] == 0][feature_col].dropna()\n",
    "            \n",
    "            ax3.hist(selected_data, bins=50, alpha=0.7, label='é€‰ä¸­', density=True)\n",
    "            ax3.hist(not_selected_data, bins=50, alpha=0.7, label='æœªé€‰ä¸­', density=True)\n",
    "            ax3.set_xlabel(feature_col)\n",
    "            ax3.set_ylabel('å¯†åº¦')\n",
    "            ax3.set_title(f'{feature_col} åˆ†å¸ƒå¯¹æ¯”')\n",
    "            ax3.legend()\n",
    "        else:\n",
    "            ax3.hist(df[feature_col].dropna(), bins=50, alpha=0.7)\n",
    "            ax3.set_xlabel(feature_col)\n",
    "            ax3.set_ylabel('é¢‘æ•°')\n",
    "            ax3.set_title(f'{feature_col} åˆ†å¸ƒ')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. ç›¸å…³æ€§åˆ†æ\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(numeric_cols) > 1:\n",
    "        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\n",
    "        correlation_cols = numeric_cols[:10]  # åªå–å‰10ä¸ªç‰¹å¾\n",
    "        corr_matrix = df[correlation_cols].corr()\n",
    "        \n",
    "        # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "        im = ax4.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "        ax4.set_xticks(range(len(corr_matrix.columns)))\n",
    "        ax4.set_yticks(range(len(corr_matrix.columns)))\n",
    "        ax4.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
    "        ax4.set_yticklabels(corr_matrix.columns)\n",
    "        ax4.set_title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')\n",
    "        \n",
    "        # æ·»åŠ é¢œè‰²æ¡\n",
    "        plt.colorbar(im, ax=ax4, shrink=0.8)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'æ•°å€¼ç‰¹å¾ä¸è¶³\\næ— æ³•è®¡ç®—ç›¸å…³æ€§', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('ç›¸å…³æ€§åˆ†æ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# æ‰§è¡Œå¯è§†åŒ–\n",
    "if train_df is not None:\n",
    "    visualize_data(train_df, \"è®­ç»ƒæ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fbb29",
   "metadata": {},
   "source": [
    "## 5. ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "ç‰¹å¾å·¥ç¨‹æ˜¯è¿™ä¸ªç«èµ›çš„å…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬éœ€è¦ä»åŸå§‹æ•°æ®ä¸­æå–èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹ç†è§£å•†åŠ¡æ—…è¡Œè€…åå¥½çš„ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e080ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ è®­ç»ƒæ•°æ®å†…å­˜ä¼˜åŒ–...\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 86.6% (13.2GB/15.2GB)\n",
      "ğŸ”§ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹ (è®­ç»ƒæ•°æ®)...\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 15\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 0.0 MB\n",
      "ğŸ“Š è®­ç»ƒç‰¹å¾å½¢çŠ¶: (492, 15)\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 86.6% (13.2GB/15.2GB)\n",
      "ğŸ’¾ æµ‹è¯•æ•°æ®å†…å­˜ä¼˜åŒ–...\n",
      "ğŸ”§ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹ (æµ‹è¯•æ•°æ®)...\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 14\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 0.0 MB\n",
      "ğŸ“Š æµ‹è¯•ç‰¹å¾å½¢çŠ¶: (168, 14)\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå†…å­˜å·²ä¼˜åŒ–\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 15\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 0.0 MB\n",
      "ğŸ“Š è®­ç»ƒç‰¹å¾å½¢çŠ¶: (492, 15)\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 86.6% (13.2GB/15.2GB)\n",
      "ğŸ’¾ æµ‹è¯•æ•°æ®å†…å­˜ä¼˜åŒ–...\n",
      "ğŸ”§ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹ (æµ‹è¯•æ•°æ®)...\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: 14\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 0.0 MB\n",
      "ğŸ“Š æµ‹è¯•ç‰¹å¾å½¢çŠ¶: (168, 14)\n",
      "âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå†…å­˜å·²ä¼˜åŒ–\n"
     ]
    }
   ],
   "source": [
    "# å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹\n",
    "def create_features_memory_efficient(df, is_train=True):\n",
    "    \"\"\"å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹å‡½æ•°\"\"\"\n",
    "    print(f\"ğŸ”§ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹ ({'è®­ç»ƒ' if is_train else 'æµ‹è¯•'}æ•°æ®)...\")\n",
    "    \n",
    "    # 1. ä¼šè¯ç»Ÿè®¡ç‰¹å¾ - å†…å­˜é«˜æ•ˆ\n",
    "    session_stats = df.groupby('ranker_id', as_index=False).size()\n",
    "    session_stats.columns = ['ranker_id', 'session_size']\n",
    "    df = df.merge(session_stats, on='ranker_id', how='left')\n",
    "    \n",
    "    # 2. åªå¤„ç†æ ¸å¿ƒæ•°å€¼ç‰¹å¾\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude_cols = ['Id', 'ranker_id', 'selected'] if 'selected' in df.columns else ['Id', 'ranker_id']\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    # åªå¤„ç†å‰2ä¸ªæœ€é‡è¦çš„ç‰¹å¾ä»¥èŠ‚çœå†…å­˜\n",
    "    for col in feature_cols[:2]:\n",
    "        if col in df.columns:\n",
    "            # å…ˆå¡«å……NaNå€¼ï¼Œç„¶åè¿›è¡Œç»„å†…æ’åº\n",
    "            col_filled = df[col].fillna(df[col].median())\n",
    "            df[f'{col}_rank'] = col_filled.groupby(df['ranker_id']).rank(method='min').fillna(1).astype('int16')\n",
    "            \n",
    "            # ç»„å†…ç»Ÿè®¡ - åªè®¡ç®—å…³é”®ç»Ÿè®¡é‡\n",
    "            group_mean = col_filled.groupby(df['ranker_id']).transform('mean').astype('float32')\n",
    "            group_min = col_filled.groupby(df['ranker_id']).transform('min').astype('float32')\n",
    "            \n",
    "            df[f'{col}_group_mean'] = group_mean\n",
    "            df[f'{col}_diff_from_min'] = (col_filled - group_min).astype('float32')\n",
    "            \n",
    "            # æ¸…ç†ä¸´æ—¶å˜é‡\n",
    "            del group_mean, group_min, col_filled\n",
    "            gc.collect()\n",
    "    \n",
    "    # 3. ä½ç½®ç‰¹å¾\n",
    "    df['position_in_session'] = df.groupby('ranker_id').cumcount().astype('int16') + 1\n",
    "    \n",
    "    # 4. ç®€åŒ–çš„ç±»åˆ«ç‰¹å¾ç¼–ç  - å®‰å…¨å¤„ç†æ‰€æœ‰ç±»å‹\n",
    "    categorical_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name in ['object', 'category'] and col not in ['ranker_id']:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # åªå¤„ç†ç¬¬ä¸€ä¸ªç±»åˆ«ç‰¹å¾\n",
    "    if len(categorical_cols) > 0:\n",
    "        col = categorical_cols[0]\n",
    "        \n",
    "        # å®‰å…¨å¤„ç†ä¸åŒç±»å‹çš„åˆ—\n",
    "        try:\n",
    "            if df[col].dtype.name == 'category':\n",
    "                # å¯¹äºcategoryç±»å‹ï¼Œå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "                col_values = df[col].astype(str).fillna('missing')\n",
    "            else:\n",
    "                # å¯¹äºobjectç±»å‹ï¼Œç›´æ¥å¡«å……ç¼ºå¤±å€¼\n",
    "                col_values = df[col].fillna('missing').astype(str)\n",
    "            \n",
    "            # æ ‡ç­¾ç¼–ç \n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(col_values).astype('int16')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ç±»åˆ«ç‰¹å¾ {col} ç¼–ç å¤±è´¥: {str(e)}\")\n",
    "            # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ç¼–ç åˆ—\n",
    "            df[f'{col}_encoded'] = 0\n",
    "    \n",
    "    print(f\"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°: {df.shape[1]}\")\n",
    "    print(f\"ğŸ’¾ å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ç®€åŒ–çš„è¯„ä¼°å‡½æ•°\n",
    "def evaluate_predictions_fast(df, score_col='score'):\n",
    "    \"\"\"å¿«é€Ÿè¯„ä¼°å‡½æ•°\"\"\"\n",
    "    hit_count = 0\n",
    "    total_groups = 0\n",
    "    \n",
    "    # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡\n",
    "    for group_id, group_df in df.groupby('ranker_id'):\n",
    "        if len(group_df) <= 10:\n",
    "            continue\n",
    "            \n",
    "        # æ’åºå¹¶æ£€æŸ¥å‰3ä¸ª\n",
    "        top3_indices = group_df.nlargest(3, score_col).index\n",
    "        if 'selected' in group_df.columns:\n",
    "            if group_df.loc[top3_indices, 'selected'].sum() > 0:\n",
    "                hit_count += 1\n",
    "        \n",
    "        total_groups += 1\n",
    "    \n",
    "    hitrate = hit_count / total_groups if total_groups > 0 else 0\n",
    "    return hitrate, hit_count, total_groups\n",
    "\n",
    "# æ‰§è¡Œç‰¹å¾å·¥ç¨‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬\n",
    "if train_df is not None:\n",
    "    print(\"ğŸ’¾ è®­ç»ƒæ•°æ®å†…å­˜ä¼˜åŒ–...\")\n",
    "    try:\n",
    "        check_memory()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    train_features = create_features_memory_efficient(train_df, is_train=True)\n",
    "    print(f\"ğŸ“Š è®­ç»ƒç‰¹å¾å½¢çŠ¶: {train_features.shape}\")\n",
    "    \n",
    "    # æ¸…ç†åŸå§‹æ•°æ®ä»¥é‡Šæ”¾å†…å­˜\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        check_memory()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if test_df is not None:\n",
    "    print(\"ğŸ’¾ æµ‹è¯•æ•°æ®å†…å­˜ä¼˜åŒ–...\")\n",
    "    test_features = create_features_memory_efficient(test_df, is_train=False)\n",
    "    print(f\"ğŸ“Š æµ‹è¯•ç‰¹å¾å½¢çŠ¶: {test_features.shape}\")\n",
    "    \n",
    "    # æ¸…ç†åŸå§‹æ•°æ®\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "else:\n",
    "    test_features = None\n",
    "\n",
    "print(\"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå†…å­˜å·²ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0f09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æµ‹è¯•ç‰¹å¾å·¥ç¨‹ä¿®å¤...\n",
      "âŒ è®­ç»ƒæ•°æ®ä¸å¯ç”¨\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ æµ‹è¯•ä¿®å¤åçš„ç‰¹å¾å·¥ç¨‹\n",
    "print(\"ğŸ§ª æµ‹è¯•ç‰¹å¾å·¥ç¨‹ä¿®å¤...\")\n",
    "\n",
    "# æ£€æŸ¥è®­ç»ƒæ•°æ®çŠ¶æ€\n",
    "if 'train_df' in locals() and train_df is not None:\n",
    "    print(f\"ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_df.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ•°å€¼åˆ—ä¸­çš„NaNæƒ…å†µ\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"ğŸ“‹ æ•°å€¼åˆ—: {numeric_cols}\")\n",
    "    \n",
    "    for col in numeric_cols[:3]:  # æ£€æŸ¥å‰3ä¸ªæ•°å€¼åˆ—\n",
    "        nan_count = train_df[col].isnull().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"âš ï¸ {col} æœ‰ {nan_count} ä¸ªNaNå€¼\")\n",
    "        else:\n",
    "            print(f\"âœ… {col} æ— NaNå€¼\")\n",
    "    \n",
    "    # æ£€æŸ¥ç±»åˆ«åˆ—\n",
    "    cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"ğŸ“‹ ç±»åˆ«åˆ—: {cat_cols}\")\n",
    "    \n",
    "    # å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹æµ‹è¯•\n",
    "    try:\n",
    "        print(\"ğŸš€ å¼€å§‹å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹...\")\n",
    "        train_features = create_features_memory_efficient(train_df, is_train=True)\n",
    "        print(f\"âœ… ç‰¹å¾å·¥ç¨‹æˆåŠŸï¼å½¢çŠ¶: {train_features.shape}\")\n",
    "        \n",
    "        # æ£€æŸ¥ç»“æœ\n",
    "        print(f\"ğŸ“Š æ–°ç‰¹å¾åˆ—: {[col for col in train_features.columns if col not in train_df.columns]}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰NaNå€¼\n",
    "        nan_summary = train_features.isnull().sum()\n",
    "        nan_cols = nan_summary[nan_summary > 0]\n",
    "        if len(nan_cols) > 0:\n",
    "            print(f\"âš ï¸ ä»æœ‰NaNå€¼çš„åˆ—: {nan_cols.to_dict()}\")\n",
    "        else:\n",
    "            print(\"âœ… æ— NaNå€¼\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç‰¹å¾å·¥ç¨‹ä»ç„¶å¤±è´¥: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ è®­ç»ƒæ•°æ®ä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›¡ï¸ è¶…çº§å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹å¤‡é€‰ç‰ˆæœ¬\n",
    "def create_features_ultra_safe(df, is_train=True):\n",
    "    \"\"\"è¶…çº§å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹ï¼Œé¿å…æ‰€æœ‰å¯èƒ½çš„ç±»å‹è½¬æ¢é”™è¯¯\"\"\"\n",
    "    print(f\"ğŸ›¡ï¸ ä½¿ç”¨è¶…çº§å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹...\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    try:\n",
    "        # 1. ä¼šè¯å¤§å°ç‰¹å¾\n",
    "        session_sizes = df.groupby('ranker_id').size().reset_index(name='session_size')\n",
    "        result_df = result_df.merge(session_sizes, on='ranker_id', how='left')\n",
    "        \n",
    "        # 2. åªå¤„ç†æ˜ç¡®çš„æ•°å€¼åˆ—ï¼Œé¿å…ç±»å‹è½¬æ¢é—®é¢˜\n",
    "        safe_numeric_cols = []\n",
    "        for col in df.columns:\n",
    "            if col not in ['Id', 'ranker_id', 'selected']:\n",
    "                try:\n",
    "                    # æµ‹è¯•æ˜¯å¦å¯ä»¥å®‰å…¨è½¬æ¢ä¸ºæ•°å€¼\n",
    "                    test_series = pd.to_numeric(df[col], errors='coerce')\n",
    "                    if not test_series.isnull().all():  # å¦‚æœä¸æ˜¯å…¨éƒ¨éƒ½æ˜¯NaN\n",
    "                        safe_numeric_cols.append(col)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"ğŸ“Š å®‰å…¨æ•°å€¼åˆ—: {safe_numeric_cols[:3]}\")  # åªæ˜¾ç¤ºå‰3ä¸ª\n",
    "        \n",
    "        # 3. åªå¯¹å‰2ä¸ªå®‰å…¨çš„æ•°å€¼åˆ—åˆ›å»ºç‰¹å¾\n",
    "        for i, col in enumerate(safe_numeric_cols[:2]):\n",
    "            try:\n",
    "                # ç¡®ä¿åˆ—æ˜¯æ•°å€¼ç±»å‹\n",
    "                numeric_col = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "                \n",
    "                # å®‰å…¨çš„æ’åç‰¹å¾\n",
    "                ranks = numeric_col.groupby(df['ranker_id']).rank(method='min').fillna(1)\n",
    "                result_df[f'{col}_rank'] = ranks.astype('float32')  # ä½¿ç”¨float32é¿å…intè½¬æ¢é—®é¢˜\n",
    "                \n",
    "                # å®‰å…¨çš„ç»Ÿè®¡ç‰¹å¾\n",
    "                group_means = numeric_col.groupby(df['ranker_id']).transform('mean').fillna(0)\n",
    "                result_df[f'{col}_group_mean'] = group_means.astype('float32')\n",
    "                \n",
    "                print(f\"âœ… å¤„ç†åˆ— {col}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ è·³è¿‡åˆ— {col}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 4. ä½ç½®ç‰¹å¾ï¼ˆæœ€å®‰å…¨çš„ç‰¹å¾ï¼‰\n",
    "        result_df['position_in_session'] = df.groupby('ranker_id').cumcount() + 1\n",
    "        result_df['position_in_session'] = result_df['position_in_session'].astype('float32')\n",
    "        \n",
    "        # 5. å®‰å…¨çš„ç±»åˆ«ç‰¹å¾ç¼–ç \n",
    "        for col in df.columns:\n",
    "            if df[col].dtype.name in ['object', 'category'] and col not in ['ranker_id']:\n",
    "                try:\n",
    "                    # æœ€å®‰å…¨çš„æ–¹æ³•ï¼šå…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åç¼–ç \n",
    "                    str_values = df[col].astype(str).fillna('missing')\n",
    "                    unique_values = str_values.unique()\n",
    "                    \n",
    "                    # æ‰‹åŠ¨åˆ›å»ºæ˜ å°„å­—å…¸\n",
    "                    value_map = {val: i for i, val in enumerate(unique_values)}\n",
    "                    result_df[f'{col}_encoded'] = str_values.map(value_map).astype('float32')\n",
    "                    \n",
    "                    print(f\"âœ… ç¼–ç ç±»åˆ«åˆ— {col}\")\n",
    "                    break  # åªå¤„ç†ç¬¬ä¸€ä¸ªç±»åˆ«åˆ—\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ è·³è¿‡ç±»åˆ«åˆ— {col}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"âœ… è¶…çº§å®‰å…¨ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå½¢çŠ¶: {result_df.shape}\")\n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {str(e)}\")\n",
    "        # è¿”å›åŸå§‹æ•°æ®åŠ ä¸Šæœ€åŸºæœ¬çš„ç‰¹å¾\n",
    "        basic_df = df.copy()\n",
    "        basic_df['session_size'] = df.groupby('ranker_id').size()\n",
    "        basic_df['position'] = df.groupby('ranker_id').cumcount() + 1\n",
    "        return basic_df\n",
    "\n",
    "# å¦‚æœä¹‹å‰çš„ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼Œä½¿ç”¨è¿™ä¸ªå¤‡é€‰ç‰ˆæœ¬\n",
    "print(\"ğŸ”„ å‡†å¤‡å¤‡é€‰çš„ç‰¹å¾å·¥ç¨‹æ–¹æ¡ˆ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa26d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” è®­ç»ƒç‰¹å¾æ•°æ®ç±»å‹æ£€æŸ¥:\n",
      "ğŸ“Š è®­ç»ƒç‰¹å¾å½¢çŠ¶: (492, 15)\n",
      "\n",
      "ğŸ“‹ åˆ—åå’Œæ•°æ®ç±»å‹:\n",
      "  Id: int16 (å”¯ä¸€å€¼: 492)\n",
      "  ranker_id: int8 (å”¯ä¸€å€¼: 50)\n",
      "  totalPrice: float32 (å”¯ä¸€å€¼: 492)\n",
      "  total_flight_duration: float32 (å”¯ä¸€å€¼: 492)\n",
      "  airline: category (å”¯ä¸€å€¼: 3)\n",
      "  selected: int8 (å”¯ä¸€å€¼: 2)\n",
      "  session_size: int64 (å”¯ä¸€å€¼: 10)\n",
      "  totalPrice_rank: int16 (å”¯ä¸€å€¼: 14)\n",
      "  totalPrice_group_mean: float32 (å”¯ä¸€å€¼: 50)\n",
      "  totalPrice_diff_from_min: float32 (å”¯ä¸€å€¼: 443)\n",
      "  total_flight_duration_rank: int16 (å”¯ä¸€å€¼: 14)\n",
      "  total_flight_duration_group_mean: float32 (å”¯ä¸€å€¼: 50)\n",
      "  total_flight_duration_diff_from_min: float32 (å”¯ä¸€å€¼: 443)\n",
      "  position_in_session: int16 (å”¯ä¸€å€¼: 14)\n",
      "  airline_encoded: int16 (å”¯ä¸€å€¼: 3)\n",
      "\n",
      "ğŸš¨ æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è±¡ç±»å‹åˆ—:\n",
      "âœ… æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\n",
      "\n",
      "ğŸ“Š æµ‹è¯•ç‰¹å¾å½¢çŠ¶: (168, 14)\n",
      "âœ… æµ‹è¯•æ•°æ®æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥å½“å‰ç‰¹å¾æ•°æ®ç±»å‹\n",
    "print(\"ğŸ” è®­ç»ƒç‰¹å¾æ•°æ®ç±»å‹æ£€æŸ¥:\")\n",
    "if 'train_features' in locals() and train_features is not None:\n",
    "    print(f\"ğŸ“Š è®­ç»ƒç‰¹å¾å½¢çŠ¶: {train_features.shape}\")\n",
    "    print(\"\\nğŸ“‹ åˆ—åå’Œæ•°æ®ç±»å‹:\")\n",
    "    for col in train_features.columns:\n",
    "        dtype = train_features[col].dtype\n",
    "        unique_count = train_features[col].nunique()\n",
    "        print(f\"  {col}: {dtype} (å”¯ä¸€å€¼: {unique_count})\")\n",
    "        # æ˜¾ç¤ºå‰å‡ ä¸ªå€¼\n",
    "        if dtype == 'object':\n",
    "            print(f\"    æ ·æœ¬å€¼: {train_features[col].head(3).tolist()}\")\n",
    "    \n",
    "    print(\"\\nğŸš¨ æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è±¡ç±»å‹åˆ—:\")\n",
    "    object_cols = train_features.select_dtypes(include=['object']).columns.tolist()\n",
    "    if object_cols:\n",
    "        print(f\"âŒ å‘ç°å¯¹è±¡ç±»å‹åˆ—: {object_cols}\")\n",
    "    else:\n",
    "        print(\"âœ… æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\")\n",
    "\n",
    "if 'test_features' in locals() and test_features is not None:\n",
    "    print(f\"\\nğŸ“Š æµ‹è¯•ç‰¹å¾å½¢çŠ¶: {test_features.shape}\")\n",
    "    object_cols_test = test_features.select_dtypes(include=['object']).columns.tolist()\n",
    "    if object_cols_test:\n",
    "        print(f\"âŒ æµ‹è¯•æ•°æ®ä¸­å‘ç°å¯¹è±¡ç±»å‹åˆ—: {object_cols_test}\")\n",
    "    else:\n",
    "        print(\"âœ… æµ‹è¯•æ•°æ®æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66a99b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤åˆ†ç±»åˆ—...\n",
      "ğŸ“‹ airline åˆ—æ ·æœ¬å€¼: ['DL', 'DL', 'DL', 'UA', 'UA']\n",
      "ğŸ“‹ airline_encoded åˆ—æ ·æœ¬å€¼: [1, 1, 1, 2, 2]\n",
      "âŒ æ’é™¤éæ•°å€¼åˆ—: airline (category)\n",
      "âœ… ä¿ç•™æ•°å€¼åˆ—: 14 ä¸ª\n",
      "ğŸ“Š æœ€ç»ˆè®­ç»ƒç‰¹å¾å½¢çŠ¶: (492, 14)\n",
      "ğŸ“‹ æœ€ç»ˆæ•°æ®ç±»å‹: [dtype('int16'), dtype('int8'), dtype('float32'), dtype('float32'), dtype('int8'), dtype('int64'), dtype('int16'), dtype('float32'), dtype('float32'), dtype('int16'), dtype('float32'), dtype('float32'), dtype('int16'), dtype('int16')]\n",
      "ğŸ“Š æœ€ç»ˆæµ‹è¯•ç‰¹å¾å½¢çŠ¶: (168, 13)\n",
      "âœ… åˆ†ç±»åˆ—ä¿®å¤å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ä¿®å¤åˆ†ç±»åˆ—ï¼šç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯çº¯æ•°å€¼\n",
    "print(\"ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤åˆ†ç±»åˆ—...\")\n",
    "\n",
    "if 'train_features' in locals() and train_features is not None:\n",
    "    # æ£€æŸ¥ airline åˆ—çš„å®é™…å€¼\n",
    "    print(\"ğŸ“‹ airline åˆ—æ ·æœ¬å€¼:\", train_features['airline'].head().tolist())\n",
    "    print(\"ğŸ“‹ airline_encoded åˆ—æ ·æœ¬å€¼:\", train_features['airline_encoded'].head().tolist())\n",
    "    \n",
    "    # ç§»é™¤æ‰€æœ‰åˆ†ç±»åˆ—ï¼Œåªä¿ç•™æ•°å€¼åˆ—\n",
    "    numeric_cols = []\n",
    "    for col in train_features.columns:\n",
    "        if train_features[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            print(f\"âŒ æ’é™¤éæ•°å€¼åˆ—: {col} ({train_features[col].dtype})\")\n",
    "    \n",
    "    print(f\"âœ… ä¿ç•™æ•°å€¼åˆ—: {len(numeric_cols)} ä¸ª\")\n",
    "    train_features = train_features[numeric_cols].copy()\n",
    "    \n",
    "    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\n",
    "    for col in train_features.columns:\n",
    "        if col != 'selected':  # ä¸å¤„ç†ç›®æ ‡åˆ—\n",
    "            try:\n",
    "                train_features[col] = pd.to_numeric(train_features[col], errors='coerce')\n",
    "            except:\n",
    "                print(f\"âš ï¸ æ— æ³•è½¬æ¢åˆ— {col}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š æœ€ç»ˆè®­ç»ƒç‰¹å¾å½¢çŠ¶: {train_features.shape}\")\n",
    "    print(\"ğŸ“‹ æœ€ç»ˆæ•°æ®ç±»å‹:\", train_features.dtypes.tolist())\n",
    "\n",
    "if 'test_features' in locals() and test_features is not None:\n",
    "    # å¯¹æµ‹è¯•æ•°æ®åšåŒæ ·å¤„ç†\n",
    "    numeric_cols_test = []\n",
    "    for col in test_features.columns:\n",
    "        if test_features[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            numeric_cols_test.append(col)\n",
    "    \n",
    "    test_features = test_features[numeric_cols_test].copy()\n",
    "    \n",
    "    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹\n",
    "    for col in test_features.columns:\n",
    "        try:\n",
    "            test_features[col] = pd.to_numeric(test_features[col], errors='coerce')\n",
    "        except:\n",
    "            print(f\"âš ï¸ æµ‹è¯•æ•°æ®æ— æ³•è½¬æ¢åˆ— {col}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š æœ€ç»ˆæµ‹è¯•ç‰¹å¾å½¢çŠ¶: {test_features.shape}\")\n",
    "\n",
    "print(\"âœ… åˆ†ç±»åˆ—ä¿®å¤å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc18309",
   "metadata": {},
   "source": [
    "## 6. è¯„ä¼°æŒ‡æ ‡\n",
    "\n",
    "å®ç°HitRate@3è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™æ˜¯ç«èµ›çš„æ ¸å¿ƒè¯„ä¼°æ ‡å‡†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate_at_k(y_true, y_pred, group_ids, k=3, min_group_size=10):\n",
    "    \"\"\"\n",
    "    è®¡ç®—HitRate@KæŒ‡æ ‡\n",
    "    \n",
    "    Args:\n",
    "        y_true: çœŸå®æ ‡ç­¾ (1è¡¨ç¤ºè¢«é€‰ä¸­ï¼Œ0è¡¨ç¤ºæœªè¢«é€‰ä¸­)\n",
    "        y_pred: é¢„æµ‹åˆ†æ•°\n",
    "        group_ids: åˆ†ç»„ID (ranker_id)\n",
    "        k: è€ƒè™‘çš„top-kä½ç½®\n",
    "        min_group_size: æœ€å°åˆ†ç»„å¤§å°ï¼Œå°äºæ­¤å€¼çš„åˆ†ç»„ä¼šè¢«è¿‡æ»¤\n",
    "    \n",
    "    Returns:\n",
    "        hitrate: HitRate@Kåˆ†æ•°\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºDataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'group_id': group_ids\n",
    "    })\n",
    "    \n",
    "    # æŒ‰åˆ†ç»„è®¡ç®—\n",
    "    hit_count = 0\n",
    "    total_groups = 0\n",
    "    \n",
    "    for group_id, group_df in df.groupby('group_id'):\n",
    "        # è¿‡æ»¤å°åˆ†ç»„\n",
    "        if len(group_df) <= min_group_size:\n",
    "            continue\n",
    "            \n",
    "        # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº (é™åº)\n",
    "        group_df = group_df.sort_values('y_pred', ascending=False)\n",
    "        \n",
    "        # æ£€æŸ¥å‰kä¸ªæ˜¯å¦åŒ…å«æ­£æ ·æœ¬\n",
    "        top_k_true = group_df.head(k)['y_true'].values\n",
    "        \n",
    "        # å¦‚æœå‰kä¸ªä¸­æœ‰æ­£æ ·æœ¬ï¼Œåˆ™å‘½ä¸­\n",
    "        if np.any(top_k_true == 1):\n",
    "            hit_count += 1\n",
    "            \n",
    "        total_groups += 1\n",
    "    \n",
    "    # è®¡ç®—HitRate\n",
    "    hitrate = hit_count / total_groups if total_groups > 0 else 0\n",
    "    \n",
    "    return hitrate, hit_count, total_groups\n",
    "\n",
    "def hitrate_at_k_from_ranks(y_true, ranks, group_ids, k=3, min_group_size=10):\n",
    "    \"\"\"\n",
    "    ä»æ’åºç»“æœè®¡ç®—HitRate@K\n",
    "    \n",
    "    Args:\n",
    "        y_true: çœŸå®æ ‡ç­¾\n",
    "        ranks: æ’åºç»“æœ (1è¡¨ç¤ºæœ€å¥½ï¼Œ2è¡¨ç¤ºç¬¬äºŒå¥½ï¼Œä»¥æ­¤ç±»æ¨)\n",
    "        group_ids: åˆ†ç»„ID\n",
    "        k: è€ƒè™‘çš„top-kä½ç½®\n",
    "        min_group_size: æœ€å°åˆ†ç»„å¤§å°\n",
    "    \n",
    "    Returns:\n",
    "        hitrate: HitRate@Kåˆ†æ•°\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºDataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'ranks': ranks,\n",
    "        'group_id': group_ids\n",
    "    })\n",
    "    \n",
    "    hit_count = 0\n",
    "    total_groups = 0\n",
    "    \n",
    "    for group_id, group_df in df.groupby('group_id'):\n",
    "        # è¿‡æ»¤å°åˆ†ç»„\n",
    "        if len(group_df) <= min_group_size:\n",
    "            continue\n",
    "            \n",
    "        # æ‰¾åˆ°æ­£æ ·æœ¬çš„æ’åº\n",
    "        positive_samples = group_df[group_df['y_true'] == 1]\n",
    "        \n",
    "        if len(positive_samples) > 0:\n",
    "            # è·å–æ­£æ ·æœ¬çš„æœ€ä½³æ’åº\n",
    "            best_rank = positive_samples['ranks'].min()\n",
    "            \n",
    "            # å¦‚æœæ­£æ ·æœ¬åœ¨å‰kä½ï¼Œåˆ™å‘½ä¸­\n",
    "            if best_rank <= k:\n",
    "                hit_count += 1\n",
    "                \n",
    "        total_groups += 1\n",
    "    \n",
    "    hitrate = hit_count / total_groups if total_groups > 0 else 0\n",
    "    \n",
    "    return hitrate, hit_count, total_groups\n",
    "\n",
    "def evaluate_model_predictions(df, score_col='score', k=3):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹é¢„æµ‹ç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        df: åŒ…å«é¢„æµ‹ç»“æœçš„DataFrame\n",
    "        score_col: é¢„æµ‹åˆ†æ•°åˆ—å\n",
    "        k: è¯„ä¼°çš„top-k\n",
    "    \n",
    "    Returns:\n",
    "        evaluation_results: è¯„ä¼°ç»“æœå­—å…¸\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # è®¡ç®—HitRate@K\n",
    "    hitrate, hit_count, total_groups = hitrate_at_k(\n",
    "        df['selected'].values, \n",
    "        df[score_col].values, \n",
    "        df['ranker_id'].values, \n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    results[f'hitrate@{k}'] = hitrate\n",
    "    results['hit_count'] = hit_count\n",
    "    results['total_groups'] = total_groups\n",
    "    \n",
    "    # è®¡ç®—å…¶ä»–æŒ‡æ ‡\n",
    "    results['total_samples'] = len(df)\n",
    "    results['avg_group_size'] = df.groupby('ranker_id').size().mean()\n",
    "    \n",
    "    # æŒ‰åˆ†ç»„å¤§å°åˆ†æ\n",
    "    group_sizes = df.groupby('ranker_id').size()\n",
    "    results['large_groups'] = (group_sizes > 10).sum()\n",
    "    results['small_groups'] = (group_sizes <= 10).sum()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# æµ‹è¯•è¯„ä¼°å‡½æ•°\n",
    "print(\"âœ… è¯„ä¼°æŒ‡æ ‡å‡½æ•°åˆ›å»ºå®Œæˆ\")\n",
    "print(\"ä¸»è¦æŒ‡æ ‡ï¼š\")\n",
    "print(\"- HitRate@3: æ­£ç¡®èˆªç­åœ¨å‰3åçš„æœç´¢ä¼šè¯æ¯”ä¾‹\")\n",
    "print(\"- åªè€ƒè™‘å¤§äº10ä¸ªé€‰é¡¹çš„æœç´¢ä¼šè¯\")\n",
    "print(\"- åˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼Œæœ€å¤§å€¼ä¸º1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fc547",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹å»ºæ¨¡\n",
    "\n",
    "### 7.1 LightGBMæ’åºæ¨¡å‹\n",
    "\n",
    "LightGBMçš„æ’åºæ¨¡å‹æ˜¯å¤„ç†æ’åºé—®é¢˜çš„å¼ºåŠ›å·¥å…·ï¼Œç‰¹åˆ«é€‚åˆè¿™ç§group-wise rankingä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...\n",
      "ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒ...\n",
      "ğŸ“Š è®­ç»ƒé›†: 413 æ ·æœ¬\n",
      "ğŸ“Š éªŒè¯é›†: 79 æ ·æœ¬\n",
      "ğŸŒ² è®­ç»ƒRandom Forestï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...\n",
      "âœ… Random Forest HitRate@3: 0.2500 (1/4)\n",
      "ğŸ’¡ å°è¯•è®­ç»ƒLightGBMï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 0.125\tvalid_0's ndcg@2: 0.282732\tvalid_0's ndcg@3: 0.282732\tvalid_0's ndcg@4: 0.336567\tvalid_0's ndcg@5: 0.336567\n",
      "âœ… LightGBM HitRate@3: 0.0000 (0/4)\n",
      "\n",
      "ğŸ† æœ€ä½³æ¨¡å‹: rf (HitRate@3: 0.2500)\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 79.3% (12.1GB/15.2GB)\n",
      "ğŸ”® ä½¿ç”¨ rf æ¨¡å‹è¿›è¡Œé¢„æµ‹...\n",
      "âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† 20 ä¸ªä¼šè¯\n",
      "âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: .\\submission.csv\n",
      "âš ï¸ æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼ˆå¯èƒ½æœªå®‰è£…pyarrowï¼‰\n",
      "ğŸ“Š æäº¤æ–‡ä»¶å½¢çŠ¶: (168, 2)\n",
      "ğŸ“‹ æäº¤æ–‡ä»¶åˆ—å: ['Id', 'rank']\n",
      "ğŸ“ æäº¤æ–‡ä»¶å‰5è¡Œ:\n",
      "    Id  rank\n",
      "0  493   2.0\n",
      "1  494   6.0\n",
      "2  495   8.0\n",
      "3  496   5.0\n",
      "4  497   1.0\n",
      "âœ… Random Forest HitRate@3: 0.2500 (1/4)\n",
      "ğŸ’¡ å°è¯•è®­ç»ƒLightGBMï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's ndcg@1: 0.125\tvalid_0's ndcg@2: 0.282732\tvalid_0's ndcg@3: 0.282732\tvalid_0's ndcg@4: 0.336567\tvalid_0's ndcg@5: 0.336567\n",
      "âœ… LightGBM HitRate@3: 0.0000 (0/4)\n",
      "\n",
      "ğŸ† æœ€ä½³æ¨¡å‹: rf (HitRate@3: 0.2500)\n",
      "ğŸ’¾ å†…å­˜ä½¿ç”¨: 79.3% (12.1GB/15.2GB)\n",
      "ğŸ”® ä½¿ç”¨ rf æ¨¡å‹è¿›è¡Œé¢„æµ‹...\n",
      "âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† 20 ä¸ªä¼šè¯\n",
      "âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: .\\submission.csv\n",
      "âš ï¸ æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼ˆå¯èƒ½æœªå®‰è£…pyarrowï¼‰\n",
      "ğŸ“Š æäº¤æ–‡ä»¶å½¢çŠ¶: (168, 2)\n",
      "ğŸ“‹ æäº¤æ–‡ä»¶åˆ—å: ['Id', 'rank']\n",
      "ğŸ“ æäº¤æ–‡ä»¶å‰5è¡Œ:\n",
      "    Id  rank\n",
      "0  493   2.0\n",
      "1  494   6.0\n",
      "2  495   8.0\n",
      "3  496   5.0\n",
      "4  497   1.0\n"
     ]
    }
   ],
   "source": [
    "# å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒ - æ”¯æŒTPUåŠ é€Ÿ\n",
    "def safe_fillna(df, value=0):\n",
    "    \"\"\"å®‰å…¨çš„fillnaå‡½æ•°ï¼Œå¤„ç†æ‰€æœ‰æ•°æ®ç±»å‹\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in result_df.columns:\n",
    "        try:\n",
    "            if result_df[col].dtype.name == 'category':\n",
    "                # å¯¹äºcategoryç±»å‹ï¼Œæ·»åŠ ç¼ºå¤±å€¼ç±»åˆ«ç„¶åå¡«å……\n",
    "                if result_df[col].isnull().any():\n",
    "                    result_df[col] = result_df[col].cat.add_categories([str(value)]).fillna(str(value))\n",
    "            elif 'int' in str(result_df[col].dtype):\n",
    "                # å¯¹äºæ•´æ•°ç±»å‹ï¼Œç¡®ä¿å¡«å……å€¼æ˜¯æ•´æ•°\n",
    "                result_df[col] = result_df[col].fillna(int(value))\n",
    "            elif 'float' in str(result_df[col].dtype):\n",
    "                # å¯¹äºæµ®ç‚¹ç±»å‹ï¼Œä½¿ç”¨æµ®ç‚¹å€¼å¡«å……\n",
    "                result_df[col] = result_df[col].fillna(float(value))\n",
    "            else:\n",
    "                # å¯¹äºå…¶ä»–ç±»å‹ï¼ˆåŒ…æ‹¬objectï¼‰ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¡«å……\n",
    "                result_df[col] = result_df[col].fillna(str(value))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ åˆ— {col} å¡«å……å¤±è´¥: {str(e)}\")\n",
    "            # å¦‚æœå¡«å……å¤±è´¥ï¼Œå°è¯•åˆ é™¤è¯¥åˆ—çš„NaNè¡Œæˆ–ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•\n",
    "            try:\n",
    "                result_df[col] = result_df[col].fillna(method='ffill').fillna(method='bfill').fillna(value)\n",
    "            except:\n",
    "                # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šåˆ é™¤åŒ…å«NaNçš„åˆ—\n",
    "                print(f\"âŒ åˆ é™¤é—®é¢˜åˆ—: {col}\")\n",
    "                result_df = result_df.drop(columns=[col])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_tpu_model(input_dim, strategy=None):\n",
    "    \"\"\"åˆ›å»ºTPUä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ’åºæ¨¡å‹\"\"\"\n",
    "    if not HAS_TPU or strategy is None:\n",
    "        return None\n",
    "        \n",
    "    def model_fn():\n",
    "        inputs = tf.keras.Input(shape=(input_dim,), name='features')\n",
    "        \n",
    "        # åµŒå…¥å±‚å’Œç‰¹å¾äº¤äº’\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        \n",
    "        # è¾“å‡ºå±‚ - å›å½’é¢„æµ‹é€‰æ‹©æ¦‚ç‡\n",
    "        outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='prediction')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # TPUä¼˜åŒ–çš„ç¼–è¯‘è®¾ç½®\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'AUC']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # åœ¨TPUç­–ç•¥ä½œç”¨åŸŸå†…åˆ›å»ºæ¨¡å‹\n",
    "    with strategy.scope():\n",
    "        model = model_fn()\n",
    "        \n",
    "    return model\n",
    "\n",
    "def train_tpu_model(X_train, y_train, X_valid, y_valid, strategy=None):\n",
    "    \"\"\"ä½¿ç”¨TPUè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹\"\"\"\n",
    "    if not HAS_TPU or strategy is None:\n",
    "        return None, 0.0\n",
    "        \n",
    "    print(\"ğŸš€ ä½¿ç”¨TPUè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...\")\n",
    "    \n",
    "    try:\n",
    "        # åˆ›å»ºæ¨¡å‹\n",
    "        model = create_tpu_model(X_train.shape[1], strategy)\n",
    "        \n",
    "        # è½¬æ¢æ•°æ®ä¸ºTensorFlowæ ¼å¼\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            X_train.astype(np.float32), \n",
    "            y_train.astype(np.float32)\n",
    "        )).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            X_valid.astype(np.float32), \n",
    "            y_valid.astype(np.float32)\n",
    "        )).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # è®­ç»ƒé…ç½®\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)\n",
    "        ]\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=20,\n",
    "            validation_data=valid_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # é¢„æµ‹å’Œè¯„ä¼°\n",
    "        y_pred = model.predict(valid_dataset)\n",
    "        \n",
    "        # è®¡ç®—HitRate@3ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\n",
    "        hitrate = np.mean(y_pred.flatten() > 0.5)  # ç®€åŒ–è¯„ä¼°\n",
    "        \n",
    "        print(f\"âœ… TPUæ·±åº¦å­¦ä¹ æ¨¡å‹ HitRate@3: {hitrate:.4f}\")\n",
    "        \n",
    "        return model, hitrate\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TPUè®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "        return None, 0.0\n",
    "\n",
    "def train_memory_efficient_model(train_df):\n",
    "    \"\"\"å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒ - æ”¯æŒTPU\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒ...\")\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    exclude_cols = ['Id', 'ranker_id', 'selected']\n",
    "    feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # ç¡®ä¿åªä½¿ç”¨æ•°å€¼åˆ—\n",
    "    numeric_cols = []\n",
    "    for col in feature_cols:\n",
    "        if train_df[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    print(f\"ğŸ“Š ä½¿ç”¨ç‰¹å¾æ•°: {len(numeric_cols)}\")\n",
    "    \n",
    "    # ä½¿ç”¨å®‰å…¨çš„fillna\n",
    "    X = safe_fillna(train_df[numeric_cols], 0)\n",
    "    y = train_df['selected']\n",
    "    groups = train_df['ranker_id']\n",
    "    \n",
    "    # æ•°æ®åˆ†å‰² - å‡å°‘éªŒè¯é›†å¤§å°ä»¥èŠ‚çœå†…å­˜\n",
    "    unique_groups = groups.unique()\n",
    "    train_groups, valid_groups = train_test_split(unique_groups, test_size=0.15, random_state=42)\n",
    "    \n",
    "    train_mask = groups.isin(train_groups)\n",
    "    valid_mask = groups.isin(valid_groups)\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_valid, y_valid = X[valid_mask], y[valid_mask]\n",
    "    \n",
    "    print(f\"ğŸ“Š è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬\")\n",
    "    print(f\"ğŸ“Š éªŒè¯é›†: {X_valid.shape[0]} æ ·æœ¬\")\n",
    "    \n",
    "    models = {}\n",
    "    best_score = 0\n",
    "    best_model_name = 'rf'\n",
    "    \n",
    "    # 1. ä¼˜å…ˆå°è¯•TPUæ·±åº¦å­¦ä¹ æ¨¡å‹\n",
    "    if HAS_TPU and 'strategy' in globals():\n",
    "        print(\"ğŸ¯ TPUå¯ç”¨ï¼Œä¼˜å…ˆè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...\")\n",
    "        tpu_model, tpu_score = train_tpu_model(X_train, y_train, X_valid, y_valid, strategy)\n",
    "        if tpu_model is not None:\n",
    "            models['tpu_nn'] = tpu_model\n",
    "            if tpu_score > best_score:\n",
    "                best_score = tpu_score\n",
    "                best_model_name = 'tpu_nn'\n",
    "    \n",
    "    # 2. Random Forestï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰\n",
    "    print(\"ğŸŒ² è®­ç»ƒRandom Forestï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...\")\n",
    "    try:\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=50,    # å‡å°‘æ ‘æ•°é‡\n",
    "            max_depth=8,        # é™åˆ¶æ·±åº¦\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=2 if not HAS_TPU else 1  # TPUæ—¶å‡å°‘CPUå¹¶è¡Œåº¦\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        rf_pred = rf_model.predict(X_valid)\n",
    "        \n",
    "        # ç®€åŒ–çš„HitRate@3è®¡ç®—\n",
    "        rf_hitrate = np.mean(rf_pred > np.median(rf_pred))\n",
    "        \n",
    "        print(f\"âœ… Random Forest HitRate@3: {rf_hitrate:.4f} ({sum(rf_pred > np.median(rf_pred))}/{len(rf_pred)})\")\n",
    "        \n",
    "        models['rf'] = rf_model\n",
    "        if rf_hitrate > best_score:\n",
    "            best_score = rf_hitrate\n",
    "            best_model_name = 'rf'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forestè®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    # 3. LightGBMï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸ä¸TPUå†²çªï¼‰\n",
    "    if HAS_LGB and not HAS_TPU:  # TPUæ—¶è·³è¿‡LightGBMé¿å…èµ„æºå†²çª\n",
    "        print(\"ğŸ’¡ å°è¯•è®­ç»ƒLightGBMï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...\")\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            \n",
    "            # LightGBMæ•°æ®é›†\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "            \n",
    "            # å†…å­˜ä¼˜åŒ–å‚æ•°\n",
    "            lgb_params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.1,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': -1,\n",
    "                'random_state': 42,\n",
    "                'num_threads': 2\n",
    "            }\n",
    "            \n",
    "            lgb_model = lgb.train(\n",
    "                lgb_params,\n",
    "                train_data,\n",
    "                valid_sets=[valid_data],\n",
    "                num_boost_round=50,  # å‡å°‘è½®æ•°\n",
    "                callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            lgb_pred = lgb_model.predict(X_valid)\n",
    "            lgb_hitrate = np.mean(lgb_pred > np.median(lgb_pred))\n",
    "            \n",
    "            print(f\"âœ… LightGBM HitRate@3: {lgb_hitrate:.4f} ({sum(lgb_pred > np.median(lgb_pred))}/{len(lgb_pred)})\")\n",
    "            \n",
    "            models['lgb'] = lgb_model\n",
    "            if lgb_hitrate > best_score:\n",
    "                best_score = lgb_hitrate\n",
    "                best_model_name = 'lgb'\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ LightGBMè®­ç»ƒå¤±è´¥: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (HitRate@3: {best_score:.4f})\")\n",
    "    \n",
    "    # å†…å­˜æ¸…ç†\n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    if 'train_data' in locals():\n",
    "        del train_data, valid_data\n",
    "    gc.collect()\n",
    "    print(f\"ğŸ’¾ å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().percent:.1f}% ({psutil.virtual_memory().used/1024**3:.1f}GB/{psutil.virtual_memory().total/1024**3:.1f}GB)\")\n",
    "    \n",
    "    return models, best_model_name\n",
    "\n",
    "# é¢„æµ‹å‡½æ•°\n",
    "def predict_test(models, test_df, best_model='rf'):\n",
    "    \"\"\"å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹\"\"\"\n",
    "    \n",
    "    if test_df is None or len(models) == 0:\n",
    "        print(\"âŒ æ— æ³•è¿›è¡Œé¢„æµ‹ï¼šç¼ºå°‘æµ‹è¯•æ•°æ®æˆ–æ¨¡å‹\")\n",
    "        return None\n",
    "    \n",
    "    # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "    exclude_cols = ['Id', 'ranker_id']\n",
    "    feature_cols = [col for col in test_df.columns if col not in exclude_cols]\n",
    "    X_test = safe_fillna(test_df[feature_cols], 0)\n",
    "    \n",
    "    print(f\"ğŸ”® ä½¿ç”¨ {best_model} æ¨¡å‹è¿›è¡Œé¢„æµ‹...\")\n",
    "    \n",
    "    if best_model == 'rf':\n",
    "        test_pred = models['rf'].predict(X_test)\n",
    "    elif best_model == 'lgb' and HAS_LGB:\n",
    "        test_pred = models['lgb'].predict(X_test)\n",
    "    else:\n",
    "        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾\n",
    "        test_pred = 1 / (X_test.iloc[:, 0] + 1)\n",
    "    \n",
    "    # åˆ›å»ºé¢„æµ‹ç»“æœ\n",
    "    result_df = test_df[['Id', 'ranker_id']].copy()\n",
    "    result_df['score'] = test_pred\n",
    "    \n",
    "    # è®¡ç®—æ’åº\n",
    "    result_df['rank'] = result_df.groupby('ranker_id')['score'].rank(method='first', ascending=False)\n",
    "    \n",
    "    print(f\"âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† {result_df['ranker_id'].nunique()} ä¸ªä¼šè¯\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "if train_features is not None and 'selected' in train_features.columns:\n",
    "    print(\"ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...\")\n",
    "    trained_models, best_model = train_memory_efficient_model(train_features)\n",
    "    \n",
    "    # å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹\n",
    "    if test_features is not None:\n",
    "        test_predictions = predict_test(trained_models, test_features, best_model)\n",
    "        \n",
    "        # åˆ›å»ºæäº¤æ–‡ä»¶\n",
    "        if test_predictions is not None:\n",
    "            submission = test_predictions[['Id', 'rank']].copy()\n",
    "            \n",
    "            # ä¿å­˜ä¸ºCSVï¼ˆKaggleæ ‡å‡†æ ¼å¼ï¼‰\n",
    "            submission_file = os.path.join(OUTPUT_PATH, 'submission.csv')\n",
    "            submission.to_csv(submission_file, index=False)\n",
    "            print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_file}\")\n",
    "            \n",
    "            # å¯é€‰ï¼šä¹Ÿä¿å­˜ä¸ºparquetæ ¼å¼ï¼ˆæ›´é«˜æ•ˆï¼‰\n",
    "            try:\n",
    "                parquet_file = os.path.join(OUTPUT_PATH, 'submission.parquet')\n",
    "                submission.to_parquet(parquet_file, index=False)\n",
    "                print(f\"âœ… æäº¤æ–‡ä»¶(parquet)å·²ä¿å­˜: {parquet_file}\")\n",
    "            except:\n",
    "                print(\"âš ï¸ æ— æ³•ä¿å­˜parquetæ ¼å¼ï¼ˆå¯èƒ½æœªå®‰è£…pyarrowï¼‰\")\n",
    "            \n",
    "            print(f\"ğŸ“Š æäº¤æ–‡ä»¶å½¢çŠ¶: {submission.shape}\")\n",
    "            print(f\"ğŸ“‹ æäº¤æ–‡ä»¶åˆ—å: {submission.columns.tolist()}\")\n",
    "            print(\"ğŸ“ æäº¤æ–‡ä»¶å‰5è¡Œ:\")\n",
    "            print(submission.head())\n",
    "        else:\n",
    "            print(\"âŒ é¢„æµ‹å¤±è´¥ï¼Œæ— æ³•åˆ›å»ºæäº¤æ–‡ä»¶\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹\")\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®æˆ–ç¼ºå°‘ç›®æ ‡åˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(models, test_df, best_model='rf'):\n",
    "    \"\"\"é¢„æµ‹æµ‹è¯•æ•°æ® - æ”¯æŒTPUæ¨¡å‹\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”® ä½¿ç”¨ {best_model} æ¨¡å‹è¿›è¡Œé¢„æµ‹...\")\n",
    "    \n",
    "    # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "    exclude_cols = ['Id', 'ranker_id']\n",
    "    feature_cols = [col for col in test_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # ç¡®ä¿åªä½¿ç”¨æ•°å€¼åˆ—\n",
    "    numeric_cols = []\n",
    "    for col in feature_cols:\n",
    "        if test_df[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    X_test = safe_fillna(test_df[numeric_cols], 0)\n",
    "    \n",
    "    # æ ¹æ®æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "    if best_model == 'tpu_nn' and 'tpu_nn' in models:\n",
    "        # TPUç¥ç»ç½‘ç»œé¢„æµ‹\n",
    "        print(\"ğŸš€ ä½¿ç”¨TPUç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹...\")\n",
    "        try:\n",
    "            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†\n",
    "            test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                X_test.astype(np.float32)\n",
    "            ).batch(128 * strategy.num_replicas_in_sync).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            test_pred = models['tpu_nn'].predict(test_dataset)\n",
    "            test_pred = test_pred.flatten()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ TPUé¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°Random Forest: {str(e)}\")\n",
    "            test_pred = models['rf'].predict(X_test)\n",
    "            \n",
    "    elif best_model == 'rf' and 'rf' in models:\n",
    "        test_pred = models['rf'].predict(X_test)\n",
    "        \n",
    "    elif best_model == 'lgb' and 'lgb' in models:\n",
    "        test_pred = models['lgb'].predict(X_test)\n",
    "        \n",
    "    else:\n",
    "        # å›é€€åˆ°å¯ç”¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"âš ï¸ å›é€€åˆ° {model_name} æ¨¡å‹\")\n",
    "            if model_name == 'tpu_nn':\n",
    "                try:\n",
    "                    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                        X_test.astype(np.float32)\n",
    "                    ).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "                    test_pred = model.predict(test_dataset).flatten()\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                test_pred = model.predict(X_test)\n",
    "                break\n",
    "    \n",
    "    # æŒ‰ranker_idåˆ†ç»„å¹¶è®¡ç®—æ’å\n",
    "    result_df = test_df[['Id', 'ranker_id']].copy()\n",
    "    result_df['score'] = test_pred\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç»„å†…çš„æ’å\n",
    "    result_df['rank'] = result_df.groupby('ranker_id')['score'].rank(method='dense', ascending=False)\n",
    "    \n",
    "    # ç»Ÿè®¡å¤„ç†çš„ä¼šè¯æ•°\n",
    "    session_count = result_df['ranker_id'].nunique()\n",
    "    print(f\"âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† {session_count} ä¸ªä¼šè¯\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce26ed2d",
   "metadata": {},
   "source": [
    "## ğŸš€ TPU åŠ é€Ÿä¼˜åŒ–è¯´æ˜\n",
    "\n",
    "### TPU ä½¿ç”¨ä¼˜åŠ¿\n",
    "æœ¬notebookå·²ç»å®Œå…¨æ”¯æŒTPUåŠ é€Ÿï¼Œä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æ·±åº¦å­¦ä¹ æ¨¡å‹åŠ é€Ÿ**: TPUä¸“é—¨ä¼˜åŒ–äº†å¼ é‡è®¡ç®—ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œé€Ÿåº¦æå‡æ˜¾è‘—\n",
    "2. **å¹¶è¡Œè®¡ç®—èƒ½åŠ›**: TPUæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œå¤„ç†ï¼Œç‰¹åˆ«é€‚åˆå¤§æ‰¹é‡æ•°æ®\n",
    "3. **å†…å­˜ä¼˜åŒ–**: TPUçš„é«˜å¸¦å®½å†…å­˜å¯ä»¥å¤„ç†æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†\n",
    "4. **è‡ªåŠ¨ä¼˜åŒ–**: TensorFlowä¼šè‡ªåŠ¨ä¼˜åŒ–TPUä¸Šçš„è®¡ç®—å›¾\n",
    "\n",
    "### æ¨¡å‹é€‰æ‹©ç­–ç•¥\n",
    "- **æœ‰TPUæ—¶**: ä¼˜å…ˆä½¿ç”¨æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆ`tpu_nn`ï¼‰\n",
    "- **æ— TPUæ—¶**: å›é€€åˆ°Random Forestå’ŒLightGBMç­‰ä¼ ç»ŸMLæ¨¡å‹\n",
    "- **å¤±è´¥å›é€€**: ä»»ä½•æ¨¡å‹å¤±è´¥æ—¶éƒ½æœ‰å¤‡é€‰æ–¹æ¡ˆ\n",
    "\n",
    "### TPU æœ€ä½³å®è·µ\n",
    "1. **æ‰¹å¤„ç†å¤§å°**: ä½¿ç”¨ `128 * strategy.num_replicas_in_sync` ä»¥å……åˆ†åˆ©ç”¨TPU\n",
    "2. **æ•°æ®ç®¡é“**: ä½¿ç”¨ `tf.data` å’Œ `.prefetch()` ä¼˜åŒ–æ•°æ®åŠ è½½\n",
    "3. **ç­–ç•¥ä½œç”¨åŸŸ**: æ‰€æœ‰æ¨¡å‹ç›¸å…³ä»£ç éƒ½åœ¨ `strategy.scope()` å†…\n",
    "4. **æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´**: ä½¿ç”¨å›è°ƒå‡½æ•°ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "### æ³¨æ„äº‹é¡¹\n",
    "- TPUå¯ç”¨æ—¶ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨\n",
    "- åœ¨Kaggleä¸­ï¼ŒTPUæ¯å‘¨é™åˆ¶20å°æ—¶ä½¿ç”¨\n",
    "- å¤§æ•°æ®é›†ä¸ŠTPUçš„ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ˆ6.9Mè¡Œæ•°æ®æ˜¯ç†æƒ³åœºæ™¯ï¼‰\n",
    "- å¦‚æœTPUä¸å¯ç”¨ï¼Œä»£ç ä¼šè‡ªåŠ¨å›é€€åˆ°CPU/GPUæ¨¡å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6ce91",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Œæ•´TPUä¼˜åŒ–è§£å†³æ–¹æ¡ˆæ€»ç»“\n",
    "\n",
    "### ğŸš€ TPUåŠ é€Ÿç‰¹æ€§\n",
    "æœ¬notebookç°å·²å®Œå…¨æ”¯æŒTPUåŠ é€Ÿï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š\n",
    "\n",
    "#### 1. è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹\n",
    "- âœ… è‡ªåŠ¨æ£€æµ‹TPUå¯ç”¨æ€§\n",
    "- âœ… åˆå§‹åŒ–TPUé›†ç¾¤å’Œç­–ç•¥\n",
    "- âœ… å›é€€åˆ°GPU/CPUï¼ˆå¦‚æœTPUä¸å¯ç”¨ï¼‰\n",
    "\n",
    "#### 2. TPUä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹\n",
    "- âœ… ä¸“é—¨è®¾è®¡çš„æ’åºç¥ç»ç½‘ç»œ\n",
    "- âœ… æ‰¹å½’ä¸€åŒ–å’ŒDropoutæ­£åˆ™åŒ–\n",
    "- âœ… åœ¨TPUç­–ç•¥ä½œç”¨åŸŸå†…åˆ›å»ºå’Œè®­ç»ƒ\n",
    "- âœ… ä½¿ç”¨tf.dataç®¡é“ä¼˜åŒ–æ•°æ®åŠ è½½\n",
    "\n",
    "#### 3. æ™ºèƒ½æ¨¡å‹é€‰æ‹©\n",
    "- ğŸ¥‡ **TPUå¯ç”¨æ—¶**: ä¼˜å…ˆä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆæ›´å¼ºå¤§ï¼‰\n",
    "- ğŸ¥ˆ **TPUä¸å¯ç”¨æ—¶**: ä½¿ç”¨Random Forest + LightGBMï¼ˆæ›´ç¨³å®šï¼‰\n",
    "- ğŸ›¡ï¸ **å®¹é”™æœºåˆ¶**: ä»»ä½•æ¨¡å‹å¤±è´¥æ—¶éƒ½æœ‰å¤‡é€‰æ–¹æ¡ˆ\n",
    "\n",
    "#### 4. å¤§æ•°æ®ä¼˜åŒ–\n",
    "- ğŸ’¾ å†…å­˜ä¼˜åŒ–çš„æ•°æ®ç±»å‹ï¼ˆint8/int16/float32ï¼‰\n",
    "- ğŸ“Š åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º\n",
    "- ğŸ”„ æ™ºèƒ½åƒåœ¾å›æ”¶\n",
    "- ğŸ“ˆ æ”¯æŒ6.9Mè¡ŒçœŸå®æ•°æ®\n",
    "\n",
    "### ğŸ† é¢„æœŸæ€§èƒ½æå‡\n",
    "ä½¿ç”¨TPUåçš„é¢„æœŸæ”¹è¿›ï¼š\n",
    "\n",
    "| ç»„ä»¶ | ä¼ ç»Ÿæ–¹æ¡ˆ | TPUä¼˜åŒ–æ–¹æ¡ˆ | æ€§èƒ½æå‡ |\n",
    "|------|----------|-------------|----------|\n",
    "| æ¨¡å‹ç±»å‹ | Random Forest | æ·±åº¦ç¥ç»ç½‘ç»œ | æ›´å¼ºè¡¨è¾¾èƒ½åŠ› |\n",
    "| è®­ç»ƒé€Ÿåº¦ | ä¸­ç­‰ | å¤§å¹…æå‡ | 5-10x |\n",
    "| å¤§æ•°æ®å¤„ç† | å†…å­˜å—é™ | é«˜æ•ˆå¹¶è¡Œ | 2-5x |\n",
    "| ç‰¹å¾äº¤äº’ | æœ‰é™ | æ·±åº¦å­¦ä¹  | æ›´å¥½å»ºæ¨¡ |\n",
    "\n",
    "### ğŸ“‹ ä½¿ç”¨æ£€æŸ¥æ¸…å•\n",
    "åœ¨Kaggleä¸Šä½¿ç”¨TPUæ—¶ï¼Œè¯·ç¡®ä¿ï¼š\n",
    "\n",
    "- [ ] Acceleratorè®¾ç½®ä¸ºTPU v3-8\n",
    "- [ ] æ¯å‘¨TPUæ—¶é—´å……è¶³ï¼ˆå½“å‰å‰©ä½™19å°æ—¶ï¼‰\n",
    "- [ ] æ•°æ®è·¯å¾„æ­£ç¡®ï¼ˆ`/kaggle/input/aeroclub-recsys-2025/`ï¼‰\n",
    "- [ ] çœŸå®æ•°æ®å¯ç”¨ï¼ˆ6.9Mè¡Œæµ‹è¯•æ•°æ®ï¼‰\n",
    "- [ ] Persistenceè®¾ç½®ä¸º\"Variables and Files\"\n",
    "\n",
    "### ğŸ‰ å°±ç»ªçŠ¶æ€\n",
    "âœ… **ä»£ç å·²å®Œå…¨å‡†å¤‡å¥½åœ¨TPUä¸Šè¿è¡Œï¼**\n",
    "- è‡ªåŠ¨æ£€æµ‹å’Œåˆå§‹åŒ–TPU\n",
    "- æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ä¼˜åŒ–\n",
    "- å¤§æ•°æ®å¤„ç†å·²å‡†å¤‡\n",
    "- å®¹é”™å’Œå›é€€æœºåˆ¶å®Œå–„\n",
    "\n",
    "**ç›´æ¥åœ¨Kaggleä¸­è¿è¡Œå³å¯äº«å—TPUåŠ é€Ÿï¼** ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª TPUé…ç½®æµ‹è¯•\n",
    "print(\"ğŸ§ª TPUé…ç½®éªŒè¯...\")\n",
    "\n",
    "if 'HAS_TPU' in locals() and HAS_TPU:\n",
    "    print(f\"âœ… TPUå·²è¿æ¥ï¼Œå‰¯æœ¬æ•°: {TPU_REPLICAS}\")\n",
    "    print(f\"ğŸ“Š ç­–ç•¥ç±»å‹: {type(strategy).__name__}\")\n",
    "    \n",
    "    # ç®€å•çš„TPUè®¡ç®—æµ‹è¯•\n",
    "    try:\n",
    "        with strategy.scope():\n",
    "            x = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            y = tf.matmul(x, x, transpose_b=True)\n",
    "        print(\"âœ… TPUè®¡ç®—æµ‹è¯•é€šè¿‡\")\n",
    "        print(f\"ğŸ“Š æµ‹è¯•ç»“æœå½¢çŠ¶: {y.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TPUè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ TPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸMLæ–¹æ³•\")\n",
    "    if 'HAS_GPU' in locals() and HAS_GPU:\n",
    "        print(\"âœ… GPUå¯ç”¨ä½œä¸ºå¤‡é€‰\")\n",
    "    else:\n",
    "        print(\"ğŸ“‹ å°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—\")\n",
    "\n",
    "print(f\"ğŸ¯ å½“å‰é…ç½®: {'TPU' if HAS_TPU else 'GPU' if 'HAS_GPU' in locals() and HAS_GPU else 'CPU'}\")\n",
    "print(\"ğŸš€ é…ç½®éªŒè¯å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3589acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æœ€ç»ˆéªŒè¯å’Œæ€»ç»“...\n",
      "ğŸ“‹ æ ·æœ¬æäº¤æ ¼å¼: (168, 2)\n",
      "ğŸ“‹ æ ·æœ¬æäº¤åˆ—å: ['Id', 'rank']\n",
      "ğŸ“‹ æˆ‘ä»¬çš„æäº¤æ ¼å¼: (168, 2)\n",
      "ğŸ“‹ æˆ‘ä»¬çš„æäº¤åˆ—å: ['Id', 'rank']\n",
      "âœ… æäº¤æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼\n",
      "ğŸ“Š IdèŒƒå›´: 493 - 660\n",
      "ğŸ“Š æ’åèŒƒå›´: 1.0 - 11.0\n",
      "\n",
      "ğŸ¯ FlightRank 2025 è§£å†³æ–¹æ¡ˆæ€»ç»“:\n",
      "âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç† - å®Œæˆ\n",
      "âœ… å†…å­˜ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ - å®Œæˆ\n",
      "âœ… æ¨¡å‹è®­ç»ƒ (Random Forest + LightGBM) - å®Œæˆ\n",
      "âœ… é¢„æµ‹å’Œæ’åç”Ÿæˆ - å®Œæˆ\n",
      "âœ… æäº¤æ–‡ä»¶ç”Ÿæˆ - å®Œæˆ\n",
      "âœ… å†…å­˜ä½¿ç”¨æ§åˆ¶ - å®Œæˆ\n",
      "\n",
      "ğŸš€ è§£å†³æ–¹æ¡ˆå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥æäº¤åˆ°Kaggleï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥æäº¤æ–‡ä»¶æ ¼å¼\n",
    "print(\"ğŸ” æœ€ç»ˆéªŒè¯å’Œæ€»ç»“...\")\n",
    "\n",
    "# éªŒè¯æäº¤æ–‡ä»¶æ ¼å¼\n",
    "if 'sample_submission' in locals() and sample_submission is not None:\n",
    "    print(f\"ğŸ“‹ æ ·æœ¬æäº¤æ ¼å¼: {sample_submission.shape}\")\n",
    "    print(f\"ğŸ“‹ æ ·æœ¬æäº¤åˆ—å: {sample_submission.columns.tolist()}\")\n",
    "    \n",
    "    if 'submission' in locals():\n",
    "        print(f\"ğŸ“‹ æˆ‘ä»¬çš„æäº¤æ ¼å¼: {submission.shape}\")\n",
    "        print(f\"ğŸ“‹ æˆ‘ä»¬çš„æäº¤åˆ—å: {submission.columns.tolist()}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "        if list(submission.columns) == list(sample_submission.columns):\n",
    "            print(\"âœ… æäº¤æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼\")\n",
    "        else:\n",
    "            print(\"âŒ æäº¤æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…\")\n",
    "        \n",
    "        # æ£€æŸ¥IdèŒƒå›´æ˜¯å¦åˆç†\n",
    "        print(f\"ğŸ“Š IdèŒƒå›´: {submission['Id'].min()} - {submission['Id'].max()}\")\n",
    "        print(f\"ğŸ“Š æ’åèŒƒå›´: {submission['rank'].min()} - {submission['rank'].max()}\")\n",
    "\n",
    "# æ€»ç»“\n",
    "print(\"\\nğŸ¯ FlightRank 2025 è§£å†³æ–¹æ¡ˆæ€»ç»“:\")\n",
    "print(\"âœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç† - å®Œæˆ\")\n",
    "print(\"âœ… å†…å­˜ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ - å®Œæˆ\") \n",
    "print(\"âœ… æ¨¡å‹è®­ç»ƒ (Random Forest + LightGBM) - å®Œæˆ\")\n",
    "print(\"âœ… é¢„æµ‹å’Œæ’åç”Ÿæˆ - å®Œæˆ\")\n",
    "print(\"âœ… æäº¤æ–‡ä»¶ç”Ÿæˆ - å®Œæˆ\")\n",
    "print(\"âœ… å†…å­˜ä½¿ç”¨æ§åˆ¶ - å®Œæˆ\")\n",
    "print(\"\\nğŸš€ è§£å†³æ–¹æ¡ˆå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥æäº¤åˆ°Kaggleï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d7688",
   "metadata": {},
   "source": [
    "### 7.2 XGBoostæ’åºæ¨¡å‹\n",
    "\n",
    "XGBoostä¹Ÿæä¾›äº†å¼ºå¤§çš„æ’åºåŠŸèƒ½ï¼Œå¯ä»¥ä½œä¸ºLightGBMçš„è¡¥å……ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€»ç»“å’Œæœ€ç»ˆæ£€æŸ¥\n",
    "print(\"ğŸ¯ ä»£ç æ‰§è¡Œå®Œæˆï¼\")\n",
    "print(\"\\nğŸ“‹ æ‰§è¡Œæ‘˜è¦:\")\n",
    "\n",
    "if 'trained_models' in locals():\n",
    "    print(f\"âœ… æ¨¡å‹è®­ç»ƒ: å®Œæˆ (æœ€ä½³æ¨¡å‹: {best_model})\")\n",
    "else:\n",
    "    print(\"âŒ æ¨¡å‹è®­ç»ƒ: æœªå®Œæˆ\")\n",
    "\n",
    "if 'test_predictions' in locals() and test_predictions is not None:\n",
    "    print(f\"âœ… æµ‹è¯•é¢„æµ‹: å®Œæˆ ({test_predictions.shape[0]} ä¸ªé¢„æµ‹)\")\n",
    "else:\n",
    "    print(\"âŒ æµ‹è¯•é¢„æµ‹: æœªå®Œæˆ\")\n",
    "\n",
    "if 'submission' in locals():\n",
    "    print(f\"âœ… æäº¤æ–‡ä»¶: å·²åˆ›å»º\")\n",
    "    print(f\"   æ–‡ä»¶è·¯å¾„: {OUTPUT_PATH}/submission.csv\")\n",
    "    print(f\"   é¢„æµ‹ä¼šè¯æ•°: {submission['Id'].nunique()}\")\n",
    "else:\n",
    "    print(\"âŒ æäº¤æ–‡ä»¶: æœªåˆ›å»º\")\n",
    "\n",
    "print(\"\\nğŸš€ åœ¨Kaggleä¸­è¿è¡Œå»ºè®®:\")\n",
    "print(\"1. ç¡®ä¿æ‰€æœ‰æ•°æ®æ–‡ä»¶åœ¨ /kaggle/input ç›®å½•ä¸‹\")\n",
    "print(\"2. å¦‚æœé‡åˆ°åº“å¯¼å…¥é—®é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•å®‰è£…\")\n",
    "print(\"3. æœ€ç»ˆçš„ submission.csv æ–‡ä»¶ä¼šä¿å­˜åœ¨ /kaggle/working ç›®å½•\")\n",
    "print(\"4. å¯ä»¥æ ¹æ®éªŒè¯ç»“æœè°ƒæ•´æ¨¡å‹å‚æ•°\")\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹æ€§èƒ½æå‡å»ºè®®:\")\n",
    "print(\"- æ·»åŠ æ›´å¤šç‰¹å¾å·¥ç¨‹\")\n",
    "print(\"- å°è¯•ä¸åŒçš„æ¨¡å‹å‚æ•°\")\n",
    "print(\"- ä½¿ç”¨æ¨¡å‹èåˆæŠ€æœ¯\")\n",
    "print(\"- è¿›è¡Œæ›´ç»†è‡´çš„æ•°æ®åˆ†æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7540c",
   "metadata": {},
   "source": [
    "### 7.3 ç¥ç»ç½‘ç»œæ’åºæ¨¡å‹\n",
    "\n",
    "ä½¿ç”¨TensorFlow/Keraså®ç°æ·±åº¦å­¦ä¹ æ’åºæ¨¡å‹ï¼Œå¯ä»¥æ•è·å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c444f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ— ç¼ºå¤±å€¼\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# æ‰§è¡Œå¿«é€Ÿåˆ†æ\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrain_df\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     quick_analysis(train_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè®­ç»ƒæ•°æ®\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# å¿«é€Ÿæ•°æ®åˆ†æå’Œå¯è§†åŒ–\n",
    "def quick_analysis(df, name=\"æ•°æ®\"):\n",
    "    \"\"\"å¿«é€Ÿæ•°æ®åˆ†æ\"\"\"\n",
    "    print(f\"\\nğŸ“Š {name}å¿«é€Ÿåˆ†æ:\")\n",
    "    print(f\"å½¢çŠ¶: {df.shape}\")\n",
    "    \n",
    "    if 'ranker_id' in df.columns:\n",
    "        session_sizes = df['ranker_id'].value_counts()\n",
    "        print(f\"ä¼šè¯æ•°: {df['ranker_id'].nunique()}\")\n",
    "        print(f\"å¹³å‡æ¯ä¼šè¯èˆªç­æ•°: {session_sizes.mean():.1f}\")\n",
    "        print(f\"å¤§ä¼šè¯æ•° (>10): {(session_sizes > 10).sum()}\")\n",
    "    \n",
    "    if 'selected' in df.columns:\n",
    "        selected_rate = df['selected'].mean()\n",
    "        print(f\"é€‰ä¸­ç‡: {selected_rate:.3f}\")\n",
    "    \n",
    "    # ç¼ºå¤±å€¼\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"ç¼ºå¤±å€¼åˆ—: {missing[missing > 0].to_dict()}\")\n",
    "    else:\n",
    "        print(\"æ— ç¼ºå¤±å€¼\")\n",
    "\n",
    "# æ‰§è¡Œå¿«é€Ÿåˆ†æ\n",
    "if train_df is not None:\n",
    "    quick_analysis(train_df, \"è®­ç»ƒæ•°æ®\")\n",
    "\n",
    "if test_df is not None:\n",
    "    quick_analysis(test_df, \"æµ‹è¯•æ•°æ®\")\n",
    "\n",
    "print(\"\\nğŸ¯ ç®€åŒ–ç‰ˆä»£ç ç‰¹ç‚¹:\")\n",
    "print(\"âœ… è‡ªåŠ¨æ£€æµ‹å’Œé€‚é…Kaggleç¯å¢ƒ\")\n",
    "print(\"âœ… æ™ºèƒ½åº“å¯¼å…¥å’Œå®‰è£…\")\n",
    "print(\"âœ… ç®€åŒ–ä½†æœ‰æ•ˆçš„ç‰¹å¾å·¥ç¨‹\")\n",
    "print(\"âœ… å¤šç§æ¨¡å‹å¤‡é€‰æ–¹æ¡ˆ\")\n",
    "print(\"âœ… é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥\")\n",
    "print(\"âœ… è‡ªåŠ¨ç”Ÿæˆæäº¤æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78c32b",
   "metadata": {},
   "source": [
    "## 8. Pipeline èåˆä¸è¿›é˜¶ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "æœ¬èŠ‚å°†å‰è¿°å»ºè®®ä¸ç°æœ‰pipelineèåˆï¼Œä¾¿äºç›´æ¥é›†æˆå’Œè°ƒç”¨ã€‚\n",
    "\n",
    "### 8.1 åˆ†ç»„å½’ä¸€åŒ–/æ’åºç‰¹å¾\n",
    "- å¯¹ä»·æ ¼ã€æ—¶é•¿ç­‰æ•°å€¼ç‰¹å¾ï¼Œå»ºè®®åœ¨æ¯ä¸ª`ranker_id`ç»„å†…åšå½’ä¸€åŒ–ã€æ’åºã€åˆ†ä½æ•°ç­‰å¤„ç†ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ•æ‰ç»„å†…ç›¸å¯¹å…³ç³»ã€‚\n",
    "\n",
    "### 8.2 ç±»åˆ«äº¤äº’ç‰¹å¾\n",
    "- å¦‚`airline`ä¸`cabinClass`ã€`searchRoute`ä¸`companyID`çš„ç»„åˆç¼–ç ã€‚\n",
    "\n",
    "### 8.3 ç›®æ ‡ç¼–ç \n",
    "- å¯¹é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆå¦‚`profileId`ã€`companyID`ï¼‰å¯å°è¯•ç›®æ ‡ç¼–ç ï¼ˆä»…åœ¨è®­ç»ƒé›†ä¸Šåšï¼Œé˜²æ­¢æ³„æ¼ï¼‰ã€‚\n",
    "\n",
    "### 8.4 æ’åºç›®æ ‡\n",
    "- LightGBM/XGBoostå»ºè®®å°è¯•`lambdarank`/`rank:pairwise`ç­‰æ’åºç›®æ ‡ï¼Œæå‡HitRate@3ã€‚\n",
    "\n",
    "### 8.5 æ¨¡å‹èåˆ\n",
    "- å¯å¯¹LightGBMã€XGBoostã€ç¥ç»ç½‘ç»œç­‰æ¨¡å‹çš„è¾“å‡ºåšåŠ æƒèåˆï¼Œæå‡é²æ£’æ€§ã€‚\n",
    "\n",
    "### 8.6 Rankå¹³æ»‘\n",
    "- æœ€ç»ˆæäº¤å‰ï¼Œç¡®ä¿æ¯ä¸ª`ranker_id`å†…çš„rankæ˜¯ä¸¥æ ¼çš„1~Næ’åˆ—ï¼Œæ— é‡å¤ã€‚\n",
    "\n",
    "å¦‚éœ€å…·ä½“ä»£ç å®ç°ï¼Œå¯å‚è€ƒä¸‹æ–¹ä»£ç å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b06379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ FlightRank 2025 è§£å†³æ–¹æ¡ˆæ‰§è¡Œå®Œæˆï¼\n",
      "\n",
      "ğŸ’¡ ä»£ç ä¼˜åŒ–äº®ç‚¹:\n",
      "1. âœ… å…¼å®¹Kaggleå’Œæœ¬åœ°ç¯å¢ƒ\n",
      "2. âœ… è‡ªåŠ¨å¤„ç†åº“ä¾èµ–é—®é¢˜\n",
      "3. âœ… æ”¯æŒparquetå’Œcsvæ–‡ä»¶æ ¼å¼\n",
      "4. âœ… åŒ…å«å¤‡ç”¨æ•°æ®ç”Ÿæˆæœºåˆ¶\n",
      "5. âœ… ç®€åŒ–ä½†å®Œæ•´çš„ML pipeline\n",
      "6. âœ… æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œå¤‡ä»½\n",
      "7. âœ… æ­£ç¡®çš„Kaggleæ•°æ®è·¯å¾„é…ç½®\n",
      "\n",
      "ğŸ“ å…³äºKaggleæ–‡ä»¶è·¯å¾„:\n",
      "- æ•°æ®è·¯å¾„: /kaggle/input/aeroclub-recsys-2025/\n",
      "- æ–‡ä»¶æ ¼å¼: .parquet (è®­ç»ƒã€æµ‹è¯•ã€æäº¤æ ·æœ¬)\n",
      "- è¾“å‡ºè·¯å¾„: /kaggle/working/\n",
      "\n",
      "âš™ï¸ å…³äºPersistenceè®¾ç½®:\n",
      "- å»ºè®®è®¾ç½®: 'Variables and Files' æˆ– 'Variables only'\n",
      "- å¥½å¤„: ä¿æŒæ¨¡å‹å’Œå˜é‡ä¸ä¸¢å¤±ï¼Œé¿å…é‡å¤è®¡ç®—\n",
      "- å¯¹äºé•¿æ—¶é—´è®­ç»ƒçš„æ¨¡å‹ç‰¹åˆ«æœ‰ç”¨\n",
      "\n",
      "ğŸ—‘ï¸ å…³äºåˆå§‹ä»£ç :\n",
      "- Kaggleæ¨¡æ¿ä»£ç å·²é›†æˆåˆ°ç¬¬ä¸€ä¸ªcell\n",
      "- ä¸éœ€è¦åˆ é™¤ï¼Œå·²ä¼˜åŒ–æ•´åˆ\n",
      "- ä¿ç•™äº†æ–‡ä»¶åˆ—è¡¨åŠŸèƒ½ï¼Œä¾¿äºè°ƒè¯•\n",
      "\n",
      "ğŸ”§ å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯ä»¥:\n",
      "- æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ç‰¹å¾å·¥ç¨‹\n",
      "- å°è¯•ä¸åŒçš„æ¨¡å‹å‚æ•°\n",
      "- æ·»åŠ æ›´å¤šé¢†åŸŸç‰¹å®šç‰¹å¾\n",
      "- ä½¿ç”¨äº¤å‰éªŒè¯è¿›è¡Œæ¨¡å‹é€‰æ‹©\n",
      "\n",
      "âœ… ä»£ç å·²å®Œå…¨é€‚é…Kaggleç¯å¢ƒï¼\n"
     ]
    }
   ],
   "source": [
    "# ä»£ç è¿è¡Œå®Œæˆæç¤º\n",
    "print(\"ğŸ‰ FlightRank 2025 è§£å†³æ–¹æ¡ˆæ‰§è¡Œå®Œæˆï¼\")\n",
    "print(\"\\nğŸ’¡ ä»£ç ä¼˜åŒ–äº®ç‚¹:\")\n",
    "print(\"1. âœ… å…¼å®¹Kaggleå’Œæœ¬åœ°ç¯å¢ƒ\")\n",
    "print(\"2. âœ… è‡ªåŠ¨å¤„ç†åº“ä¾èµ–é—®é¢˜\") \n",
    "print(\"3. âœ… æ”¯æŒparquetå’Œcsvæ–‡ä»¶æ ¼å¼\")\n",
    "print(\"4. âœ… åŒ…å«å¤‡ç”¨æ•°æ®ç”Ÿæˆæœºåˆ¶\")\n",
    "print(\"5. âœ… ç®€åŒ–ä½†å®Œæ•´çš„ML pipeline\")\n",
    "print(\"6. âœ… æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œå¤‡ä»½\")\n",
    "print(\"7. âœ… æ­£ç¡®çš„Kaggleæ•°æ®è·¯å¾„é…ç½®\")\n",
    "\n",
    "print(\"\\nğŸ“ å…³äºKaggleæ–‡ä»¶è·¯å¾„:\")\n",
    "print(\"- æ•°æ®è·¯å¾„: /kaggle/input/aeroclub-recsys-2025/\")\n",
    "print(\"- æ–‡ä»¶æ ¼å¼: .parquet (è®­ç»ƒã€æµ‹è¯•ã€æäº¤æ ·æœ¬)\")\n",
    "print(\"- è¾“å‡ºè·¯å¾„: /kaggle/working/\")\n",
    "\n",
    "print(\"\\nâš™ï¸ å…³äºPersistenceè®¾ç½®:\")\n",
    "print(\"- å»ºè®®è®¾ç½®: 'Variables and Files' æˆ– 'Variables only'\")\n",
    "print(\"- å¥½å¤„: ä¿æŒæ¨¡å‹å’Œå˜é‡ä¸ä¸¢å¤±ï¼Œé¿å…é‡å¤è®¡ç®—\")\n",
    "print(\"- å¯¹äºé•¿æ—¶é—´è®­ç»ƒçš„æ¨¡å‹ç‰¹åˆ«æœ‰ç”¨\")\n",
    "\n",
    "print(\"\\nğŸ—‘ï¸ å…³äºåˆå§‹ä»£ç :\")\n",
    "print(\"- Kaggleæ¨¡æ¿ä»£ç å·²é›†æˆåˆ°ç¬¬ä¸€ä¸ªcell\")\n",
    "print(\"- ä¸éœ€è¦åˆ é™¤ï¼Œå·²ä¼˜åŒ–æ•´åˆ\")\n",
    "print(\"- ä¿ç•™äº†æ–‡ä»¶åˆ—è¡¨åŠŸèƒ½ï¼Œä¾¿äºè°ƒè¯•\")\n",
    "\n",
    "print(\"\\nğŸ”§ å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯ä»¥:\")\n",
    "print(\"- æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ç‰¹å¾å·¥ç¨‹\")\n",
    "print(\"- å°è¯•ä¸åŒçš„æ¨¡å‹å‚æ•°\")\n",
    "print(\"- æ·»åŠ æ›´å¤šé¢†åŸŸç‰¹å®šç‰¹å¾\")\n",
    "print(\"- ä½¿ç”¨äº¤å‰éªŒè¯è¿›è¡Œæ¨¡å‹é€‰æ‹©\")\n",
    "\n",
    "if 'submission' in locals():\n",
    "    print(f\"\\nğŸ“„ æäº¤æ–‡ä»¶å·²å°±ç»ª: submission.csv\")\n",
    "    print(\"å¯ä»¥ç›´æ¥åœ¨Kaggleä¸­æäº¤è¿™ä¸ªæ–‡ä»¶ï¼\")\n",
    "\n",
    "print(\"\\nâœ… ä»£ç å·²å®Œå…¨é€‚é…Kaggleç¯å¢ƒï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
