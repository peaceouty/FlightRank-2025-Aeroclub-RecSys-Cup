{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331e7e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:24:59.533299Z",
     "iopub.status.busy": "2025-07-19T13:24:59.533066Z",
     "iopub.status.idle": "2025-07-19T13:27:20.248152Z",
     "shell.execute_reply": "2025-07-19T13:27:20.242156Z"
    },
    "papermill": {
     "duration": 140.726764,
     "end_time": "2025-07-19T13:27:20.251458",
     "exception": false,
     "start_time": "2025-07-19T13:24:59.524694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U xgboost\n",
    "!pip install -U polars\n",
    "!pip install -U optuna\n",
    "!pip install -U lightgbm\n",
    "!pip install -U tensorflow\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9b21b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:27:20.266129Z",
     "iopub.status.busy": "2025-07-19T13:27:20.265852Z",
     "iopub.status.idle": "2025-07-19T13:27:29.592209Z",
     "shell.execute_reply": "2025-07-19T13:27:29.588031Z"
    },
    "papermill": {
     "duration": 9.337472,
     "end_time": "2025-07-19T13:27:29.594557",
     "exception": false,
     "start_time": "2025-07-19T13:27:20.257085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ç¥ç»ç½‘ç»œç›¸å…³å¯¼å…¥ - å·²ç§»é™¤ (åŸºäº0.47497æˆåŠŸç»éªŒ)\n",
    "# import tensorflow as tf  \n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "# tf.random.set_seed(RANDOM_STATE)  # å·²ç§»é™¤ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6570ef53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:27:29.608897Z",
     "iopub.status.busy": "2025-07-19T13:27:29.608575Z",
     "iopub.status.idle": "2025-07-19T13:27:34.036084Z",
     "shell.execute_reply": "2025-07-19T13:27:34.031683Z"
    },
    "papermill": {
     "duration": 4.437852,
     "end_time": "2025-07-19T13:27:34.038775",
     "exception": false,
     "start_time": "2025-07-19T13:27:29.600923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found data files in: /kaggle/input/aeroclub-recsys-2025/\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully:\n",
      "  Train: (18145372, 126)\n",
      "  Test: (6897776, 126)\n",
      "  Combined: (25043148, 126)\n"
     ]
    }
   ],
   "source": [
    "# Load data - æ”¯æŒå¤šç§æ•°æ®è·¯å¾„\n",
    "import os\n",
    "\n",
    "# å®šä¹‰å¯èƒ½çš„æ•°æ®è·¯å¾„\n",
    "possible_paths = [\n",
    "    '/kaggle/input/aeroclub-recsys-2025/',  # Kaggleç¯å¢ƒ\n",
    "    './data/',                              # æœ¬åœ°dataæ–‡ä»¶å¤¹\n",
    "    './',                                   # å½“å‰ç›®å½•\n",
    "    'c:/Users/ShuaiZhiyu/Desktop/FlightRank_2025/',  # ç»å¯¹è·¯å¾„\n",
    "]\n",
    "\n",
    "# æŸ¥æ‰¾æ•°æ®æ–‡ä»¶\n",
    "train_file = None\n",
    "test_file = None\n",
    "\n",
    "for path in possible_paths:\n",
    "    train_path = os.path.join(path, 'train.parquet')\n",
    "    test_path = os.path.join(path, 'test.parquet')\n",
    "    \n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        train_file = train_path\n",
    "        test_file = test_path\n",
    "        print(f\"âœ… Found data files in: {path}\")\n",
    "        break\n",
    "\n",
    "if train_file is None:\n",
    "    print(\"âŒ Data files not found in any of the expected locations:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"  - {path}\")\n",
    "    print(\"\\nPlease ensure train.parquet and test.parquet are available in one of these locations.\")\n",
    "    print(\"Or update the possible_paths list with the correct path.\")\n",
    "    raise FileNotFoundError(\"Data files not found\")\n",
    "\n",
    "try:\n",
    "    # Load data\n",
    "    print(\"Loading training data...\")\n",
    "    train = pl.read_parquet(train_file)\n",
    "    if '__index_level_0__' in train.columns:\n",
    "        train = train.drop('__index_level_0__')\n",
    "    \n",
    "    print(\"Loading test data...\")\n",
    "    test = pl.read_parquet(test_file)\n",
    "    if '__index_level_0__' in test.columns:\n",
    "        test = test.drop('__index_level_0__')\n",
    "    test = test.with_columns(pl.lit(0, dtype=pl.Int64).alias(\"selected\"))\n",
    "\n",
    "    data_raw = pl.concat((train, test))\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully:\")\n",
    "    print(f\"  Train: {train.shape}\")\n",
    "    print(f\"  Test: {test.shape}\")\n",
    "    print(f\"  Combined: {data_raw.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    print(\"Please check if the data files are in the correct format.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914afe0",
   "metadata": {
    "papermill": {
     "duration": 0.005046,
     "end_time": "2025-07-19T13:27:34.048744",
     "exception": false,
     "start_time": "2025-07-19T13:27:34.043698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146204c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:27:34.061657Z",
     "iopub.status.busy": "2025-07-19T13:27:34.061414Z",
     "iopub.status.idle": "2025-07-19T13:27:34.073026Z",
     "shell.execute_reply": "2025-07-19T13:27:34.068719Z"
    },
    "papermill": {
     "duration": 0.022119,
     "end_time": "2025-07-19T13:27:34.075398",
     "exception": false,
     "start_time": "2025-07-19T13:27:34.053279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def hitrate_at_3(y_true, y_pred, groups):\n",
    "    df = pl.DataFrame({\n",
    "        'group': groups,\n",
    "        'pred': y_pred,\n",
    "        'true': y_true\n",
    "    })\n",
    "    \n",
    "    return (\n",
    "        df.filter(pl.col(\"group\").count().over(\"group\") > 10)\n",
    "        .sort([\"group\", \"pred\"], descending=[False, True])\n",
    "        .group_by(\"group\", maintain_order=True)\n",
    "        .head(3)\n",
    "        .group_by(\"group\")\n",
    "        .agg(pl.col(\"true\").max())\n",
    "        .select(pl.col(\"true\").mean())\n",
    "        .item()\n",
    "    )\n",
    "\n",
    "# More efficient duration to minutes converter\n",
    "def dur_to_min(col):\n",
    "    # Extract days and time parts in one pass\n",
    "    days = col.str.extract(r\"^(\\d+)\\.\", 1).cast(pl.Int64).fill_null(0) * 1440\n",
    "    time_str = pl.when(col.str.contains(r\"^\\d+\\.\")).then(col.str.replace(r\"^\\d+\\.\", \"\")).otherwise(col)\n",
    "    hours = time_str.str.extract(r\"^(\\d+):\", 1).cast(pl.Int64).fill_null(0) * 60\n",
    "    minutes = time_str.str.extract(r\":(\\d+):\", 1).cast(pl.Int64).fill_null(0)\n",
    "    return (days + hours + minutes).fill_null(0)\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3449078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:27:34.088828Z",
     "iopub.status.busy": "2025-07-19T13:27:34.088584Z",
     "iopub.status.idle": "2025-07-19T13:28:03.593529Z",
     "shell.execute_reply": "2025-07-19T13:28:03.589365Z"
    },
    "papermill": {
     "duration": 29.515616,
     "end_time": "2025-07-19T13:28:03.596058",
     "exception": false,
     "start_time": "2025-07-19T13:27:34.080442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Starting comprehensive feature engineering...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Core feature engineering completed!\n",
      "ğŸš€ FrequentFlyer features enhanced with 9+ new features based on 0.48737 reference!\n",
      "   New FF features: hasFrequentFlyer, ff_string_length, ff_SU/S7/U6/DP/EK/TK, ff_substantial, ff_multiple_programs\n",
      "   FF carrier matching: ff_carrier_match_SU/S7/U6\n",
      "   Expected performance boost: +0.005-0.007 points\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Core Feature Engineering\n",
    "print(\"ğŸ”§ Starting comprehensive feature engineering...\")\n",
    "\n",
    "df = data_raw.clone()\n",
    "\n",
    "# Process duration columns\n",
    "dur_cols = [\"legs0_duration\", \"legs1_duration\"] + [f\"legs{l}_segments{s}_duration\" for l in (0, 1) for s in (0, 1)]\n",
    "dur_exprs = [dur_to_min(pl.col(c)).alias(c) for c in dur_cols if c in df.columns]\n",
    "\n",
    "# Apply duration transformations first\n",
    "if dur_exprs:\n",
    "    df = df.with_columns(dur_exprs)\n",
    "\n",
    "# Precompute marketing carrier columns check\n",
    "mc_cols = [f'legs{l}_segments{s}_marketingCarrier_code' for l in (0, 1) for s in range(4)]\n",
    "mc_exists = [col for col in mc_cols if col in df.columns]\n",
    "\n",
    "# Step 1: Create base features (including n_ff_programs first)\n",
    "df = df.with_columns([\n",
    "        # Price features\n",
    "        (pl.col(\"totalPrice\") / (pl.col(\"taxes\") + 1)).alias(\"price_per_tax\"),\n",
    "        (pl.col(\"taxes\") / (pl.col(\"totalPrice\") + 1)).alias(\"tax_rate\"),\n",
    "        pl.col(\"totalPrice\").log1p().alias(\"log_price\"),\n",
    "        \n",
    "        # Duration features\n",
    "        (pl.col(\"legs0_duration\").fill_null(0) + pl.col(\"legs1_duration\").fill_null(0)).alias(\"total_duration\"),\n",
    "        pl.when(pl.col(\"legs1_duration\").fill_null(0) > 0)\n",
    "            .then(pl.col(\"legs0_duration\") / (pl.col(\"legs1_duration\") + 1))\n",
    "            .otherwise(1.0).alias(\"duration_ratio\"),\n",
    "        \n",
    "        # Trip type\n",
    "        (pl.col(\"legs1_duration\").is_null() | \n",
    "         (pl.col(\"legs1_duration\") == 0) | \n",
    "         pl.col(\"legs1_segments0_departureFrom_airport_iata\").is_null()).cast(pl.Int32).alias(\"is_one_way\"),\n",
    "        \n",
    "        # Total segments count\n",
    "        (pl.sum_horizontal(pl.col(col).is_not_null().cast(pl.UInt8) for col in mc_exists) \n",
    "         if mc_exists else pl.lit(0)).alias(\"l0_seg\"),\n",
    "        \n",
    "        # ğŸš€ Base FF features - å¿…é¡»é¦–å…ˆåˆ›å»ºn_ff_programs\n",
    "        (pl.col(\"frequentFlyer\").fill_null(\"\").str.count_matches(\"/\") + \n",
    "         (pl.col(\"frequentFlyer\").fill_null(\"\") != \"\").cast(pl.Int32)).alias(\"n_ff_programs\"),\n",
    "        \n",
    "        # Basic FF features that don't depend on n_ff_programs\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.len_chars().alias(\"ff_string_length\"),\n",
    "        pl.col(\"frequentFlyer\").is_not_null().cast(pl.Int32).alias(\"hasFrequentFlyer\"),\n",
    "        \n",
    "        # ä¸»è¦èˆªç©ºå…¬å¸FFæ£€æµ‹ - åŸºäºæ•°æ®ä¸­çš„å¸¸è§èˆªç©ºå…¬å¸ (å®‰å…¨å¤„ç†nullå€¼)\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"SU\").cast(pl.Int32).alias(\"ff_SU\"),\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"S7\").cast(pl.Int32).alias(\"ff_S7\"),\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"U6\").cast(pl.Int32).alias(\"ff_U6\"),\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"DP\").cast(pl.Int32).alias(\"ff_DP\"),\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"EK\").cast(pl.Int32).alias(\"ff_EK\"),\n",
    "        pl.col(\"frequentFlyer\").fill_null(\"\").str.contains(\"TK\").cast(pl.Int32).alias(\"ff_TK\"),\n",
    "        \n",
    "        # FFè´¨é‡æŒ‡æ ‡\n",
    "        (pl.col(\"frequentFlyer\").fill_null(\"\").str.len_chars() > 5).cast(pl.Int32).alias(\"ff_substantial\"),\n",
    "        \n",
    "        # Binary features\n",
    "        pl.col(\"corporateTariffCode\").is_not_null().cast(pl.Int32).alias(\"has_corporate_tariff\"),\n",
    "        (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"has_access_tp\"),\n",
    "        \n",
    "        # Baggage & fees\n",
    "        (pl.col(\"legs0_segments0_baggageAllowance_quantity\").fill_null(0) + \n",
    "         pl.col(\"legs1_segments0_baggageAllowance_quantity\").fill_null(0)).alias(\"baggage_total\"),\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(0) + \n",
    "         pl.col(\"miniRules1_monetaryAmount\").fill_null(0)).alias(\"total_fees\"),\n",
    "        \n",
    "        # Routes & carriers\n",
    "        pl.col(\"searchRoute\").is_in([\"MOWLED/LEDMOW\", \"LEDMOW/MOWLED\", \"MOWLED\", \"LEDMOW\", \"MOWAER/AERMOW\"])\n",
    "            .cast(pl.Int32).alias(\"is_popular_route\"),\n",
    "        \n",
    "        # Cabin\n",
    "        pl.mean_horizontal([\"legs0_segments0_cabinClass\", \"legs1_segments0_cabinClass\"]).alias(\"avg_cabin_class\"),\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(0) - \n",
    "         pl.col(\"legs1_segments0_cabinClass\").fill_null(0)).alias(\"cabin_class_diff\"),\n",
    "])\n",
    "\n",
    "# Step 2: Now we can create features that depend on n_ff_programs\n",
    "df = df.with_columns([\n",
    "    # FFç¨‹åºè´¨é‡æŒ‡æ ‡ (ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨n_ff_programs)\n",
    "    (pl.col(\"n_ff_programs\") >= 2).cast(pl.Int32).alias(\"ff_multiple_programs\"),\n",
    "])\n",
    "\n",
    "# Segment counts - more efficient\n",
    "seg_exprs = []\n",
    "for leg in (0, 1):\n",
    "    seg_cols = [f\"legs{leg}_segments{s}_duration\" for s in range(4) if f\"legs{leg}_segments{s}_duration\" in df.columns]\n",
    "    if seg_cols:\n",
    "        seg_exprs.append(\n",
    "            pl.sum_horizontal(pl.col(c).is_not_null() for c in seg_cols)\n",
    "                .cast(pl.Int32).alias(f\"n_segments_leg{leg}\")\n",
    "        )\n",
    "    else:\n",
    "        seg_exprs.append(pl.lit(0).cast(pl.Int32).alias(f\"n_segments_leg{leg}\"))\n",
    "\n",
    "# Add segment-based features\n",
    "df = df.with_columns(seg_exprs)\n",
    "\n",
    "# Then use them for derived features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"n_segments_leg0\") + pl.col(\"n_segments_leg1\")).alias(\"total_segments\"),\n",
    "    (pl.col(\"n_segments_leg0\") == 1).cast(pl.Int32).alias(\"is_direct_leg0\"),\n",
    "    pl.when(pl.col(\"is_one_way\") == 1).then(0)\n",
    "        .otherwise((pl.col(\"n_segments_leg1\") == 1).cast(pl.Int32)).alias(\"is_direct_leg1\"),\n",
    "])\n",
    "\n",
    "# More derived features with enhanced FF integration\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"is_direct_leg0\") & pl.col(\"is_direct_leg1\")).cast(pl.Int32).alias(\"both_direct\"),\n",
    "    # å¢å¼ºçš„VIPæ£€æµ‹ - æ•´åˆæ–°çš„FFç‰¹å¾\n",
    "    ((pl.col(\"isVip\") == 1) | (pl.col(\"n_ff_programs\") > 0) | (pl.col(\"hasFrequentFlyer\") == 1)).cast(pl.Int32).alias(\"is_vip_freq\"),\n",
    "    (pl.col(\"baggage_total\") > 0).cast(pl.Int32).alias(\"has_baggage\"),\n",
    "    (pl.col(\"total_fees\") > 0).cast(pl.Int32).alias(\"has_fees\"),\n",
    "    (pl.col(\"total_fees\") / (pl.col(\"totalPrice\") + 1)).alias(\"fee_rate\"),\n",
    "    pl.col(\"Id\").count().over(\"ranker_id\").alias(\"group_size\"),\n",
    "])\n",
    "\n",
    "# Add major carrier flag with FF carrier matching\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\"])\n",
    "            .cast(pl.Int32).alias(\"is_major_carrier\"),\n",
    "        # FFèˆªç©ºå…¬å¸åŒ¹é…ç‰¹å¾ - æ£€æµ‹FFç¨‹åºæ˜¯å¦ä¸æ‰¿è¿èˆªç©ºå…¬å¸åŒ¹é…\n",
    "        ((pl.col(\"legs0_segments0_marketingCarrier_code\") == \"SU\") & (pl.col(\"ff_SU\") == 1)).cast(pl.Int32).alias(\"ff_carrier_match_SU\"),\n",
    "        ((pl.col(\"legs0_segments0_marketingCarrier_code\") == \"S7\") & (pl.col(\"ff_S7\") == 1)).cast(pl.Int32).alias(\"ff_carrier_match_S7\"),\n",
    "        ((pl.col(\"legs0_segments0_marketingCarrier_code\") == \"U6\") & (pl.col(\"ff_U6\") == 1)).cast(pl.Int32).alias(\"ff_carrier_match_U6\"),\n",
    "    ])\n",
    "else:\n",
    "    df = df.with_columns(pl.lit(0).alias(\"is_major_carrier\"))\n",
    "\n",
    "df = df.with_columns(pl.col(\"group_size\").log1p().alias(\"group_size_log\"))\n",
    "\n",
    "print(\"âœ… Core feature engineering completed!\")\n",
    "print(\"ğŸš€ FrequentFlyer features enhanced with 9+ new features based on 0.48737 reference!\")\n",
    "print(\"   New FF features: hasFrequentFlyer, ff_string_length, ff_SU/S7/U6/DP/EK/TK, ff_substantial, ff_multiple_programs\")\n",
    "print(\"   FF carrier matching: ff_carrier_match_SU/S7/U6\")\n",
    "print(\"   Expected performance boost: +0.005-0.007 points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c224942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:28:03.610053Z",
     "iopub.status.busy": "2025-07-19T13:28:03.609800Z",
     "iopub.status.idle": "2025-07-19T13:28:47.257922Z",
     "shell.execute_reply": "2025-07-19T13:28:47.254670Z"
    },
    "papermill": {
     "duration": 43.659716,
     "end_time": "2025-07-19T13:28:47.261282",
     "exception": false,
     "start_time": "2025-07-19T13:28:03.601566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Time features and rankings completed!\n"
     ]
    }
   ],
   "source": [
    "# Time features - batch process\n",
    "time_exprs = []\n",
    "for col in (\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"):\n",
    "    if col in df.columns:\n",
    "        dt = pl.col(col).str.to_datetime(strict=False)\n",
    "        h = dt.dt.hour().fill_null(12)\n",
    "        time_exprs.extend([\n",
    "            h.alias(f\"{col}_hour\"),\n",
    "            dt.dt.weekday().fill_null(0).alias(f\"{col}_weekday\"),\n",
    "            (((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))).cast(pl.Int32).alias(f\"{col}_business_time\")\n",
    "        ])\n",
    "if time_exprs:\n",
    "    df = df.with_columns(time_exprs)\n",
    "\n",
    "# Price and duration basic ranks\n",
    "rank_exprs = []\n",
    "for col, alias in [(\"totalPrice\", \"price\"), (\"total_duration\", \"duration\")]:\n",
    "    rank_exprs.append(pl.col(col).rank().over(\"ranker_id\").alias(f\"{alias}_rank\"))\n",
    "\n",
    "# Price-specific features\n",
    "price_exprs = [\n",
    "    (pl.col(\"totalPrice\").rank(\"average\").over(\"ranker_id\") / \n",
    "     pl.col(\"totalPrice\").count().over(\"ranker_id\")).alias(\"price_pct_rank\"),\n",
    "    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheapest\"),\n",
    "    ((pl.col(\"totalPrice\") - pl.col(\"totalPrice\").median().over(\"ranker_id\")) / \n",
    "     (pl.col(\"totalPrice\").std().over(\"ranker_id\") + 1)).alias(\"price_from_median\"),\n",
    "    (pl.col(\"l0_seg\") == pl.col(\"l0_seg\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_min_segments\"),\n",
    "]\n",
    "\n",
    "# Apply initial ranks\n",
    "df = df.with_columns(rank_exprs + price_exprs)\n",
    "\n",
    "# Add fast option feature\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"total_duration\") <= pl.col(\"total_duration\").quantile(0.3).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_fast_option\"),\n",
    "])\n",
    "\n",
    "# Cheapest direct - more efficient\n",
    "direct_cheapest = (\n",
    "    df.filter(pl.col(\"is_direct_leg0\") == 1)\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.col(\"totalPrice\").min().alias(\"min_direct\"))\n",
    ")\n",
    "\n",
    "df = df.join(direct_cheapest, on=\"ranker_id\", how=\"left\").with_columns(\n",
    "    ((pl.col(\"is_direct_leg0\") == 1) & \n",
    "     (pl.col(\"totalPrice\") == pl.col(\"min_direct\"))).cast(pl.Int32).fill_null(0).alias(\"is_direct_cheapest\")\n",
    ").drop(\"min_direct\")\n",
    "\n",
    "print(\"âœ… Time features and rankings completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee575dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:28:47.275243Z",
     "iopub.status.busy": "2025-07-19T13:28:47.275019Z",
     "iopub.status.idle": "2025-07-19T13:28:48.404868Z",
     "shell.execute_reply": "2025-07-19T13:28:48.399239Z"
    },
    "papermill": {
     "duration": 1.140531,
     "end_time": "2025-07-19T13:28:48.407284",
     "exception": false,
     "start_time": "2025-07-19T13:28:47.266753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding business traveler features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Business traveler features completed!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Business Traveler Features\n",
    "print(\"Adding business traveler features...\")\n",
    "\n",
    "# 1. åŸºç¡€ä»·æ ¼å’Œæ”¿ç­–ç‰¹å¾\n",
    "df = df.with_columns([\n",
    "    # ä¼ä¸šæ”¿ç­–åˆè§„\n",
    "    (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"policy_compliant\"),\n",
    "    \n",
    "    # ä»·æ ¼åˆ†æ¡¶ (ç¨³å®šç‰¹å¾)\n",
    "    pl.when(pl.col(\"price_pct_rank\") <= 0.2).then(1)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.4).then(2)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.6).then(3)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.8).then(4)\n",
    "    .otherwise(5).alias(\"price_bucket\"),\n",
    "    \n",
    "    # ä»·æ ¼ç«äº‰åŠ›\n",
    "    (pl.col(\"totalPrice\") <= pl.col(\"totalPrice\").quantile(0.25).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheap_quartile\"),\n",
    "    \n",
    "    # ç¨åŠ¡æ•ˆç‡\n",
    "    pl.when(pl.col(\"taxes\") > 0).then(pl.col(\"totalPrice\") / pl.col(\"taxes\")).otherwise(0).alias(\"price_tax_efficiency\"),\n",
    "])\n",
    "\n",
    "# 2. æ—¶é—´åå¥½ç‰¹å¾ (å•†åŠ¡æ—…è¡Œè€…)\n",
    "time_features = []\n",
    "for prefix in [\"legs0_departureAt\", \"legs0_arrivalAt\"]:\n",
    "    hour_col = f\"{prefix}_hour\"\n",
    "    if hour_col in df.columns:\n",
    "        time_features.extend([\n",
    "            # å•†åŠ¡é»„é‡‘æ—¶æ®µ (7-9am, 5-7pm)\n",
    "            (((pl.col(hour_col) >= 7) & (pl.col(hour_col) <= 9)) | \n",
    "             ((pl.col(hour_col) >= 17) & (pl.col(hour_col) <= 19))).cast(pl.Int32).alias(f\"{prefix}_business_prime\"),\n",
    "            \n",
    "            # é¿å…çº¢çœ¼èˆªç­\n",
    "            ((pl.col(hour_col) >= 23) | (pl.col(hour_col) <= 5)).cast(pl.Int32).alias(f\"{prefix}_red_eye\"),\n",
    "        ])\n",
    "\n",
    "if time_features:\n",
    "    df = df.with_columns(time_features)\n",
    "\n",
    "# 3. èˆªçº¿å’ŒæœåŠ¡è´¨é‡\n",
    "route_features = []\n",
    "if \"legs0_segments0_departureFrom_airport_iata\" in df.columns:\n",
    "    route_features.extend([\n",
    "        # ä¸»è¦æ¢çº½æœºåœº\n",
    "        pl.col(\"legs0_segments0_departureFrom_airport_iata\").is_in([\"SVO\", \"DME\", \"VKO\"]).cast(pl.Int32).alias(\"major_hub_departure\"),\n",
    "        pl.col(\"legs0_segments0_arrivalTo_airport_iata\").is_in([\"LED\", \"PKC\"]).cast(pl.Int32).alias(\"major_hub_arrival\"),\n",
    "    ])\n",
    "\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    route_features.extend([\n",
    "        # é«˜çº§èˆªç©ºå…¬å¸\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\"]).cast(pl.Int32).alias(\"premium_carrier\"),\n",
    "        \n",
    "        # èˆªç©ºå…¬å¸ä¸€è‡´æ€§\n",
    "        (pl.col(\"legs0_segments0_marketingCarrier_code\") == \n",
    "         pl.col(\"legs1_segments0_marketingCarrier_code\").fill_null(\"\")).cast(pl.Int32).alias(\"carrier_consistency\"),\n",
    "    ])\n",
    "\n",
    "if route_features:\n",
    "    df = df.with_columns(route_features)\n",
    "\n",
    "# 4. å•†åŠ¡ä»·å€¼ç»„åˆç‰¹å¾ (å®‰å…¨è®¡ç®—)\n",
    "business_combinations = [\n",
    "    # ç›´é£ + ä¾¿å®œçš„ç»„åˆ\n",
    "    (pl.col(\"both_direct\") * pl.col(\"is_cheap_quartile\")).alias(\"direct_and_cheap\"),\n",
    "    \n",
    "    # æ•ˆç‡å¾—åˆ†\n",
    "    (pl.col(\"both_direct\") * 2 + pl.col(\"is_cheapest\")).alias(\"efficiency_score\"),\n",
    "    \n",
    "    # ä»·å€¼æ„ŸçŸ¥\n",
    "    (pl.col(\"both_direct\") / (pl.col(\"price_pct_rank\") + 0.1)).alias(\"value_perception\"),\n",
    "]\n",
    "\n",
    "# æ¡ä»¶æ€§æ·»åŠ å•†åŠ¡æ—¶é—´ç‰¹å¾ (å®‰å…¨æ£€æŸ¥)\n",
    "if \"legs0_departureAt_business_prime\" in df.columns:\n",
    "    business_combinations.append(\n",
    "        (pl.col(\"legs0_departureAt_business_prime\") * pl.col(\"policy_compliant\")).alias(\"business_compliant\")\n",
    "    )\n",
    "else:\n",
    "    business_combinations.append(pl.lit(0).alias(\"business_compliant\"))\n",
    "\n",
    "# åº”ç”¨æ‰€æœ‰ç»„åˆç‰¹å¾\n",
    "df = df.with_columns(business_combinations)\n",
    "\n",
    "print(\"âœ… Business traveler features completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c22269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:28:48.423202Z",
     "iopub.status.busy": "2025-07-19T13:28:48.422937Z",
     "iopub.status.idle": "2025-07-19T13:29:01.235661Z",
     "shell.execute_reply": "2025-07-19T13:29:01.230910Z"
    },
    "papermill": {
     "duration": 12.824516,
     "end_time": "2025-07-19T13:29:01.238039",
     "exception": false,
     "start_time": "2025-07-19T13:28:48.413523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding advanced business features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Advanced business features completed!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Advanced Business Features\n",
    "print(\"Adding advanced business features...\")\n",
    "\n",
    "# 1. é¢„è®¢æ—¶é—´æ™ºèƒ½åˆ†æ (åŸºäºrequestDate)\n",
    "if \"requestDate\" in df.columns:\n",
    "    request_date_expr = pl.col(\"requestDate\")\n",
    "    \n",
    "    # å¦‚æœrequestDateä¸æ˜¯datetimeç±»å‹ï¼Œæ‰è¿›è¡Œè½¬æ¢\n",
    "    if str(df.select(pl.col(\"requestDate\")).dtypes[0]) not in [\"Datetime\", \"Datetime(time_unit='ns', time_zone=None)\"]:\n",
    "        request_date_expr = pl.col(\"requestDate\").str.to_datetime(strict=False)\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        # æå‰é¢„è®¢å¤©æ•°\n",
    "        ((pl.col(\"legs0_departureAt\").str.to_datetime(strict=False) - \n",
    "          request_date_expr).dt.total_days()).alias(\"booking_lead_days\"),\n",
    "    ])\n",
    "    \n",
    "    # é¢„è®¢æ¨¡å¼ç‰¹å¾\n",
    "    df = df.with_columns([\n",
    "        # çŸ­æœŸé¢„è®¢ (å•†åŠ¡æ€¥éœ€)\n",
    "        (pl.col(\"booking_lead_days\") <= 3).cast(pl.Int32).alias(\"urgent_booking\"),\n",
    "        # æœ€ä¼˜é¢„è®¢çª—å£ (14-30å¤©)\n",
    "        ((pl.col(\"booking_lead_days\") >= 14) & (pl.col(\"booking_lead_days\") <= 30)).cast(pl.Int32).alias(\"optimal_booking_window\"),\n",
    "        # è¶…å‰é¢„è®¢ (>60å¤©ï¼Œé€šå¸¸ä¼‘é—²)\n",
    "        (pl.col(\"booking_lead_days\") > 60).cast(pl.Int32).alias(\"early_leisure_booking\"),\n",
    "        # é¢„è®¢æ—¶é—´ä¸ç»„å†…æ¯”è¾ƒ\n",
    "        (pl.col(\"booking_lead_days\") / (pl.col(\"booking_lead_days\").mean().over(\"ranker_id\") + 1)).alias(\"relative_booking_lead\"),\n",
    "    ])\n",
    "\n",
    "# 2. é«˜çº§ä»·æ ¼å¼¹æ€§å’Œç«äº‰åŠ›ç‰¹å¾\n",
    "df = df.with_columns([\n",
    "    # ä»·æ ¼å¼¹æ€§åˆ†æ\n",
    "    (pl.col(\"totalPrice\").std().over(\"ranker_id\") / (pl.col(\"totalPrice\").mean().over(\"ranker_id\") + 1)).alias(\"price_volatility\"),\n",
    "    \n",
    "    # ä»·æ ¼æ¢¯åº¦ç‰¹å¾\n",
    "    (pl.col(\"totalPrice\").rank().over(\"ranker_id\") / pl.col(\"totalPrice\").count().over(\"ranker_id\")).alias(\"price_percentile\"),\n",
    "    \n",
    "    # ä»·å€¼æ„ŸçŸ¥ (æ€§ä»·æ¯”)\n",
    "    (pl.col(\"total_duration\") / (pl.col(\"totalPrice\") + 1)).alias(\"time_per_dollar\"),\n",
    "    (pl.col(\"both_direct\") / (pl.col(\"totalPrice\") + 1) * 1000).alias(\"convenience_per_dollar\"),\n",
    "])\n",
    "\n",
    "# ç„¶ååŸºäº price_percentile æ·»åŠ è¡ç”Ÿç‰¹å¾\n",
    "df = df.with_columns([\n",
    "    # è¶…çº§ä¼˜æƒ æ£€æµ‹ (åº•éƒ¨10%)\n",
    "    (pl.col(\"price_percentile\") <= 0.1).cast(pl.Int32).alias(\"super_deal\"),\n",
    "    \n",
    "    # ä»·æ ¼ç¦»ç¾¤æ£€æµ‹ (é¡¶éƒ¨20%)\n",
    "    (pl.col(\"price_percentile\") >= 0.8).cast(pl.Int32).alias(\"premium_priced\"),\n",
    "])\n",
    "\n",
    "# 3. é«˜çº§äº’åŠ¨ç‰¹å¾\n",
    "basic_interactions = [\n",
    "    # å•†åŠ¡ä»·å€¼ç»¼åˆå¾—åˆ†\n",
    "    (pl.col(\"both_direct\") * pl.col(\"is_cheap_quartile\")).alias(\"business_value_combo\"),\n",
    "    \n",
    "    # æ—¶é—´æ•ˆç‡å¾—åˆ†\n",
    "    (pl.col(\"both_direct\") * 2).alias(\"efficiency_base_score\"),\n",
    "    \n",
    "    # ä»·æ ¼ä¼˜åŠ¿å¾—åˆ†\n",
    "    (pl.col(\"is_cheapest\") * pl.col(\"both_direct\")).alias(\"price_advantage_score\"),\n",
    "]\n",
    "\n",
    "# åº”ç”¨åŸºç¡€äº¤äº’ç‰¹å¾\n",
    "df = df.with_columns(basic_interactions)\n",
    "\n",
    "# æ¡ä»¶æ€§æ·»åŠ é«˜çº§äº¤äº’ç‰¹å¾\n",
    "advanced_interactions = []\n",
    "\n",
    "# æ£€æŸ¥å¹¶æ·»åŠ æ€¥éœ€å•†åŠ¡ç‰¹å¾\n",
    "if \"urgent_booking\" in df.columns:\n",
    "    advanced_interactions.append(\n",
    "        (pl.col(\"urgent_booking\") * pl.col(\"both_direct\")).alias(\"urgent_business_score\")\n",
    "    )\n",
    "\n",
    "# åº”ç”¨é«˜çº§äº¤äº’ç‰¹å¾\n",
    "if advanced_interactions:\n",
    "    df = df.with_columns(advanced_interactions)\n",
    "\n",
    "# 4. æœ€ç»ˆå¢å¼ºç‰¹å¾\n",
    "final_features = []\n",
    "\n",
    "# ä»·å€¼ç»¼åˆæŒ‡æ•°\n",
    "if all(col in df.columns for col in [\"both_direct\", \"price_pct_rank\", \"is_cheapest\"]):\n",
    "    final_features.extend([\n",
    "        # ç”œç‚¹é€‰é¡¹ (ç›´é£ + ä¾¿å®œ)\n",
    "        ((pl.col(\"both_direct\") == 1) & (pl.col(\"price_pct_rank\") <= 0.3)).cast(pl.Int32).alias(\"sweet_spot_option\"),\n",
    "        \n",
    "        # ä»·å€¼æ•ˆç‡æ¯”\n",
    "        (pl.col(\"both_direct\") / (pl.col(\"price_pct_rank\") + 0.1)).alias(\"value_efficiency_ratio\"),\n",
    "    ])\n",
    "\n",
    "# ç«äº‰ä¼˜åŠ¿ç‰¹å¾\n",
    "if \"group_size\" in df.columns:\n",
    "    final_features.extend([\n",
    "        # é€‰æ‹©å¤æ‚åº¦\n",
    "        (1 / (pl.col(\"group_size\").log1p() + 1)).alias(\"choice_simplicity\"),\n",
    "        \n",
    "        # å¤§é€‰æ‹©é›†ä¼˜åŠ¿\n",
    "        (pl.col(\"group_size\") >= 15).cast(pl.Int32).alias(\"large_choice_advantage\"),\n",
    "    ])\n",
    "\n",
    "# å®‰å…¨æ·»åŠ æœ€ç»ˆç‰¹å¾\n",
    "if final_features:\n",
    "    df = df.with_columns(final_features)\n",
    "\n",
    "print(\"âœ… Advanced business features completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b4788a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:29:01.252964Z",
     "iopub.status.busy": "2025-07-19T13:29:01.252688Z",
     "iopub.status.idle": "2025-07-19T13:29:05.393555Z",
     "shell.execute_reply": "2025-07-19T13:29:05.388294Z"
    },
    "papermill": {
     "duration": 4.151902,
     "end_time": "2025-07-19T13:29:05.395905",
     "exception": false,
     "start_time": "2025-07-19T13:29:01.244003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (25043148, 222)\n",
      "Dataset ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# Fill nulls and prepare final dataset\n",
    "data = df.with_columns(\n",
    "    [pl.col(c).fill_null(0) for c in df.select(pl.selectors.numeric()).columns] +\n",
    "    [pl.col(c).fill_null(\"missing\") for c in df.select(pl.selectors.string()).columns]\n",
    ")\n",
    "\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "print(f\"Dataset ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971823e6",
   "metadata": {
    "papermill": {
     "duration": 0.005743,
     "end_time": "2025-07-19T13:29:05.407417",
     "exception": false,
     "start_time": "2025-07-19T13:29:05.401674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a46daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:29:05.421763Z",
     "iopub.status.busy": "2025-07-19T13:29:05.421482Z",
     "iopub.status.idle": "2025-07-19T13:34:23.251998Z",
     "shell.execute_reply": "2025-07-19T13:34:23.245383Z"
    },
    "papermill": {
     "duration": 317.842393,
     "end_time": "2025-07-19T13:34:23.254948",
     "exception": false,
     "start_time": "2025-07-19T13:29:05.412555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Feature selection and data preparation...\n",
      "âœ… Using 161 features (35 categorical)\n",
      "ğŸ“Š Categorical features: ['nationality', 'searchRoute', 'corporateTariffCode', 'bySelf', 'sex']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Encoding categorical features for both XGBoost and LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded nationality: 49 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded searchRoute: 6807 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded corporateTariffCode: 182 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded bySelf: 2 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded sex: 2 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded companyID: 682 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_aircraft_code: 111 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_arrivalTo_airport_city_iata: 536 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_arrivalTo_airport_iata: 575 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_departureFrom_airport_iata: 433 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_marketingCarrier_code: 166 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_operatingCarrier_code: 225 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments0_flightNumber: 8016 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_aircraft_code: 105 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_arrivalTo_airport_city_iata: 459 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_arrivalTo_airport_iata: 515 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_departureFrom_airport_iata: 486 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_marketingCarrier_code: 157 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_operatingCarrier_code: 215 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs0_segments1_flightNumber: 7114 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_aircraft_code: 103 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_arrivalTo_airport_city_iata: 335 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_arrivalTo_airport_iata: 363 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_departureFrom_airport_iata: 370 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_marketingCarrier_code: 151 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_operatingCarrier_code: 193 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments0_flightNumber: 5876 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_aircraft_code: 88 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_arrivalTo_airport_city_iata: 194 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_arrivalTo_airport_iata: 211 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_departureFrom_airport_iata: 319 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_marketingCarrier_code: 126 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_operatingCarrier_code: 146 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded legs1_segments1_flightNumber: 3607 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Encoded price_bucket: 5 unique values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data prepared for both models with unified encoding\n",
      "ğŸ“ˆ Features shape: (25043148, 161)\n",
      "ğŸ¯ Both XGBoost and LightGBM will use the same encoded data!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Feature Selection and Data Preparation\n",
    "print(\"ğŸ”§ Feature selection and data preparation...\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Categorical features (åŸå§‹åˆ†ç±»åˆ—)\n",
    "cat_features = [\n",
    "    'nationality', 'searchRoute', 'corporateTariffCode',\n",
    "    'bySelf', 'sex', 'companyID',\n",
    "    # Leg 0 segments 0-1\n",
    "    'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',\n",
    "    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',\n",
    "    'legs0_segments0_flightNumber',\n",
    "    'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',\n",
    "    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',\n",
    "    'legs0_segments1_flightNumber',\n",
    "    # Leg 1 segments 0-1\n",
    "    'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',\n",
    "    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',\n",
    "    'legs1_segments0_flightNumber',\n",
    "    'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',\n",
    "    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',\n",
    "    'legs1_segments1_flightNumber',\n",
    "    # New categorical\n",
    "    'price_bucket'\n",
    "]\n",
    "\n",
    "# Columns to exclude (uninformative or problematic)\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing\n",
    "    'frequentFlyer',  # Already processed\n",
    "    'pricingInfo_passengerCount'  # Constant column\n",
    "]\n",
    "\n",
    "# Exclude segment 2-3 columns (>98% missing)\n",
    "for leg in [0, 1]:\n",
    "    for seg in [2, 3]:\n",
    "        for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',\n",
    "                      'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',\n",
    "                      'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',\n",
    "                      'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:\n",
    "            exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "cat_features_final = [col for col in cat_features if col in feature_cols]\n",
    "\n",
    "print(f\"âœ… Using {len(feature_cols)} features ({len(cat_features_final)} categorical)\")\n",
    "print(f\"ğŸ“Š Categorical features: {cat_features_final[:5]}...\" if cat_features_final else \"No categorical features\")\n",
    "\n",
    "# åˆ›å»ºæœ€ç»ˆçš„ç‰¹å¾çŸ©é˜µ\n",
    "X = data.select(feature_cols)\n",
    "y = data.select('selected')\n",
    "groups = data.select('ranker_id')\n",
    "\n",
    "# è½¬æ¢ä¸ºpandas\n",
    "data_raw_pandas = X.to_pandas()\n",
    "y_pandas = y.to_pandas()['selected']\n",
    "groups_pandas = groups.to_pandas()['ranker_id']\n",
    "\n",
    "# ğŸ”§ ç»Ÿä¸€ç¼–ç æ‰€æœ‰åˆ†ç±»ç‰¹å¾ - è§£å†³XGBoostå’ŒLightGBMå…¼å®¹æ€§é—®é¢˜\n",
    "print(\"ğŸ”§ Encoding categorical features for both XGBoost and LightGBM...\")\n",
    "\n",
    "# åˆ›å»ºç»Ÿä¸€çš„ç¼–ç æ•°æ®\n",
    "data_encoded = data_raw_pandas.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "# å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡Œæ ‡ç­¾ç¼–ç \n",
    "for cat_col in cat_features_final:\n",
    "    if cat_col in data_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        # å¤„ç†ç¼ºå¤±å€¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "        data_encoded[cat_col] = data_encoded[cat_col].astype(str).fillna('missing')\n",
    "        # æ‹Ÿåˆå¹¶è½¬æ¢æ•´ä¸ªåˆ—\n",
    "        data_encoded[cat_col] = le.fit_transform(data_encoded[cat_col])\n",
    "        label_encoders[cat_col] = le\n",
    "        print(f\"   âœ… Encoded {cat_col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# ğŸ”§ XGBoostå’ŒLightGBMéƒ½ä½¿ç”¨ç›¸åŒçš„ç¼–ç æ•°æ®\n",
    "data_xgb = data_encoded.copy()  # XGBoostä½¿ç”¨ç¼–ç åçš„æ•°æ®\n",
    "data_lgb = data_encoded.copy()  # LightGBMä½¿ç”¨ç›¸åŒçš„ç¼–ç æ•°æ®\n",
    "\n",
    "print(f\"âœ… Data prepared for both models with unified encoding\")\n",
    "print(f\"ğŸ“ˆ Features shape: {data_encoded.shape}\")\n",
    "print(f\"ğŸ¯ Both XGBoost and LightGBM will use the same encoded data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fced5",
   "metadata": {
    "papermill": {
     "duration": 0.006919,
     "end_time": "2025-07-19T13:34:23.269241",
     "exception": false,
     "start_time": "2025-07-19T13:34:23.262322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training and Tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1939d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T13:34:23.287360Z",
     "iopub.status.busy": "2025-07-19T13:34:23.287121Z",
     "iopub.status.idle": "2025-07-19T15:12:04.370165Z",
     "shell.execute_reply": "2025-07-19T15:12:04.366038Z"
    },
    "papermill": {
     "duration": 5861.098085,
     "end_time": "2025-07-19T15:12:04.374429",
     "exception": false,
     "start_time": "2025-07-19T13:34:23.276344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training optimized LightGBM DART model...\n",
      "Based on successful 0.47497 strategy: XGBoost + DART ensemble\n",
      "ğŸ”§ Using GroupShuffleSplit for proper Learning to Rank data splitting...\n",
      "Based on successful 0.48737 implementation: 20% validation split\n",
      "Ensuring each ranker_id appears completely in either train or validation\n",
      "ğŸ“Š Data boundaries: n1=16,487,352, n2=18,145,372\n",
      "ğŸ“Š Total data: 25,043,148 rows\n",
      "ğŸ“Š Training data for split: 18,145,372 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Unique ranker_ids: 105,539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GroupShuffleSplit completed:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training: (14477429, 161) (84431 unique ranker_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Validation: (3667943, 161) (21108 unique ranker_ids)\n",
      "   Test: (6897776, 161)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… No overlap between train/val ranker_ids: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split completed:\n",
      "   Training: (14477429, 161)\n",
      "   Validation: (3667943, 161)\n",
      "   Test: (6897776, 161)\n",
      "Creating LightGBM Datasets...\n",
      "LightGBM Datasets created successfully.\n",
      "\n",
      "--- Training LightGBM DART Model ---\n",
      "Optimized for better performance based on error analysis\n",
      "Performance optimization strategy:\n",
      "- Iterations: 2000 (extended for better convergence)\n",
      "- Learning rate: 0.03 (lower for finer training)\n",
      "- Leaves: 63 (increased capacity)\n",
      "- Dropout: 0.15/0.5 (enhanced regularization)\n",
      "- Target: Exceed 0.843+ validation NDCG@3\n",
      "- Categorical features: 35 properly encoded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's ndcg@3: 0.817053"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@3: 0.818653"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid_0's ndcg@3: 0.821046"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's ndcg@3: 0.821292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\tvalid_0's ndcg@3: 0.822849"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's ndcg@3: 0.824423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\tvalid_0's ndcg@3: 0.824742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\tvalid_0's ndcg@3: 0.826232"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\tvalid_0's ndcg@3: 0.827371"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's ndcg@3: 0.828072"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550]\tvalid_0's ndcg@3: 0.829539"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\tvalid_0's ndcg@3: 0.830641"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650]\tvalid_0's ndcg@3: 0.831133"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\tvalid_0's ndcg@3: 0.832212"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750]\tvalid_0's ndcg@3: 0.83223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\tvalid_0's ndcg@3: 0.832915"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\tvalid_0's ndcg@3: 0.833662"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\tvalid_0's ndcg@3: 0.8346"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950]\tvalid_0's ndcg@3: 0.835317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's ndcg@3: 0.83592"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050]\tvalid_0's ndcg@3: 0.836416"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100]\tvalid_0's ndcg@3: 0.837018"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1150]\tvalid_0's ndcg@3: 0.837723"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200]\tvalid_0's ndcg@3: 0.838493"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1250]\tvalid_0's ndcg@3: 0.838819"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1300]\tvalid_0's ndcg@3: 0.839348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1350]\tvalid_0's ndcg@3: 0.839674"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400]\tvalid_0's ndcg@3: 0.839964"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1450]\tvalid_0's ndcg@3: 0.840593"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500]\tvalid_0's ndcg@3: 0.841135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1550]\tvalid_0's ndcg@3: 0.841363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600]\tvalid_0's ndcg@3: 0.841366"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1650]\tvalid_0's ndcg@3: 0.841951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1700]\tvalid_0's ndcg@3: 0.842682"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1750]\tvalid_0's ndcg@3: 0.842898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800]\tvalid_0's ndcg@3: 0.843353"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1850]\tvalid_0's ndcg@3: 0.843754"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1900]\tvalid_0's ndcg@3: 0.844586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1950]\tvalid_0's ndcg@3: 0.844812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_0's ndcg@3: 0.845233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LightGBM DART model training completed!\n",
      "Performance optimizations applied:\n",
      "- Extended training iterations for better convergence\n",
      "- Enhanced regularization to prevent overfitting\n",
      "- Improved model capacity with more leaves\n",
      "- Target: 0.845+ DART validation performance\n",
      "\n",
      "Preparing XGBoost data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost with optimized parameters..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unified encoded data for consistency"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.78999\tval-ndcg@3:0.79229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-ndcg@3:0.82191\tval-ndcg@3:0.81798"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-ndcg@3:0.83137\tval-ndcg@3:0.82223"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttrain-ndcg@3:0.83995\tval-ndcg@3:0.82619"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain-ndcg@3:0.84804\tval-ndcg@3:0.82865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain-ndcg@3:0.85498\tval-ndcg@3:0.83017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttrain-ndcg@3:0.86136\tval-ndcg@3:0.83198"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\ttrain-ndcg@3:0.86637\tval-ndcg@3:0.83289"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttrain-ndcg@3:0.87026\tval-ndcg@3:0.83397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\ttrain-ndcg@3:0.87350\tval-ndcg@3:0.83499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain-ndcg@3:0.87677\tval-ndcg@3:0.83550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550]\ttrain-ndcg@3:0.87993\tval-ndcg@3:0.83673"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttrain-ndcg@3:0.88371\tval-ndcg@3:0.83748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650]\ttrain-ndcg@3:0.88718\tval-ndcg@3:0.83833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttrain-ndcg@3:0.89049\tval-ndcg@3:0.83911"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750]\ttrain-ndcg@3:0.89476\tval-ndcg@3:0.84013"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttrain-ndcg@3:0.89912\tval-ndcg@3:0.84080"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\ttrain-ndcg@3:0.90255\tval-ndcg@3:0.84180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\ttrain-ndcg@3:0.90594\tval-ndcg@3:0.84171"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950]\ttrain-ndcg@3:0.90956\tval-ndcg@3:0.84264"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain-ndcg@3:0.91192\tval-ndcg@3:0.84296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050]\ttrain-ndcg@3:0.91476\tval-ndcg@3:0.84381"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100]\ttrain-ndcg@3:0.91779\tval-ndcg@3:0.84437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1150]\ttrain-ndcg@3:0.92066\tval-ndcg@3:0.84496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1199]\ttrain-ndcg@3:0.92278\tval-ndcg@3:0.84519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Both models trained successfully!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with unified encoded data - no type conflicts!\n",
      "Ready for ensemble optimization...\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ åŸºäº0.47497æˆåŠŸç»éªŒï¼šä¼˜å…ˆè®­ç»ƒLightGBM DARTæ¨¡å‹\n",
    "print(\"Training optimized LightGBM DART model...\")\n",
    "print(\"Based on successful 0.47497 strategy: XGBoost + DART ensemble\")\n",
    "\n",
    "# ğŸ”§ ä½¿ç”¨GroupShuffleSplitè¿›è¡Œæ­£ç¡®çš„æ•°æ®åˆ†å‰² - åŸºäº0.48737æˆåŠŸæ–¹æ¡ˆ\n",
    "print(\"ğŸ”§ Using GroupShuffleSplit for proper Learning to Rank data splitting...\")\n",
    "print(\"Based on successful 0.48737 implementation: 20% validation split\")\n",
    "print(\"Ensuring each ranker_id appears completely in either train or validation\")\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# é‡è¦ï¼šä¿ç•™n1å’Œn2çš„æ¦‚å¿µï¼Œè¿™æ˜¯æ•°æ®åˆ†å‰²çš„å…³é”®\n",
    "# n1: åŸå§‹æ•°æ®ä¸­é€šè¿‡æ—¶é—´åˆ†å‰²çš„è®­ç»ƒæ•°æ®è¾¹ç•Œï¼ˆç°åœ¨ç”¨äºGroupShuffleSplitçš„åŸºç¡€ï¼‰\n",
    "# n2: è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¾¹ç•Œ  \n",
    "n1 = 16487352  # ä¿ç•™åŸå§‹æ—¶é—´åˆ†å‰²ä½ç½®ä½œä¸ºå‚è€ƒ\n",
    "n2 = train.height  # è®­ç»ƒé›†ç»“æŸä½ç½®\n",
    "\n",
    "print(f\"ğŸ“Š Data boundaries: n1={n1:,}, n2={n2:,}\")\n",
    "print(f\"ğŸ“Š Total data: {len(data_encoded):,} rows\")\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ•°æ® (ä¸åŒ…æ‹¬æµ‹è¯•é›†) - è¿™æ˜¯GroupShuffleSplitçš„è¾“å…¥\n",
    "data_train_encoded = data_encoded[:n2]  # åªå–è®­ç»ƒéƒ¨åˆ†\n",
    "y_train_full = y_pandas[:n2]\n",
    "groups_train_full = groups_pandas[:n2]\n",
    "\n",
    "print(f\"ğŸ“Š Training data for split: {len(data_train_encoded):,} rows\")\n",
    "print(f\"ğŸ“Š Unique ranker_ids: {groups_train_full.nunique():,}\")\n",
    "\n",
    "# ä½¿ç”¨GroupShuffleSplitæŒ‰ranker_idåˆ†ç»„åˆ†å‰² - éµå¾ª0.48737å‚è€ƒå®ç°\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # 20%éªŒè¯é›†ï¼Œä¸å‚è€ƒä¸€è‡´\n",
    "train_idx, val_idx = next(gss.split(data_train_encoded, groups=groups_train_full))\n",
    "\n",
    "# åˆ›å»ºæ­£ç¡®çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "X_tr = data_train_encoded.iloc[train_idx].copy()\n",
    "X_va = data_train_encoded.iloc[val_idx].copy()\n",
    "X_te = data_encoded[n2:]  # æµ‹è¯•é›†ä¿æŒä¸å˜\n",
    "\n",
    "y_tr = y_train_full.iloc[train_idx]\n",
    "y_va = y_train_full.iloc[val_idx] \n",
    "y_te = y_pandas[n2:]\n",
    "\n",
    "groups_tr = groups_train_full.iloc[train_idx]\n",
    "groups_va = groups_train_full.iloc[val_idx]\n",
    "groups_te = groups_pandas[n2:]\n",
    "\n",
    "print(f\"âœ… GroupShuffleSplit completed:\")\n",
    "print(f\"   Training: {X_tr.shape} ({len(groups_tr.unique())} unique ranker_ids)\")\n",
    "print(f\"   Validation: {X_va.shape} ({len(groups_va.unique())} unique ranker_ids)\")\n",
    "print(f\"   Test: {X_te.shape}\")\n",
    "print(f\"   âœ… No overlap between train/val ranker_ids: {len(set(groups_tr.unique()) & set(groups_va.unique())) == 0}\")\n",
    "\n",
    "# å‡†å¤‡ç»„å¤§å°æ•°ç»„\n",
    "import pandas as pd\n",
    "group_sizes_tr = pd.Series(groups_tr).value_counts().sort_index().values\n",
    "group_sizes_va = pd.Series(groups_va).value_counts().sort_index().values  \n",
    "group_sizes_te = pd.Series(groups_te).value_counts().sort_index().values\n",
    "\n",
    "print(f\"âœ… Data split completed:\")\n",
    "print(f\"   Training: {X_tr.shape}\")\n",
    "print(f\"   Validation: {X_va.shape}\")\n",
    "print(f\"   Test: {X_te.shape}\")\n",
    "\n",
    "# ç»„å¤§å°æ•°ç»„å·²ç»åœ¨ä¸Šé¢è®¡ç®—è¿‡äº†ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è®¡ç®—\n",
    "\n",
    "# å‡†å¤‡åˆ†ç±»ç‰¹å¾ç´¢å¼•ï¼ˆåŸºäºåŸå§‹åˆ†ç±»ç‰¹å¾åœ¨ç¼–ç æ•°æ®ä¸­çš„ä½ç½®ï¼‰\n",
    "cat_feature_indices = [data_encoded.columns.get_loc(col) for col in cat_features_final if col in data_encoded.columns]\n",
    "\n",
    "# å‡†å¤‡LightGBMæ•°æ®\n",
    "print(\"Creating LightGBM Datasets...\")\n",
    "lgb_train = lgb.Dataset(\n",
    "    data=X_tr, \n",
    "    label=y_tr, \n",
    "    group=group_sizes_tr,\n",
    "    feature_name=list(data_encoded.columns),\n",
    "    categorical_feature=cat_feature_indices,  # æŒ‡å®šåˆ†ç±»ç‰¹å¾ç´¢å¼•\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "lgb_val = lgb.Dataset(\n",
    "    data=X_va, \n",
    "    label=y_va, \n",
    "    group=group_sizes_va,\n",
    "    feature_name=list(data_encoded.columns),\n",
    "    categorical_feature=cat_feature_indices,  # æŒ‡å®šåˆ†ç±»ç‰¹å¾ç´¢å¼•\n",
    "    reference=lgb_train,\n",
    "    free_raw_data=False\n",
    ")\n",
    "print(\"LightGBM Datasets created successfully.\")\n",
    "\n",
    "# ğŸ¯ ä¼˜åŒ–çš„DARTå‚æ•° - é’ˆå¯¹æ€§èƒ½æå‡\n",
    "print(\"\\n--- Training LightGBM DART Model ---\")\n",
    "print(\"Optimized for better performance based on error analysis\")\n",
    "\n",
    "dart_params = {\n",
    "    'objective': 'lambdarank', \n",
    "    'metric': 'ndcg', \n",
    "    'eval_at': [3],\n",
    "    'boosting_type': 'dart', \n",
    "    'n_estimators': 2100,        # å¢åŠ è¿­ä»£æ•°\n",
    "    'learning_rate': 0.03,       # é™ä½å­¦ä¹ ç‡ä»¥æ›´ç²¾ç»†è®­ç»ƒ\n",
    "    'num_leaves': 63,            # å¢åŠ å¶å­æ•°ä»¥æå‡è¡¨è¾¾èƒ½åŠ›\n",
    "    'drop_rate': 0.15,           # æé«˜dropoutç‡ä»¥é¿å…è¿‡æ‹Ÿåˆ\n",
    "    'skip_drop': 0.5,            # ä¿æŒè·³è¿‡ç‡\n",
    "    'subsample': 0.85,           # æé«˜é‡‡æ ·ç‡\n",
    "    'colsample_bytree': 0.8,     # é™ä½ç‰¹å¾é‡‡æ ·é¿å…è¿‡æ‹Ÿåˆ\n",
    "    'reg_alpha': 0.013,           # å¢åŠ L1æ­£åˆ™åŒ–\n",
    "    'reg_lambda': 1.5,           # å¢åŠ L2æ­£åˆ™åŒ–\n",
    "    'min_child_samples': 10,     # é™ä½æœ€å°æ ·æœ¬æ•°ä»¥æå‡æ€§èƒ½\n",
    "    'feature_pre_filter': False,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'label_gain': [0, 1],\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "print(\"Performance optimization strategy:\")\n",
    "print(\"- Iterations: 2000 (extended for better convergence)\")\n",
    "print(\"- Learning rate: 0.03 (lower for finer training)\")\n",
    "print(\"- Leaves: 63 (increased capacity)\")\n",
    "print(\"- Dropout: 0.15/0.5 (enhanced regularization)\")\n",
    "print(\"- Target: Exceed 0.843+ validation NDCG@3\")\n",
    "print(f\"- Categorical features: {len(cat_feature_indices)} properly encoded\")\n",
    "\n",
    "lgb_model_dart = lgb.train(\n",
    "    dart_params,\n",
    "    lgb_train, \n",
    "    num_boost_round=dart_params['n_estimators'], \n",
    "    valid_sets=[lgb_val],\n",
    "    valid_names=['valid_0'],\n",
    "    callbacks=[lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… LightGBM DART model training completed!\")\n",
    "print(\"Performance optimizations applied:\")\n",
    "print(\"- Extended training iterations for better convergence\")\n",
    "print(\"- Enhanced regularization to prevent overfitting\")\n",
    "print(\"- Improved model capacity with more leaves\")\n",
    "print(\"- Target: 0.845+ DART validation performance\")\n",
    "\n",
    "# ç°åœ¨å‡†å¤‡XGBoostæ•°æ® (ä½¿ç”¨ç›¸åŒçš„ç¼–ç æ•°æ®)\n",
    "print(\"\\nPreparing XGBoost data...\")\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr, group=group_sizes_tr, feature_names=list(data_encoded.columns))\n",
    "dval   = xgb.DMatrix(X_va, label=y_va, group=group_sizes_va, feature_names=list(data_encoded.columns))\n",
    "dtest  = xgb.DMatrix(X_te, label=y_te, group=group_sizes_te, feature_names=list(data_encoded.columns))\n",
    "\n",
    "# Optimized XGBoost parameters\n",
    "final_xgb_params = {\n",
    "    'objective': 'rank:pairwise', \n",
    "    'eval_metric': 'ndcg@3', \n",
    "    'max_depth': 8,              \n",
    "    'min_child_weight': 10,      \n",
    "    'subsample': 0.92,           \n",
    "    'colsample_bytree': 0.9,     \n",
    "    'lambda': 3.0,              \n",
    "    'alpha': 0.12,              \n",
    "    'learning_rate': 0.065,     \n",
    "    'gamma': 0.06,              \n",
    "    'seed': RANDOM_STATE, \n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'lossguide'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining XGBoost with optimized parameters...\")\n",
    "print(\"Using unified encoded data for consistency\")\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    final_xgb_params, dtrain,\n",
    "    num_boost_round=1200,        \n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    early_stopping_rounds=80,    \n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Both models trained successfully!\")\n",
    "print(\"Training with unified encoded data - no type conflicts!\")\n",
    "print(\"Ready for ensemble optimization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab25a9",
   "metadata": {
    "papermill": {
     "duration": 0.013173,
     "end_time": "2025-07-19T15:12:04.401717",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.388544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92f16dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T15:12:04.431669Z",
     "iopub.status.busy": "2025-07-19T15:12:04.431286Z",
     "iopub.status.idle": "2025-07-19T15:12:04.441794Z",
     "shell.execute_reply": "2025-07-19T15:12:04.436983Z"
    },
    "papermill": {
     "duration": 0.030229,
     "end_time": "2025-07-19T15:12:04.444849",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.414620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Skipping Neural Network Training ---\n",
      "Based on 0.47497 success: Focus on XGBoost + LightGBM DART only\n",
      "Neural Network showed performance drag (0.3938 vs others 0.48+)\n",
      "Proceeding with proven two-model strategy...\n",
      "âœ… Neural Network training skipped for efficiency\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ è·³è¿‡Neural Networkè®­ç»ƒ (åŸºäº0.47497æˆåŠŸç»éªŒ)\n",
    "print(\"--- Skipping Neural Network Training ---\")\n",
    "print(\"Based on 0.47497 success: Focus on XGBoost + LightGBM DART only\")\n",
    "print(\"Neural Network showed performance drag (0.3938 vs others 0.48+)\")\n",
    "print(\"Proceeding with proven two-model strategy...\")\n",
    "print(\"âœ… Neural Network training skipped for efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2d3ba",
   "metadata": {
    "papermill": {
     "duration": 0.013049,
     "end_time": "2025-07-19T15:12:04.471684",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.458635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Blending and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da9d0fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T15:12:04.500812Z",
     "iopub.status.busy": "2025-07-19T15:12:04.500459Z",
     "iopub.status.idle": "2025-07-19T15:12:04.514454Z",
     "shell.execute_reply": "2025-07-19T15:12:04.508527Z"
    },
    "papermill": {
     "duration": 0.03267,
     "end_time": "2025-07-19T15:12:04.517006",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.484336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimized Three-Model Ensemble Strategy ---\n",
      "Models: XGBoost + LightGBM DART (proven 0.47497 strategy)\n",
      "Rationale: Maximum diversity with efficient training\n",
      "\n",
      "Model readiness check:\n",
      "  XGBoost: âœ… Ready\n",
      "  LightGBM DART: âœ… Ready\n",
      "\n",
      "ğŸ‰ All three models are ready for ensemble!\n",
      "\n",
      "Next steps:\n",
      "1. Generate predictions from all three models\n",
      "2. Optimize ensemble weights based on validation performance\n",
      "3. Create final submission with intelligent weighting\n",
      "4. Target: 0.5+ Kaggle score with three diverse models\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ åŒæ¨¡å‹é›†æˆç­–ç•¥ (XGBoost + LightGBM DART)\n",
    "print(\"\\n--- Optimized Three-Model Ensemble Strategy ---\")\n",
    "print(\"Models: XGBoost + LightGBM DART (proven 0.47497 strategy)\")\n",
    "print(\"Rationale: Maximum diversity with efficient training\")\n",
    "\n",
    "# éªŒè¯æ‰€æœ‰æ¨¡å‹å·²è®­ç»ƒå®Œæˆ\n",
    "models_ready = {\n",
    "    'XGBoost': 'xgb_model' in locals(),\n",
    "    'LightGBM DART': 'lgb_model_dart' in locals()\n",
    "}\n",
    "\n",
    "print(\"\\nModel readiness check:\")\n",
    "for model_name, ready in models_ready.items():\n",
    "    status = \"âœ… Ready\" if ready else \"âŒ Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "if all(models_ready.values()):\n",
    "    print(\"\\nğŸ‰ All three models are ready for ensemble!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some models are not ready. Please complete training first.\")\n",
    "    \n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Generate predictions from all three models\")\n",
    "print(\"2. Optimize ensemble weights based on validation performance\")\n",
    "print(\"3. Create final submission with intelligent weighting\")\n",
    "print(\"4. Target: 0.5+ Kaggle score with three diverse models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692832f",
   "metadata": {
    "papermill": {
     "duration": 0.013186,
     "end_time": "2025-07-19T15:12:04.542987",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.529801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff53af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T15:12:04.604629Z",
     "iopub.status.busy": "2025-07-19T15:12:04.604352Z",
     "iopub.status.idle": "2025-07-19T15:13:46.059634Z",
     "shell.execute_reply": "2025-07-19T15:13:46.054240Z"
    },
    "papermill": {
     "duration": 101.475845,
     "end_time": "2025-07-19T15:13:46.062560",
     "exception": false,
     "start_time": "2025-07-19T15:12:04.586715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROVEN TWO-MODEL ENSEMBLE STRATEGY ===\n",
      "Based on successful 0.47497 Kaggle submission\n",
      "\n",
      "Model readiness check:\n",
      "  âœ… XGBoost - Tree-based gradient boosting\n",
      "  âœ… LightGBM DART - Enhanced dropout regularization\n",
      "  âŒ LightGBM GBDT - Removed (lowest performance)\n",
      "  âŒ Neural Network - Removed (performance drag)\n",
      "\n",
      "ğŸ“Š Generating two-model predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All two-model predictions generated successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Individual model validation performance:\n",
      "  XGBoost: 0.5200\n",
      "  LightGBM_DART: 0.5143\n",
      "\n",
      " Testing two-model ensemble strategies:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ”¥ Proven_Success  : 0.5334 [XGB:0.55, DART:0.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DART_Strong     : 0.5297 [XGB:0.45, DART:0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ”¥ Performance_Based: 0.5340 [XGB:0.60, DART:0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Balanced        : 0.5314 [XGB:0.50, DART:0.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Conservative    : 0.5321 [XGB:0.52, DART:0.48]\n",
      "\n",
      "ğŸ† Best two-model strategy: Performance_Based (HR@3: 0.5340)\n",
      "   Optimal weights: XGBoost=0.600, DART=0.400\n",
      "   Expected Kaggle improvement over 0.47497: +59.0 points\n",
      "\n",
      "âœ… Two-model ensemble optimization completed!\n",
      "ğŸ¯ Target 0.485+ score - Current validation: 0.5340\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ åŒæ¨¡å‹é›†æˆç­–ç•¥ (åŸºäº0.47497æˆåŠŸç»éªŒ)\n",
    "print(\"=== PROVEN TWO-MODEL ENSEMBLE STRATEGY ===\")\n",
    "print(\"Based on successful 0.47497 Kaggle submission\")\n",
    "\n",
    "# 1. éªŒè¯åŒæ¨¡å‹å‡†å¤‡å°±ç»ª\n",
    "print(\"\\nModel readiness check:\")\n",
    "print(\"  âœ… XGBoost - Tree-based gradient boosting\")\n",
    "print(\"  âœ… LightGBM DART - Enhanced dropout regularization\")\n",
    "print(\"  âŒ LightGBM GBDT - Removed (lowest performance)\")\n",
    "print(\"  âŒ Neural Network - Removed (performance drag)\")\n",
    "\n",
    "# 2. ç”ŸæˆåŒæ¨¡å‹é¢„æµ‹\n",
    "print(\"\\nğŸ“Š Generating two-model predictions...\")\n",
    "\n",
    "# éªŒè¯é›†é¢„æµ‹ (ä½¿ç”¨ç»Ÿä¸€çš„ç¼–ç æ•°æ®)\n",
    "xgb_val_preds = xgb_model.predict(dval)\n",
    "dart_val_preds = lgb_model_dart.predict(X_va)\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "xgb_test_preds = xgb_model.predict(dtest)\n",
    "dart_test_preds = lgb_model_dart.predict(X_te)\n",
    "\n",
    "print(\"âœ… All two-model predictions generated successfully\")\n",
    "\n",
    "# 3. è®¡ç®—ä¸ªä½“æ¨¡å‹æ€§èƒ½ (ä½¿ç”¨æ­£ç¡®çš„å˜é‡)\n",
    "val_hitrates = {}\n",
    "val_hitrates['XGBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), xgb_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['LightGBM_DART'] = hitrate_at_3(y_va.to_numpy().flatten(), dart_val_preds, groups_va.to_numpy().flatten())\n",
    "\n",
    "print(\"\\nğŸ“ˆ Individual model validation performance:\")\n",
    "for model, hr in val_hitrates.items():\n",
    "    print(f\"  {model}: {hr:.4f}\")\n",
    "\n",
    "# 4. åŸºäºçœŸå®æ€§èƒ½çš„æƒé‡ç­–ç•¥ (è€ƒè™‘DARTå¼ºåŠ²è¡¨ç°)\n",
    "strategies = {\n",
    "   # \"Proven_Success\": [0.55, 0.45],     # åŸºäº0.47497æˆåŠŸç»éªŒ\n",
    "    \"XGBoost_Heavy\": [0.65, 0.35],     # æ›´é‡è§†XGBoost\n",
    "    \"slight_xgb\":[0.59, 0.41],          # å¹³è¡¡ä½†ç•¥å€¾å‘XGBoost\n",
    "    \"more_xgb\":[0.61, 0.39],              # æ›´é‡è§†XGBoost\n",
    "    \"DART_Strong\": [0.45, 0.55],        # è€ƒè™‘DARTå¼ºåŠ²è¡¨ç°\n",
    "   # \"Performance_Based\": [0.6, 0.4],    # æ›´é‡è§†XGBoost\n",
    "    \"Balanced\": [0.5, 0.5],             # å¹³è¡¡æƒé‡\n",
    "    \"Conservative\": [0.52, 0.48],       # è½»å¾®å€¾å‘XGBoost\n",
    "}\n",
    "\n",
    "# æµ‹è¯•æ‰€æœ‰åŒæ¨¡å‹ç­–ç•¥\n",
    "print(\"\\n Testing two-model ensemble strategies:\")\n",
    "best_hr3 = 0\n",
    "best_strategy_name = \"Proven_Success\"\n",
    "best_weights = [0.55, 0.45]\n",
    "\n",
    "for name, weights in strategies.items():\n",
    "    # åŠ æƒç»„åˆé¢„æµ‹\n",
    "    ensemble_pred = weights[0] * xgb_val_preds + weights[1] * dart_val_preds\n",
    "    \n",
    "    hr3 = hitrate_at_3(y_va.to_numpy().flatten(), ensemble_pred, groups_va.to_numpy().flatten())\n",
    "    \n",
    "    status = \"ğŸ”¥\" if hr3 > best_hr3 else \"  \"\n",
    "    print(f\"   {status} {name:16}: {hr3:.4f} [XGB:{weights[0]:.2f}, DART:{weights[1]:.2f}]\")\n",
    "    \n",
    "    if hr3 > best_hr3:\n",
    "        best_hr3 = hr3\n",
    "        best_strategy_name = name\n",
    "        best_weights = weights\n",
    "\n",
    "print(f\"\\nğŸ† Best two-model strategy: {best_strategy_name} (HR@3: {best_hr3:.4f})\")\n",
    "print(f\"   Optimal weights: XGBoost={best_weights[0]:.3f}, DART={best_weights[1]:.3f}\")\n",
    "print(f\"   Expected Kaggle improvement over 0.47497: +{(best_hr3-0.475)*1000:.1f} points\")\n",
    "\n",
    "print(\"\\nâœ… Two-model ensemble optimization completed!\")\n",
    "print(f\"ğŸ¯ Target 0.485+ score - Current validation: {best_hr3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6f80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T15:13:46.093318Z",
     "iopub.status.busy": "2025-07-19T15:13:46.093089Z",
     "iopub.status.idle": "2025-07-19T15:15:16.296165Z",
     "shell.execute_reply": "2025-07-19T15:15:16.290244Z"
    },
    "papermill": {
     "duration": 90.222289,
     "end_time": "2025-07-19T15:15:16.298701",
     "exception": false,
     "start_time": "2025-07-19T15:13:46.076412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KAGGLE-VERIFIED ENSEMBLE AND SUBMISSION ===\n",
      "Model availability check:\n",
      "  XGBoost: âœ…\n",
      "  LightGBM DART: âœ…\n",
      "  LightGBM GBDT: âŒ Removed (lowest performance)\n",
      "  Neural Network: âŒ Removed (performance drag)\n",
      "âœ… Both proven models are ready for ensemble!\n",
      "\n",
      "ğŸ“Š Generating two-model predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All two-model predictions generated successfully\n",
      "\n",
      " Individual model validation performance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  XGBoost: 0.5200\n",
      "  LightGBM_DART: 0.5143\n",
      "\n",
      " Calculating intelligent weights for two-model ensemble...\n",
      "\n",
      " Testing two-model ensemble strategies:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ”¥ Proven_Success  : 0.5334 [XGB:0.55, DART:0.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      DART_Strong     : 0.5297 [XGB:0.45, DART:0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ”¥ Performance_Based: 0.5340 [XGB:0.60, DART:0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Balanced        : 0.5314 [XGB:0.50, DART:0.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Conservative    : 0.5321 [XGB:0.52, DART:0.48]\n",
      "\n",
      "ğŸ† Best two-model strategy: Performance_Based (HR@3: 0.5340)\n",
      "   Optimal weights: XGBoost=0.600, DART=0.400\n",
      "   Expected Kaggle improvement over 0.47497: +59.0 points\n",
      "\n",
      "âœ… Two-model ensemble optimization completed!\n",
      "ğŸ¯ Target 0.50+ score - Current validation: 0.5340\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ KaggleéªŒè¯çš„é›†æˆä¸æäº¤ (æœ€ç»ˆç‰ˆæœ¬)\n",
    "print(\"=== KAGGLE-VERIFIED ENSEMBLE AND SUBMISSION ===\")\n",
    "\n",
    "# 1. éªŒè¯åŒæ¨¡å‹å‡†å¤‡å°±ç»ª\n",
    "models_ready = {\n",
    "    'XGBoost': 'xgb_model' in locals(),\n",
    "    'LightGBM DART': 'lgb_model_dart' in locals()\n",
    "}\n",
    "\n",
    "print(\"Model availability check:\")\n",
    "for model_name, ready in models_ready.items():\n",
    "    status = \"âœ…\" if ready else \"âŒ\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "print(\"  LightGBM GBDT: âŒ Removed (lowest performance)\")\n",
    "print(\"  Neural Network: âŒ Removed (performance drag)\")\n",
    "\n",
    "if not all(models_ready.values()):\n",
    "    print(\"  Required models are missing. Please run the training cells first.\")\n",
    "else:\n",
    "    print(\"âœ… Both proven models are ready for ensemble!\")\n",
    "    \n",
    "    # 2. ç”ŸæˆåŒæ¨¡å‹é¢„æµ‹\n",
    "    print(\"\\nğŸ“Š Generating two-model predictions...\")\n",
    "    try:\n",
    "        # éªŒè¯é›†é¢„æµ‹ (ä½¿ç”¨æ­£ç¡®çš„å˜é‡å)\n",
    "        xgb_val_preds = xgb_model.predict(dval)\n",
    "        dart_val_preds = lgb_model_dart.predict(X_va)  # ä¿®å¤: ä½¿ç”¨X_vaè€Œä¸æ˜¯X_va_lgb\n",
    "        # æµ‹è¯•é›†é¢„æµ‹\n",
    "        xgb_test_preds = xgb_model.predict(dtest)\n",
    "        dart_test_preds = lgb_model_dart.predict(X_te)  # ä¿®å¤: ä½¿ç”¨X_teè€Œä¸æ˜¯X_te_lgb\n",
    "        print(\"âœ… All two-model predictions generated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating predictions: {e}\")\n",
    "        print(\"Please check if both models are properly trained\")\n",
    "    \n",
    "    # 3. è®¡ç®—ä¸ªä½“æ¨¡å‹æ€§èƒ½\n",
    "    print(\"\\n Individual model validation performance:\")\n",
    "    val_hitrates = {}\n",
    "    val_hitrates['XGBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), xgb_val_preds, groups_va.to_numpy().flatten())\n",
    "    val_hitrates['LightGBM_DART'] = hitrate_at_3(y_va.to_numpy().flatten(), dart_val_preds, groups_va.to_numpy().flatten())\n",
    "    for model, hr in val_hitrates.items():\n",
    "        print(f\"  {model}: {hr:.4f}\")\n",
    "    # 4. æ™ºèƒ½åŒæ¨¡å‹æƒé‡ä¼˜åŒ– (åŸºäº0.47497æˆåŠŸç»éªŒ)\n",
    "    print(\"\\n Calculating intelligent weights for two-model ensemble...\")\n",
    "    historical_weights = {\n",
    "        # \"Proven_Success\": [0.55, 0.45],     # åŸºäº0.47497æˆåŠŸç»éªŒ\n",
    "        \"XGBoost_Heavy\": [0.65, 0.35],     # æ›´é‡è§†XGBoost\n",
    "        \"slight_xgb\":[0.59, 0.41],          # å¹³è¡¡ä½†ç•¥å€¾å‘XGBoost\n",
    "        \"more_xgb\":[0.61, 0.39],              # æ›´é‡è§†XGBoost\n",
    "        \"DART_Strong\": [0.45, 0.55],        # è€ƒè™‘DARTå¼ºåŠ²è¡¨ç°\n",
    "         # \"Performance_Based\": [0.6, 0.4],    # æ›´é‡è§†XGBoost\n",
    "        \"Balanced\": [0.5, 0.5],             # å¹³è¡¡æƒé‡\n",
    "        \"Conservative\": [0.52, 0.48],       # è½»å¾®å€¾å‘XGBoost\n",
    "    }\n",
    "    print(\"\\n Testing two-model ensemble strategies:\")\n",
    "    best_hr3 = 0\n",
    "    best_strategy_name = \"Proven_Success\"\n",
    "    best_weights = [0.55, 0.45]\n",
    "    for name, weights in historical_weights.items():\n",
    "        ensemble_pred = weights[0] * xgb_val_preds + weights[1] * dart_val_preds\n",
    "        hr3 = hitrate_at_3(y_va.to_numpy().flatten(), ensemble_pred, groups_va.to_numpy().flatten())\n",
    "        status = \"ğŸ”¥\" if hr3 > best_hr3 else \"  \"\n",
    "        print(f\"   {status} {name:16}: {hr3:.4f} [XGB:{weights[0]:.2f}, DART:{weights[1]:.2f}]\")\n",
    "        if hr3 > best_hr3:\n",
    "            best_hr3 = hr3\n",
    "            best_strategy_name = name\n",
    "            best_weights = weights\n",
    "    print(f\"\\nğŸ† Best two-model strategy: {best_strategy_name} (HR@3: {best_hr3:.4f})\")\n",
    "    print(f\"   Optimal weights: XGBoost={best_weights[0]:.3f}, DART={best_weights[1]:.3f}\")\n",
    "    print(f\"   Expected Kaggle improvement over 0.47497: +{(best_hr3-0.475)*1000:.1f} points\")\n",
    "    print(\"\\nâœ… Two-model ensemble optimization completed!\")\n",
    "    print(f\"ğŸ¯ Target 0.50+ score - Current validation: {best_hr3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d76902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T15:15:16.330429Z",
     "iopub.status.busy": "2025-07-19T15:15:16.330169Z",
     "iopub.status.idle": "2025-07-19T15:15:17.649418Z",
     "shell.execute_reply": "2025-07-19T15:15:17.644696Z"
    },
    "papermill": {
     "duration": 1.339629,
     "end_time": "2025-07-19T15:15:17.652564",
     "exception": false,
     "start_time": "2025-07-19T15:15:16.312935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating two-model predictions on test set...\n",
      "Both models have generated test predictions successfully!\n",
      "Test predictions prepared for 6897776 samples\n",
      "Models in ensemble:\n",
      "  âœ… XGBoost - Tree-based gradient boosting\n",
      "  âœ… LightGBM DART - Enhanced dropout regularization\n",
      "\n",
      "Proven two-model strategy with unified encoding!\n",
      "\n",
      "Creating optimized two-model ensemble submission...\n",
      "Applying best ensemble strategy: Performance_Based\n",
      "Weights: XGB=0.600, DART=0.400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final submission validation...\n",
      "Submission shape: (6897776, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ranker_ids: 45231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank range: 1 to 7022\n",
      "Validating submission format...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission format validation passed!\n",
      "\n",
      "Sample ensemble submission:\n",
      "shape: (10, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Id       â”† ranker_id                       â”† selected â”‚\n",
      "â”‚ ---      â”† ---                             â”† ---      â”‚\n",
      "â”‚ i64      â”† str                             â”† u32      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 18144679 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 17       â”‚\n",
      "â”‚ 18144680 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 34       â”‚\n",
      "â”‚ 18144681 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 204      â”‚\n",
      "â”‚ 18144682 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 98       â”‚\n",
      "â”‚ 18144683 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 82       â”‚\n",
      "â”‚ 18144684 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 206      â”‚\n",
      "â”‚ 18144685 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 351      â”‚\n",
      "â”‚ 18144686 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 32       â”‚\n",
      "â”‚ 18144687 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 59       â”‚\n",
      "â”‚ 18144688 â”† c9373e5f772e43d593dd6ad2fa90f6â€¦ â”† 239      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Ensemble submission saved: submission.csv\n",
      "ğŸš€ Targeting 0.50+ Kaggle score with outstanding performance!\n",
      "\n",
      "=== OUTSTANDING RESULTS SUMMARY ===\n",
      "âœ… XGBoost: 0.5200 HR@3\n",
      "âœ… LightGBM DART: 0.5143 HR@3\n",
      "ğŸ† Ensemble: 0.5340 HR@3\n",
      "ğŸ“ˆ Expected Kaggle score: 0.50+ (amazing improvement!)\n",
      "ğŸ‰ Performance gain: +59.0 points over 0.47497\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ç”ŸæˆåŒæ¨¡å‹æµ‹è¯•é¢„æµ‹å¹¶åˆ›å»ºæäº¤æ–‡ä»¶\n",
    "print(\"Generating two-model predictions on test set...\")\n",
    "\n",
    "if 'xgb_test_preds' not in locals() or 'dart_test_preds' not in locals():\n",
    "    print(\"Generating test predictions...\")\n",
    "    xgb_test_preds = xgb_model.predict(dtest)\n",
    "    dart_test_preds = lgb_model_dart.predict(X_te)\n",
    "\n",
    "print(\"Both models have generated test predictions successfully!\")\n",
    "print(f\"Test predictions prepared for {len(xgb_test_preds)} samples\")\n",
    "print(\"Models in ensemble:\")\n",
    "print(\"  âœ… XGBoost - Tree-based gradient boosting\")\n",
    "print(\"  âœ… LightGBM DART - Enhanced dropout regularization\")\n",
    "print()\n",
    "print(\"Proven two-model strategy with unified encoding!\")\n",
    "\n",
    "print(f\"\\nCreating optimized two-model ensemble submission...\")\n",
    "print(f\"Applying best ensemble strategy: {best_strategy_name}\")\n",
    "print(f\"Weights: XGB={best_weights[0]:.3f}, DART={best_weights[1]:.3f}\")\n",
    "\n",
    "final_test_ensemble = best_weights[0] * xgb_test_preds + best_weights[1] * dart_test_preds\n",
    "submission_df = test.select(['Id', 'ranker_id']).with_columns([\n",
    "    pl.Series('ensemble_score', final_test_ensemble)\n",
    "])\n",
    "final_submission = submission_df.with_columns([\n",
    "    pl.col('ensemble_score').rank(method='ordinal', descending=True).over('ranker_id').alias('selected')\n",
    "]).select(['Id', 'ranker_id', 'selected'])\n",
    "\n",
    "print(f\"Final submission validation...\")\n",
    "print(f\"Submission shape: {final_submission.shape}\")\n",
    "print(f\"Unique ranker_ids: {final_submission['ranker_id'].n_unique()}\")\n",
    "print(f\"Rank range: {final_submission['selected'].min()} to {final_submission['selected'].max()}\")\n",
    "\n",
    "# ä¿®å¤éªŒè¯é€»è¾‘ - ç®€åŒ–éªŒè¯è¿‡ç¨‹\n",
    "print(\"Validating submission format...\")\n",
    "sample_validation = final_submission.head(1000)\n",
    "validation_passed = True\n",
    "\n",
    "# æ£€æŸ¥åŸºæœ¬æ ¼å¼\n",
    "if final_submission.shape[1] != 3:\n",
    "    print(\"âŒ Submission should have exactly 3 columns\")\n",
    "    validation_passed = False\n",
    "    \n",
    "if not all(col in final_submission.columns for col in ['Id', 'ranker_id', 'selected']):\n",
    "    print(\"âŒ Missing required columns\")\n",
    "    validation_passed = False\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®ç±»å‹\n",
    "if final_submission['selected'].dtype not in [pl.Int32, pl.Int64, pl.UInt32, pl.UInt64]:\n",
    "    print(\"âŒ 'selected' column should be integer type\")\n",
    "    validation_passed = False\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"âœ… Submission format validation passed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Submission format issues detected\")\n",
    "\n",
    "print(f\"\\nSample ensemble submission:\")\n",
    "print(final_submission.head(10))\n",
    "final_submission.write_csv('submission.csv')\n",
    "print(f\"\\nğŸ¯ Ensemble submission saved: submission.csv\")\n",
    "print(f\"ğŸš€ Targeting 0.50+ Kaggle score with outstanding performance!\")\n",
    "\n",
    "print(f\"\\n=== OUTSTANDING RESULTS SUMMARY ===\")\n",
    "print(f\"âœ… XGBoost: {val_hitrates['XGBoost']:.4f} HR@3\")\n",
    "print(f\"âœ… LightGBM DART: {val_hitrates['LightGBM_DART']:.4f} HR@3\")\n",
    "print(f\"ğŸ† Ensemble: {best_hr3:.4f} HR@3\")\n",
    "print(f\"ğŸ“ˆ Expected Kaggle score: 0.50+ (amazing improvement!)\")\n",
    "print(f\"ğŸ‰ Performance gain: +{(best_hr3-0.475)*1000:.1f} points over 0.47497\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 12733338,
     "sourceId": 105399,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6631.264562,
   "end_time": "2025-07-19T15:15:27.702229",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-19T13:24:56.437667",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
