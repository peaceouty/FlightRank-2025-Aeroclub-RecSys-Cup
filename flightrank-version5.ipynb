{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9694ea5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:09:40.389896Z",
     "iopub.status.busy": "2025-07-11T07:09:40.389605Z",
     "iopub.status.idle": "2025-07-11T07:10:30.560310Z",
     "shell.execute_reply": "2025-07-11T07:10:30.555424Z"
    },
    "papermill": {
     "duration": 50.182085,
     "end_time": "2025-07-11T07:10:30.563006",
     "exception": false,
     "start_time": "2025-07-11T07:09:40.380921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U xgboost\n",
    "!pip install -U polars\n",
    "!pip install -U optuna\n",
    "!pip install -U catboost\n",
    "!pip install -U lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad35d9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:10:30.574601Z",
     "iopub.status.busy": "2025-07-11T07:10:30.574369Z",
     "iopub.status.idle": "2025-07-11T07:10:41.943963Z",
     "shell.execute_reply": "2025-07-11T07:10:41.940690Z"
    },
    "papermill": {
     "duration": 11.380667,
     "end_time": "2025-07-11T07:10:41.947763",
     "exception": false,
     "start_time": "2025-07-11T07:10:30.567096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cf327e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:10:41.960733Z",
     "iopub.status.busy": "2025-07-11T07:10:41.960364Z",
     "iopub.status.idle": "2025-07-11T07:10:47.223585Z",
     "shell.execute_reply": "2025-07-11T07:10:47.217524Z"
    },
    "papermill": {
     "duration": 5.273532,
     "end_time": "2025-07-11T07:10:47.226697",
     "exception": false,
     "start_time": "2025-07-11T07:10:41.953165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "系统找不到指定的路径。 (os error 3): /kaggle/input/aeroclub-recsys-2025/train.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/aeroclub-recsys-2025/train.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/aeroclub-recsys-2025/test.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mInt64)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m data_raw \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat((train, test))\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    125\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    126\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[0;32m    127\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    125\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    126\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[0;32m    127\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\io\\parquet\\functions.py:283\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m         lf \u001b[38;5;241m=\u001b[39m lf\u001b[38;5;241m.\u001b[39mselect(columns)\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min-memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\lazyframe\\opt_flags.py:330\u001b[0m, in \u001b[0;36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m         optflags \u001b[38;5;241m=\u001b[39m cb(optflags, kwargs\u001b[38;5;241m.\u001b[39mpop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[0;32m    329\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optflags\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ShuaiZhiyu\\anaconda3\\envs\\ML\\lib\\site-packages\\polars\\lazyframe\\frame.py:2332\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: 系统找不到指定的路径。 (os error 3): /kaggle/input/aeroclub-recsys-2025/train.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet').drop('__index_level_0__')\n",
    "test = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet').drop('__index_level_0__').with_columns(pl.lit(0, dtype=pl.Int64).alias(\"selected\"))\n",
    "\n",
    "data_raw = pl.concat((train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3398379",
   "metadata": {
    "papermill": {
     "duration": 0.003876,
     "end_time": "2025-07-11T07:10:47.235109",
     "exception": false,
     "start_time": "2025-07-11T07:10:47.231233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474131b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:10:47.246250Z",
     "iopub.status.busy": "2025-07-11T07:10:47.245997Z",
     "iopub.status.idle": "2025-07-11T07:10:47.257439Z",
     "shell.execute_reply": "2025-07-11T07:10:47.252251Z"
    },
    "papermill": {
     "duration": 0.020867,
     "end_time": "2025-07-11T07:10:47.259710",
     "exception": false,
     "start_time": "2025-07-11T07:10:47.238843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hitrate_at_3(y_true, y_pred, groups):\n",
    "    df = pl.DataFrame({\n",
    "        'group': groups,\n",
    "        'pred': y_pred,\n",
    "        'true': y_true\n",
    "    })\n",
    "    \n",
    "    return (\n",
    "        df.filter(pl.col(\"group\").count().over(\"group\") > 10)\n",
    "        .sort([\"group\", \"pred\"], descending=[False, True])\n",
    "        .group_by(\"group\", maintain_order=True)\n",
    "        .head(3)\n",
    "        .group_by(\"group\")\n",
    "        .agg(pl.col(\"true\").max())\n",
    "        .select(pl.col(\"true\").mean())\n",
    "        .item()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42595746",
   "metadata": {
    "papermill": {
     "duration": 0.003739,
     "end_time": "2025-07-11T07:10:47.267303",
     "exception": false,
     "start_time": "2025-07-11T07:10:47.263564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ef4d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:10:47.278544Z",
     "iopub.status.busy": "2025-07-11T07:10:47.278320Z",
     "iopub.status.idle": "2025-07-11T07:11:52.163072Z",
     "shell.execute_reply": "2025-07-11T07:11:52.158140Z"
    },
    "papermill": {
     "duration": 64.895176,
     "end_time": "2025-07-11T07:11:52.166468",
     "exception": false,
     "start_time": "2025-07-11T07:10:47.271292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = data_raw.clone()\n",
    "\n",
    "# More efficient duration to minutes converter\n",
    "def dur_to_min(col):\n",
    "    # Extract days and time parts in one pass\n",
    "    days = col.str.extract(r\"^(\\d+)\\.\", 1).cast(pl.Int64).fill_null(0) * 1440\n",
    "    time_str = pl.when(col.str.contains(r\"^\\d+\\.\")).then(col.str.replace(r\"^\\d+\\.\", \"\")).otherwise(col)\n",
    "    hours = time_str.str.extract(r\"^(\\d+):\", 1).cast(pl.Int64).fill_null(0) * 60\n",
    "    minutes = time_str.str.extract(r\":(\\d+):\", 1).cast(pl.Int64).fill_null(0)\n",
    "    return (days + hours + minutes).fill_null(0)\n",
    "\n",
    "# Process duration columns\n",
    "dur_cols = [\"legs0_duration\", \"legs1_duration\"] + [f\"legs{l}_segments{s}_duration\" for l in (0, 1) for s in (0, 1)]\n",
    "dur_exprs = [dur_to_min(pl.col(c)).alias(c) for c in dur_cols if c in df.columns]\n",
    "\n",
    "# Apply duration transformations first\n",
    "if dur_exprs:\n",
    "    df = df.with_columns(dur_exprs)\n",
    "\n",
    "# Precompute marketing carrier columns check\n",
    "mc_cols = [f'legs{l}_segments{s}_marketingCarrier_code' for l in (0, 1) for s in range(4)]\n",
    "mc_exists = [col for col in mc_cols if col in df.columns]\n",
    "\n",
    "# Combine all initial transformations\n",
    "df = df.with_columns([\n",
    "        # Price features\n",
    "        (pl.col(\"totalPrice\") / (pl.col(\"taxes\") + 1)).alias(\"price_per_tax\"),\n",
    "        (pl.col(\"taxes\") / (pl.col(\"totalPrice\") + 1)).alias(\"tax_rate\"),\n",
    "        pl.col(\"totalPrice\").log1p().alias(\"log_price\"),\n",
    "        \n",
    "        # Duration features\n",
    "        (pl.col(\"legs0_duration\").fill_null(0) + pl.col(\"legs1_duration\").fill_null(0)).alias(\"total_duration\"),\n",
    "        pl.when(pl.col(\"legs1_duration\").fill_null(0) > 0)\n",
    "            .then(pl.col(\"legs0_duration\") / (pl.col(\"legs1_duration\") + 1))\n",
    "            .otherwise(1.0).alias(\"duration_ratio\"),\n",
    "        \n",
    "        # Trip type\n",
    "        (pl.col(\"legs1_duration\").is_null() | \n",
    "         (pl.col(\"legs1_duration\") == 0) | \n",
    "         pl.col(\"legs1_segments0_departureFrom_airport_iata\").is_null()).cast(pl.Int32).alias(\"is_one_way\"),\n",
    "        \n",
    "        # Total segments count\n",
    "        (pl.sum_horizontal(pl.col(col).is_not_null().cast(pl.UInt8) for col in mc_exists) \n",
    "         if mc_exists else pl.lit(0)).alias(\"l0_seg\"),\n",
    "        \n",
    "        # FF features\n",
    "        (pl.col(\"frequentFlyer\").fill_null(\"\").str.count_matches(\"/\") + \n",
    "         (pl.col(\"frequentFlyer\").fill_null(\"\") != \"\").cast(pl.Int32)).alias(\"n_ff_programs\"),\n",
    "        \n",
    "        # Binary features\n",
    "        pl.col(\"corporateTariffCode\").is_not_null().cast(pl.Int32).alias(\"has_corporate_tariff\"),\n",
    "        (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"has_access_tp\"),\n",
    "        \n",
    "        # Baggage & fees\n",
    "        (pl.col(\"legs0_segments0_baggageAllowance_quantity\").fill_null(0) + \n",
    "         pl.col(\"legs1_segments0_baggageAllowance_quantity\").fill_null(0)).alias(\"baggage_total\"),\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(0) + \n",
    "         pl.col(\"miniRules1_monetaryAmount\").fill_null(0)).alias(\"total_fees\"),\n",
    "        \n",
    "        # Routes & carriers\n",
    "        pl.col(\"searchRoute\").is_in([\"MOWLED/LEDMOW\", \"LEDMOW/MOWLED\", \"MOWLED\", \"LEDMOW\", \"MOWAER/AERMOW\"])\n",
    "            .cast(pl.Int32).alias(\"is_popular_route\"),\n",
    "        \n",
    "        # Cabin\n",
    "        pl.mean_horizontal([\"legs0_segments0_cabinClass\", \"legs1_segments0_cabinClass\"]).alias(\"avg_cabin_class\"),\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(0) - \n",
    "         pl.col(\"legs1_segments0_cabinClass\").fill_null(0)).alias(\"cabin_class_diff\"),\n",
    "])\n",
    "\n",
    "# Segment counts - more efficient\n",
    "seg_exprs = []\n",
    "for leg in (0, 1):\n",
    "    seg_cols = [f\"legs{leg}_segments{s}_duration\" for s in range(4) if f\"legs{leg}_segments{s}_duration\" in df.columns]\n",
    "    if seg_cols:\n",
    "        seg_exprs.append(\n",
    "            pl.sum_horizontal(pl.col(c).is_not_null() for c in seg_cols)\n",
    "                .cast(pl.Int32).alias(f\"n_segments_leg{leg}\")\n",
    "        )\n",
    "    else:\n",
    "        seg_exprs.append(pl.lit(0).cast(pl.Int32).alias(f\"n_segments_leg{leg}\"))\n",
    "\n",
    "# Add segment-based features\n",
    "# First create segment counts\n",
    "df = df.with_columns(seg_exprs)\n",
    "\n",
    "# Then use them for derived features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"n_segments_leg0\") + pl.col(\"n_segments_leg1\")).alias(\"total_segments\"),\n",
    "    (pl.col(\"n_segments_leg0\") == 1).cast(pl.Int32).alias(\"is_direct_leg0\"),\n",
    "    pl.when(pl.col(\"is_one_way\") == 1).then(0)\n",
    "        .otherwise((pl.col(\"n_segments_leg1\") == 1).cast(pl.Int32)).alias(\"is_direct_leg1\"),\n",
    "])\n",
    "\n",
    "# More derived features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"is_direct_leg0\") & pl.col(\"is_direct_leg1\")).cast(pl.Int32).alias(\"both_direct\"),\n",
    "    ((pl.col(\"isVip\") == 1) | (pl.col(\"n_ff_programs\") > 0)).cast(pl.Int32).alias(\"is_vip_freq\"),\n",
    "    (pl.col(\"baggage_total\") > 0).cast(pl.Int32).alias(\"has_baggage\"),\n",
    "    (pl.col(\"total_fees\") > 0).cast(pl.Int32).alias(\"has_fees\"),\n",
    "    (pl.col(\"total_fees\") / (pl.col(\"totalPrice\") + 1)).alias(\"fee_rate\"),\n",
    "    pl.col(\"Id\").count().over(\"ranker_id\").alias(\"group_size\"),\n",
    "])\n",
    "\n",
    "# Add major carrier flag if column exists\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\"])\n",
    "            .cast(pl.Int32).alias(\"is_major_carrier\")\n",
    "    )\n",
    "else:\n",
    "    df = df.with_columns(pl.lit(0).alias(\"is_major_carrier\"))\n",
    "\n",
    "df = df.with_columns(pl.col(\"group_size\").log1p().alias(\"group_size_log\"))\n",
    "\n",
    "# Time features - batch process\n",
    "time_exprs = []\n",
    "for col in (\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"):\n",
    "    if col in df.columns:\n",
    "        dt = pl.col(col).str.to_datetime(strict=False)\n",
    "        h = dt.dt.hour().fill_null(12)\n",
    "        time_exprs.extend([\n",
    "            h.alias(f\"{col}_hour\"),\n",
    "            dt.dt.weekday().fill_null(0).alias(f\"{col}_weekday\"),\n",
    "            (((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))).cast(pl.Int32).alias(f\"{col}_business_time\")\n",
    "        ])\n",
    "if time_exprs:\n",
    "    df = df.with_columns(time_exprs)\n",
    "\n",
    "# Batch rank computations - more efficient with single pass\n",
    "# First apply the columns that will be used for ranking\n",
    "df = df.with_columns([\n",
    "    pl.col(\"group_size\").log1p().alias(\"group_size_log\"),\n",
    "])\n",
    "\n",
    "# Price and duration basic ranks\n",
    "rank_exprs = []\n",
    "for col, alias in [(\"totalPrice\", \"price\"), (\"total_duration\", \"duration\")]:\n",
    "    rank_exprs.append(pl.col(col).rank().over(\"ranker_id\").alias(f\"{alias}_rank\"))\n",
    "\n",
    "# Price-specific features\n",
    "price_exprs = [\n",
    "    (pl.col(\"totalPrice\").rank(\"average\").over(\"ranker_id\") / \n",
    "     pl.col(\"totalPrice\").count().over(\"ranker_id\")).alias(\"price_pct_rank\"),\n",
    "    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheapest\"),\n",
    "    ((pl.col(\"totalPrice\") - pl.col(\"totalPrice\").median().over(\"ranker_id\")) / \n",
    "     (pl.col(\"totalPrice\").std().over(\"ranker_id\") + 1)).alias(\"price_from_median\"),\n",
    "    (pl.col(\"l0_seg\") == pl.col(\"l0_seg\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_min_segments\"),\n",
    "]\n",
    "\n",
    "# Apply initial ranks\n",
    "df = df.with_columns(rank_exprs + price_exprs)\n",
    "\n",
    "# Cheapest direct - more efficient\n",
    "direct_cheapest = (\n",
    "    df.filter(pl.col(\"is_direct_leg0\") == 1)\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.col(\"totalPrice\").min().alias(\"min_direct\"))\n",
    ")\n",
    "\n",
    "df = df.join(direct_cheapest, on=\"ranker_id\", how=\"left\").with_columns(\n",
    "    ((pl.col(\"is_direct_leg0\") == 1) & \n",
    "     (pl.col(\"totalPrice\") == pl.col(\"min_direct\"))).cast(pl.Int32).fill_null(0).alias(\"is_direct_cheapest\")\n",
    ").drop(\"min_direct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering - Focus on Business Traveler Preferences\n",
    "print(\"Adding advanced business traveler features...\")\n",
    "\n",
    "# 1. Enhanced price and policy features\n",
    "df = df.with_columns([\n",
    "    # Corporate policy compliance\n",
    "    (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"policy_compliant\"),\n",
    "    \n",
    "    # Enhanced price buckets with controlled group stats\n",
    "    pl.when(pl.col(\"price_pct_rank\") <= 0.15).then(1)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.3).then(2)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.5).then(3)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.7).then(4)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.85).then(5)\n",
    "    .otherwise(6).alias(\"price_bucket\"),\n",
    "    \n",
    "    # Price competitiveness (keep essential group features)\n",
    "    (pl.col(\"totalPrice\") <= pl.col(\"totalPrice\").quantile(0.25).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheap_quartile\"),\n",
    "    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_absolute_cheapest\"),\n",
    "    \n",
    "    # Tax efficiency for business\n",
    "    pl.when(pl.col(\"taxes\") > 0).then(pl.col(\"totalPrice\") / pl.col(\"taxes\")).otherwise(0).alias(\"price_tax_efficiency\"),\n",
    "])\n",
    "\n",
    "# 2. Advanced time features for business travelers\n",
    "time_features = []\n",
    "for prefix in [\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"]:\n",
    "    if f\"{prefix}_hour\" in df.columns:\n",
    "        time_features.extend([\n",
    "            # Premium business hours\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 7) & (pl.col(f\"{prefix}_hour\") <= 9)).cast(pl.Int32).alias(f\"{prefix}_morning_business\"),\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 17) & (pl.col(f\"{prefix}_hour\") <= 19)).cast(pl.Int32).alias(f\"{prefix}_evening_business\"),\n",
    "            # Avoid red-eye flights\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 23) | (pl.col(f\"{prefix}_hour\") <= 5)).cast(pl.Int32).alias(f\"{prefix}_red_eye\"),\n",
    "            # Premium time slots\n",
    "            (pl.col(f\"{prefix}_hour\").is_in([7, 8, 9, 18, 19, 20])).cast(pl.Int32).alias(f\"{prefix}_premium_time\"),\n",
    "            # Weekend patterns\n",
    "            ((pl.col(f\"{prefix}_weekday\") >= 5) & (pl.col(f\"{prefix}_hour\") >= 10)).cast(pl.Int32).alias(f\"{prefix}_weekend_leisure\"),\n",
    "        ])\n",
    "\n",
    "if time_features:\n",
    "    df = df.with_columns(time_features)\n",
    "\n",
    "# 3. Seat availability and scarcity features\n",
    "seat_features = []\n",
    "for leg in [0, 1]:\n",
    "    for seg in [0, 1]:\n",
    "        seat_col = f\"legs{leg}_segments{seg}_seatsAvailable\"\n",
    "        if seat_col in df.columns:\n",
    "            seat_features.extend([\n",
    "                # Seat scarcity (low availability premium)\n",
    "                (pl.col(seat_col).fill_null(0) <= 5).cast(pl.Int32).alias(f\"{seat_col}_scarce\"),\n",
    "                # High availability\n",
    "                (pl.col(seat_col).fill_null(0) >= 20).cast(pl.Int32).alias(f\"{seat_col}_abundant\"),\n",
    "                # Normalized seat availability within group\n",
    "                (pl.col(seat_col) / (pl.col(seat_col).mean().over(\"ranker_id\") + 1)).alias(f\"{seat_col}_relative\"),\n",
    "            ])\n",
    "\n",
    "if seat_features:\n",
    "    df = df.with_columns(seat_features)\n",
    "\n",
    "# 4. Cancellation and change policy features (critical for business)\n",
    "policy_features = []\n",
    "has_cancellation = False\n",
    "has_exchange = False\n",
    "\n",
    "# Check if cancellation features exist\n",
    "if \"miniRules0_monetaryAmount\" in df.columns:\n",
    "    has_cancellation = True\n",
    "    policy_features.extend([\n",
    "        # Flexible cancellation\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(999999) == 0).cast(pl.Int32).alias(\"free_cancellation\"),\n",
    "        # Low cancellation fee\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(999999) <= pl.col(\"totalPrice\") * 0.1).cast(pl.Int32).alias(\"low_cancel_fee\"),\n",
    "        # Cancellation fee rate\n",
    "        (pl.col(\"miniRules0_monetaryAmount\") / (pl.col(\"totalPrice\") + 1)).alias(\"cancel_fee_rate\"),\n",
    "    ])\n",
    "\n",
    "if \"miniRules1_monetaryAmount\" in df.columns:\n",
    "    has_exchange = True\n",
    "    policy_features.extend([\n",
    "        # Flexible exchange\n",
    "        (pl.col(\"miniRules1_monetaryAmount\").fill_null(999999) == 0).cast(pl.Int32).alias(\"free_exchange\"),\n",
    "        # Low exchange fee\n",
    "        (pl.col(\"miniRules1_monetaryAmount\").fill_null(999999) <= pl.col(\"totalPrice\") * 0.1).cast(pl.Int32).alias(\"low_exchange_fee\"),\n",
    "        # Exchange fee rate\n",
    "        (pl.col(\"miniRules1_monetaryAmount\") / (pl.col(\"totalPrice\") + 1)).alias(\"exchange_fee_rate\"),\n",
    "    ])\n",
    "\n",
    "if policy_features:\n",
    "    df = df.with_columns(policy_features)\n",
    "\n",
    "# 5. Route and carrier sophistication\n",
    "route_features = []\n",
    "has_airports = False\n",
    "has_carriers = False\n",
    "\n",
    "if \"legs0_segments0_departureFrom_airport_iata\" in df.columns:\n",
    "    has_airports = True\n",
    "    route_features.extend([\n",
    "        # Major hub airports (Moscow, St. Petersburg)\n",
    "        pl.col(\"legs0_segments0_departureFrom_airport_iata\").is_in([\"SVO\", \"DME\", \"VKO\", \"LED\", \"PKC\"])\n",
    "        .cast(pl.Int32).alias(\"departs_from_major_hub\"),\n",
    "        \n",
    "        # International airports\n",
    "        pl.col(\"legs0_segments0_arrivalTo_airport_iata\").is_in([\"LED\", \"SVO\", \"DME\", \"VKO\"])\n",
    "        .cast(pl.Int32).alias(\"arrives_to_major_hub\"),\n",
    "        \n",
    "        # Airport consistency (same departure/arrival airports for round trips)\n",
    "        (pl.col(\"legs0_segments0_departureFrom_airport_iata\") == \n",
    "         pl.col(\"legs1_segments0_arrivalTo_airport_iata\").fill_null(\"\")).cast(pl.Int32).alias(\"consistent_airports\"),\n",
    "    ])\n",
    "\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    has_carriers = True\n",
    "    route_features.extend([\n",
    "        # Carrier consistency across legs\n",
    "        (pl.col(\"legs0_segments0_marketingCarrier_code\") == \n",
    "         pl.col(\"legs1_segments0_marketingCarrier_code\").fill_null(\"\")).cast(pl.Int32).alias(\"same_carrier_both_legs\"),\n",
    "        \n",
    "        # Premium carriers (Aeroflot, S7, etc.)\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\", \"DP\"])\n",
    "        .cast(pl.Int32).alias(\"is_premium_carrier\"),\n",
    "        \n",
    "        # Marketing vs Operating carrier alignment\n",
    "        (pl.col(\"legs0_segments0_marketingCarrier_code\") == \n",
    "         pl.col(\"legs0_segments0_operatingCarrier_code\").fill_null(\"\")).cast(pl.Int32).alias(\"aligned_carriers_leg0\"),\n",
    "    ])\n",
    "\n",
    "if route_features:\n",
    "    df = df.with_columns(route_features)\n",
    "\n",
    "# 6. Aircraft and service quality features\n",
    "aircraft_features = []\n",
    "has_aircraft = False\n",
    "\n",
    "if \"legs0_segments0_aircraft_code\" in df.columns:\n",
    "    has_aircraft = True\n",
    "    aircraft_features.extend([\n",
    "        # Wide-body aircraft (better for long flights)\n",
    "        pl.col(\"legs0_segments0_aircraft_code\").is_in([\"330\", \"777\", \"787\", \"320\", \"321\"])\n",
    "        .cast(pl.Int32).alias(\"wide_body_leg0\"),\n",
    "        \n",
    "        # Modern aircraft\n",
    "        pl.col(\"legs0_segments0_aircraft_code\").is_in([\"787\", \"350\", \"320\", \"321\", \"737\"])\n",
    "        .cast(pl.Int32).alias(\"modern_aircraft_leg0\"),\n",
    "    ])\n",
    "\n",
    "if aircraft_features:\n",
    "    df = df.with_columns(aircraft_features)\n",
    "\n",
    "# 7. Cabin class optimization\n",
    "cabin_features = []\n",
    "has_cabin = False\n",
    "\n",
    "if \"legs0_segments0_cabinClass\" in df.columns:\n",
    "    has_cabin = True\n",
    "    cabin_features.extend([\n",
    "        # Business class upgrade availability\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(1) >= 2).cast(pl.Int32).alias(\"business_class_leg0\"),\n",
    "        # Premium economy or better\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(1) >= 1.5).cast(pl.Int32).alias(\"premium_class_leg0\"),\n",
    "        # Cabin class consistency\n",
    "        (pl.col(\"legs0_segments0_cabinClass\") == pl.col(\"legs1_segments0_cabinClass\")).cast(pl.Int32).alias(\"consistent_cabin\"),\n",
    "    ])\n",
    "\n",
    "if cabin_features:\n",
    "    df = df.with_columns(cabin_features)\n",
    "\n",
    "# 8. Competitive analysis within group (basic features first)\n",
    "competitive_features = [\n",
    "    # Direct flight premium\n",
    "    pl.col(\"both_direct\").sum().over(\"ranker_id\").alias(\"n_direct_options\"),\n",
    "    \n",
    "    # Premium combinations\n",
    "    ((pl.col(\"both_direct\") == 1) & (pl.col(\"is_cheap_quartile\") == 1)).cast(pl.Int32).alias(\"is_cheap_direct\"),\n",
    "    ((pl.col(\"policy_compliant\") == 1) & (pl.col(\"is_cheap_quartile\") == 1)).cast(pl.Int32).alias(\"compliant_and_cheap\"),\n",
    "    \n",
    "    # Business convenience\n",
    "    ((pl.col(\"legs0_departureAt_hour\").fill_null(12) >= 7) & \n",
    "     (pl.col(\"legs0_departureAt_hour\").fill_null(12) <= 9)).cast(pl.Int32).alias(\"morning_departure_business\"),\n",
    "    \n",
    "    # Fast and convenient\n",
    "    (pl.col(\"total_duration\") <= pl.col(\"total_duration\").quantile(0.3).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_fast_option\"),\n",
    "]\n",
    "\n",
    "df = df.with_columns(competitive_features)\n",
    "\n",
    "# 8b. Advanced competitive features (requires previous features to exist)\n",
    "advanced_competitive = []\n",
    "\n",
    "if has_carriers:\n",
    "    advanced_competitive.extend([\n",
    "        # VIP treatment with premium carriers\n",
    "        ((pl.col(\"is_vip_freq\") == 1) & (pl.col(\"is_premium_carrier\") == 1)).cast(pl.Int32).alias(\"vip_premium_carrier\"),\n",
    "    ])\n",
    "\n",
    "if has_cancellation and has_carriers:\n",
    "    advanced_competitive.extend([\n",
    "        # Flexible and premium\n",
    "        ((pl.col(\"free_cancellation\") == 1) & (pl.col(\"is_premium_carrier\") == 1)).cast(pl.Int32).alias(\"flexible_premium\"),\n",
    "    ])\n",
    "\n",
    "if advanced_competitive:\n",
    "    df = df.with_columns(advanced_competitive)\n",
    "\n",
    "# 9. High-impact interaction features (safe with all previous features defined)\n",
    "interaction_features = [\n",
    "    # Price-policy compliance\n",
    "    (pl.col(\"price_bucket\") * pl.col(\"policy_compliant\")).alias(\"price_policy_interaction\"),\n",
    "    \n",
    "    # Business convenience score\n",
    "    (pl.col(\"morning_departure_business\") * pl.col(\"is_cheap_quartile\") * pl.col(\"both_direct\")).alias(\"business_value_score\"),\n",
    "    \n",
    "    # Hub efficiency (basic version)\n",
    "    (pl.col(\"both_direct\") * pl.col(\"is_fast_option\")).alias(\"hub_efficiency_score\"),\n",
    "    \n",
    "    # Corporate optimization (basic version)\n",
    "    (pl.col(\"policy_compliant\") * pl.col(\"compliant_and_cheap\")).alias(\"corporate_optimization\"),\n",
    "]\n",
    "\n",
    "# Add premium service interaction if features exist\n",
    "if has_carriers and has_cabin:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"is_premium_carrier\") * pl.col(\"business_class_leg0\")).alias(\"premium_service_score\")\n",
    "    )\n",
    "\n",
    "# Add enhanced hub efficiency if airport features exist\n",
    "if has_airports:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"departs_from_major_hub\") * pl.col(\"both_direct\") * pl.col(\"is_fast_option\")).alias(\"enhanced_hub_efficiency_score\")\n",
    "    )\n",
    "\n",
    "# Add enhanced corporate optimization if carrier features exist\n",
    "if has_carriers:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"policy_compliant\") * pl.col(\"compliant_and_cheap\") * pl.col(\"same_carrier_both_legs\")).alias(\"enhanced_corporate_optimization\")\n",
    "    )\n",
    "\n",
    "df = df.with_columns(interaction_features)\n",
    "\n",
    "print(\"Advanced business traveler features added successfully!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"- Corporate policy compliance and tax efficiency\")\n",
    "print(\"- Advanced time preferences for business travelers\")\n",
    "print(\"- Seat scarcity and availability features\")\n",
    "print(\"- Cancellation/exchange policy flexibility\")\n",
    "print(\"- Premium carrier and aircraft preferences\")\n",
    "print(\"- Hub airport and route optimization\")\n",
    "print(\"- Strategic business value interactions\")\n",
    "print(\"- Safe feature creation with proper dependency ordering\")\n",
    "print(\"- Targeting 0.5+ Kaggle score with business insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de99f3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:11:52.179060Z",
     "iopub.status.busy": "2025-07-11T07:11:52.178806Z",
     "iopub.status.idle": "2025-07-11T07:11:56.002276Z",
     "shell.execute_reply": "2025-07-11T07:11:55.997176Z"
    },
    "papermill": {
     "duration": 3.834337,
     "end_time": "2025-07-11T07:11:56.005408",
     "exception": false,
     "start_time": "2025-07-11T07:11:52.171071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill nulls\n",
    "data = df.with_columns(\n",
    "    [pl.col(c).fill_null(0) for c in df.select(pl.selectors.numeric()).columns] +\n",
    "    [pl.col(c).fill_null(\"missing\") for c in df.select(pl.selectors.string()).columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7420b",
   "metadata": {
    "papermill": {
     "duration": 0.004204,
     "end_time": "2025-07-11T07:11:56.014436",
     "exception": false,
     "start_time": "2025-07-11T07:11:56.010232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4107c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:11:56.026607Z",
     "iopub.status.busy": "2025-07-11T07:11:56.026285Z",
     "iopub.status.idle": "2025-07-11T07:11:56.042831Z",
     "shell.execute_reply": "2025-07-11T07:11:56.038110Z"
    },
    "papermill": {
     "duration": 0.026738,
     "end_time": "2025-07-11T07:11:56.045338",
     "exception": false,
     "start_time": "2025-07-11T07:11:56.018600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 112 features (34 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "cat_features = [\n",
    "    'nationality', 'searchRoute', 'corporateTariffCode',\n",
    "    'bySelf', 'sex', 'companyID',\n",
    "    # Leg 0 segments 0-1\n",
    "    'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',\n",
    "    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',\n",
    "    'legs0_segments0_flightNumber',\n",
    "    'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',\n",
    "    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',\n",
    "    'legs0_segments1_flightNumber',\n",
    "    # Leg 1 segments 0-1\n",
    "    'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',\n",
    "    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',\n",
    "    'legs1_segments0_flightNumber',\n",
    "    'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',\n",
    "    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',\n",
    "    'legs1_segments1_flightNumber',\n",
    "    # Only add price_bucket as new categorical\n",
    "    'price_bucket'\n",
    "]\n",
    "\n",
    "# Columns to exclude (uninformative or problematic)\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing\n",
    "    'frequentFlyer',  # Already processed\n",
    "    # Exclude constant columns\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "\n",
    "# Exclude segment 2-3 columns (>98% missing)\n",
    "for leg in [0, 1]:\n",
    "    for seg in [2, 3]:\n",
    "        for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',\n",
    "                      'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',\n",
    "                      'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',\n",
    "                      'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:\n",
    "            exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "cat_features_final = [col for col in cat_features if col in feature_cols]\n",
    "\n",
    "# Create CatBoost categorical feature indices (column positions in feature_cols)\n",
    "catboost_cat_indices = [i for i, col in enumerate(feature_cols) if col in cat_features_final]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features ({len(cat_features_final)} categorical)\")\n",
    "print(f\"CatBoost categorical indices: {len(catboost_cat_indices)} features\")\n",
    "\n",
    "X = data.select(feature_cols)\n",
    "y = data.select('selected')\n",
    "groups = data.select('ranker_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea663e",
   "metadata": {
    "papermill": {
     "duration": 0.004457,
     "end_time": "2025-07-11T07:11:56.053943",
     "exception": false,
     "start_time": "2025-07-11T07:11:56.049486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training and Tuning\n",
    "\n",
    "### 🔍 Early CatBoost Validation (Preventing Late Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED Early CatBoost Validation (Using SAME logic as actual training)\n",
    "print(\"Performing FIXED Early CatBoost Validation...\")\n",
    "\n",
    "# Check 1: Verify catboost_cat_indices exists and is properly defined\n",
    "try:\n",
    "    if 'catboost_cat_indices' not in locals():\n",
    "        print(\"ERROR: catboost_cat_indices is not defined!\")\n",
    "        print(\"   This variable is required for CatBoost Pool creation.\")\n",
    "        print(\"   Expected: List of column indices for categorical features\")\n",
    "        raise NameError(\"catboost_cat_indices is not defined in Feature Selection section\")\n",
    "    else:\n",
    "        print(f\"catboost_cat_indices found: {len(catboost_cat_indices)} categorical features\")\n",
    "        print(f\"   Indices: {catboost_cat_indices[:10]}{'...' if len(catboost_cat_indices) > 10 else ''}\")\n",
    "except NameError as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "    print(\"This error would occur during CatBoost training after hours of waiting!\")\n",
    "    print(\"   Solution: Define catboost_cat_indices in Feature Selection section\")\n",
    "    raise\n",
    "\n",
    "# Check 2: Verify categorical features exist in dataset\n",
    "try:\n",
    "    missing_cat_features = [col for col in cat_features_final if col not in feature_cols]\n",
    "    if missing_cat_features:\n",
    "        print(f\"WARNING: {len(missing_cat_features)} categorical features missing from feature_cols\")\n",
    "        print(f\"   Missing: {missing_cat_features[:5]}{'...' if len(missing_cat_features) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"All {len(cat_features_final)} categorical features found in dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in categorical feature validation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check 3: Test CatBoost Pool creation with EXACT SAME logic as actual training\n",
    "try:\n",
    "    print(\"Testing CatBoost Pool creation with EXACT SAME logic as actual training...\")\n",
    "    \n",
    "    # Take a small sample for testing\n",
    "    sample_size = min(1000, len(X))\n",
    "    \n",
    "    # CRITICAL: Use EXACT SAME unified categorical encoding as actual training\n",
    "    print(\"   Applying EXACT SAME unified categorical encoding as actual training...\")\n",
    "    \n",
    "    # Step 1: Get sample data (same as actual training)\n",
    "    X_sample_test = X.head(sample_size).clone()\n",
    "    y_sample = y.head(sample_size).to_numpy().flatten()\n",
    "    groups_sample = groups.head(sample_size).to_numpy().flatten()\n",
    "    \n",
    "    # Step 2: Apply EXACT SAME unified categorical encoding as actual training\n",
    "    # CRITICAL: Use EXACT SAME logic as the actual CatBoost training cell\n",
    "    for col in cat_features_final:\n",
    "        print(f\"      Processing categorical feature: {col}\")\n",
    "        \n",
    "        # FIXED: Use the EXACT SAME unified approach as actual training\n",
    "        # Create mapping from ALL unique values in the sample (simulating full dataset)\n",
    "        all_values = X_sample_test.select(col).unique().sort(col)\n",
    "        \n",
    "        # Create EXACT SAME mapping: unique values -> integers (0, 1, 2, ...)\n",
    "        mapping_dict = {\n",
    "            val: idx for idx, val in enumerate(all_values[col].to_list())\n",
    "        }\n",
    "        \n",
    "        # Apply EXACT SAME mapping with map_elements and explicit Int32\n",
    "        X_sample_test = X_sample_test.with_columns(\n",
    "            pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "        )\n",
    "    \n",
    "    # Step 3: Convert numerical features to float32 (SAME AS TRAINING)\n",
    "    non_cat_features = [col for col in feature_cols if col not in cat_features_final]\n",
    "    for col in non_cat_features:\n",
    "        X_sample_test = X_sample_test.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    \n",
    "    # Step 4: Convert to numpy (SAME AS TRAINING)\n",
    "    sample_data = X_sample_test.to_numpy()\n",
    "    \n",
    "    # Verify data types (should match training)\n",
    "    print(f\"   Sample data shape: {sample_data.shape}\")\n",
    "    print(f\"   Sample data dtype: {sample_data.dtype}\")\n",
    "    print(f\"   Categorical indices: {catboost_cat_indices[:5]}...\")\n",
    "    \n",
    "    # Check categorical columns data types\n",
    "    for i, cat_idx in enumerate(catboost_cat_indices[:3]):\n",
    "        col_data = sample_data[:, cat_idx]\n",
    "        print(f\"   Cat feature {i} (col {cat_idx}): dtype={col_data.dtype}, sample={col_data[:3]}\")\n",
    "    \n",
    "    # Step 5: Test Pool creation with EXACT SAME logic as actual training\n",
    "    from catboost import Pool\n",
    "    test_pool = Pool(\n",
    "        data=sample_data,\n",
    "        label=y_sample,\n",
    "        group_id=groups_sample,\n",
    "        cat_features=catboost_cat_indices\n",
    "    )\n",
    "    \n",
    "    print(f\"CatBoost Pool created successfully with EXACT SAME logic as training!\")\n",
    "    print(f\"   Sample size: {sample_size} rows\")\n",
    "    print(f\"   Features: {sample_data.shape[1]} columns\")\n",
    "    print(f\"   Categorical features: {len(catboost_cat_indices)} indices\")\n",
    "    print(f\"   Data types verified: mixed int32/float32 array compatible with CatBoost\")\n",
    "    \n",
    "    # Step 6: Test quick model creation with SAME parameters as actual training\n",
    "    from catboost import CatBoostRanker\n",
    "    test_model = CatBoostRanker(\n",
    "        loss_function='YetiRank',\n",
    "        iterations=10,  # Just a few iterations for testing\n",
    "        verbose=False,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    test_model.fit(test_pool)\n",
    "    test_preds = test_model.predict(test_pool)\n",
    "    \n",
    "    print(f\"CatBoost model training test successful with EXACT SAME logic!\")\n",
    "    print(f\"   Predictions shape: {test_preds.shape}\")\n",
    "    print(f\"   Sample predictions: {test_preds[:5]}\")\n",
    "    print(f\"   No data type conflicts detected!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR in CatBoost validation: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    print(\"This error would occur during actual training after hours!\")\n",
    "    print(\"   Fix required before proceeding with full training\")\n",
    "    if \"floating point numerical type\" in str(e):\n",
    "        print(\"   IDENTIFIED: Data type mismatch - categorical features in float array\")\n",
    "        print(\"   SOLUTION: Use unified categorical encoding with proper type control\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nEarly CatBoost Validation PASSED with FIXED unified encoding!\")\n",
    "print(\"All CatBoost components verified and working\")\n",
    "print(\"Data type processing EXACTLY matches actual training\")\n",
    "print(\"No late-stage errors expected during actual training\")\n",
    "print(\"Safe to proceed with full model training pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b656e",
   "metadata": {
    "papermill": {
     "duration": 0.004107,
     "end_time": "2025-07-11T07:11:56.062510",
     "exception": false,
     "start_time": "2025-07-11T07:11:56.058403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7be53d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:11:56.116479Z",
     "iopub.status.busy": "2025-07-11T07:11:56.116209Z",
     "iopub.status.idle": "2025-07-11T07:12:40.145635Z",
     "shell.execute_reply": "2025-07-11T07:12:40.139921Z"
    },
    "papermill": {
     "duration": 44.08218,
     "end_time": "2025-07-11T07:12:40.148977",
     "exception": false,
     "start_time": "2025-07-11T07:11:56.066797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_xgb = X.with_columns([(pl.col(c).rank(\"dense\") - 1).fill_null(-1).cast(pl.Int32) for c in cat_features_final])\n",
    "\n",
    "n1 = 16487352 # split train to train and val (10%) in time\n",
    "n2 = train.height\n",
    "data_xgb_tr, data_xgb_va, data_xgb_te = data_xgb[:n1], data_xgb[n1:n2], data_xgb[n2:]\n",
    "y_tr, y_va, y_te = y[:n1], y[n1:n2], y[n2:]\n",
    "groups_tr, groups_va, groups_te = groups[:n1], groups[n1:n2], groups[n2:]\n",
    "\n",
    "group_sizes_tr = groups_tr.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_va = groups_va.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_te = groups_te.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "dtrain = xgb.DMatrix(data_xgb_tr, label=y_tr, group=group_sizes_tr, feature_names=data_xgb.columns)\n",
    "dval   = xgb.DMatrix(data_xgb_va, label=y_va, group=group_sizes_va, feature_names=data_xgb.columns)\n",
    "dtest  = xgb.DMatrix(data_xgb_te, label=y_te, group=group_sizes_te, feature_names=data_xgb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756e108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:12:40.161944Z",
     "iopub.status.busy": "2025-07-11T07:12:40.161674Z",
     "iopub.status.idle": "2025-07-11T07:22:29.447824Z",
     "shell.execute_reply": "2025-07-11T07:22:29.441561Z"
    },
    "papermill": {
     "duration": 589.297017,
     "end_time": "2025-07-11T07:22:29.450692",
     "exception": false,
     "start_time": "2025-07-11T07:12:40.153675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final XGBoost model with optimized parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.78524\tval-ndcg@3:0.80685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-ndcg@3:0.81400\tval-ndcg@3:0.83248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-ndcg@3:0.82217\tval-ndcg@3:0.83573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttrain-ndcg@3:0.82861\tval-ndcg@3:0.83820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain-ndcg@3:0.83513\tval-ndcg@3:0.83957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain-ndcg@3:0.84131\tval-ndcg@3:0.84052"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttrain-ndcg@3:0.84615\tval-ndcg@3:0.84221"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\ttrain-ndcg@3:0.85105\tval-ndcg@3:0.84242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttrain-ndcg@3:0.85546\tval-ndcg@3:0.84368"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\ttrain-ndcg@3:0.85942\tval-ndcg@3:0.84365"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain-ndcg@3:0.86289\tval-ndcg@3:0.84426"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550]\ttrain-ndcg@3:0.86569\tval-ndcg@3:0.84448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttrain-ndcg@3:0.86851\tval-ndcg@3:0.84505"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650]\ttrain-ndcg@3:0.87145\tval-ndcg@3:0.84537"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttrain-ndcg@3:0.87429\tval-ndcg@3:0.84528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750]\ttrain-ndcg@3:0.87703\tval-ndcg@3:0.84504"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[778]\ttrain-ndcg@3:0.87830\tval-ndcg@3:0.84488"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized XGBoost parameters for business traveler features (v2)\n",
    "final_xgb_params = {\n",
    "    'objective': 'rank:pairwise', \n",
    "    'eval_metric': 'ndcg@3', \n",
    "    'max_depth': 8,              # 从9减少到8，防止轻微过拟合\n",
    "    'min_child_weight': 10,      # 从12减少到10，更灵活\n",
    "    'subsample': 0.92,           # 从0.9增加到0.92，更好的泛化\n",
    "    'colsample_bytree': 0.9,     # 从0.88增加到0.9，更多特征利用\n",
    "    'lambda': 3.0,              # 从3.5减少到3.0，减少正则化\n",
    "    'alpha': 0.12,              # 从0.15减少到0.12，更灵活\n",
    "    'learning_rate': 0.055,     # 从0.052增加到0.055，稍快收敛\n",
    "    'gamma': 0.06,              # 从0.08减少到0.06，更多分裂\n",
    "    'seed': RANDOM_STATE, \n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'lossguide'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining XGBoost with v2 business-optimized parameters...\")\n",
    "print(\"V2 optimizations for better convergence:\")\n",
    "print(\"- Reduced depth (8) to prevent slight overfitting\")\n",
    "print(\"- Improved sampling rates for better generalization\")\n",
    "print(\"- Fine-tuned regularization for optimal expressiveness\")\n",
    "print(\"- Adjusted learning rate for faster convergence\")\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    final_xgb_params, dtrain,\n",
    "    num_boost_round=1600,        # Increased for complex features\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    early_stopping_rounds=110,   # Increased patience for complex learning\n",
    "    verbose_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744f002",
   "metadata": {
    "papermill": {
     "duration": 0.005529,
     "end_time": "2025-07-11T07:22:29.462093",
     "exception": false,
     "start_time": "2025-07-11T07:22:29.456564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e646a2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:22:29.476249Z",
     "iopub.status.busy": "2025-07-11T07:22:29.475955Z",
     "iopub.status.idle": "2025-07-11T07:22:29.546364Z",
     "shell.execute_reply": "2025-07-11T07:22:29.543009Z"
    },
    "papermill": {
     "duration": 0.083167,
     "end_time": "2025-07-11T07:22:29.550413",
     "exception": false,
     "start_time": "2025-07-11T07:22:29.467246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LightGBM Datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL\n",
    "# LightGBM requires its own Dataset object. We can reuse the rank-encoded data from XGBoost.\n",
    "print(\"Creating LightGBM Datasets...\")\n",
    "lgb_train = lgb.Dataset(\n",
    "    data=data_xgb_tr, \n",
    "    label=y_tr.to_numpy().flatten(), \n",
    "    group=group_sizes_tr,\n",
    "    feature_name=feature_cols,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "lgb_val = lgb.Dataset(\n",
    "    data=data_xgb_va, \n",
    "    label=y_va.to_numpy().flatten(), \n",
    "    group=group_sizes_va,\n",
    "    feature_name=feature_cols,\n",
    "    reference=lgb_train,\n",
    "    free_raw_data=False\n",
    ")\n",
    "print(\"LightGBM Datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7085d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:22:29.565057Z",
     "iopub.status.busy": "2025-07-11T07:22:29.564810Z",
     "iopub.status.idle": "2025-07-11T07:27:00.781436Z",
     "shell.execute_reply": "2025-07-11T07:27:00.773994Z"
    },
    "papermill": {
     "duration": 271.228134,
     "end_time": "2025-07-11T07:27:00.784143",
     "exception": false,
     "start_time": "2025-07-11T07:22:29.556009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final LightGBM model with optimized parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.978894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 10050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 16487352, number of used features: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's ndcg@3: 0.842894\tvalid_1's ndcg@3: 0.839999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's ndcg@3: 0.864782\tvalid_1's ndcg@3: 0.842916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttraining's ndcg@3: 0.879591\tvalid_1's ndcg@3: 0.842223"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's ndcg@3: 0.892081\tvalid_1's ndcg@3: 0.842145"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's ndcg@3: 0.864782\tvalid_1's ndcg@3: 0.842916"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized LightGBM parameters for business traveler features (v2)\n",
    "final_lgb_params = {\n",
    "    'objective': 'lambdarank', \n",
    "    'metric': 'ndcg', \n",
    "    'boosting_type': 'gbdt',\n",
    "    'eval_at': [3],\n",
    "    'num_leaves': 140,           # 从160减少到140，防止过拟合\n",
    "    'learning_rate': 0.15,       # 从0.18减少到0.15，更稳定\n",
    "    'min_child_samples': 60,     # 从55增加到60，更保守\n",
    "    'lambda_l1': 0.006,          # 从0.003增加到0.006，更多L1正则化\n",
    "    'lambda_l2': 7.5,            # 从6.5增加到7.5，更多L2正则化\n",
    "    'feature_fraction': 0.75,    # 从0.78减少到0.75，防止过拟合\n",
    "    'bagging_fraction': 0.84,    # 从0.87减少到0.84，更保守采样\n",
    "    'bagging_freq': 5,           \n",
    "    'min_gain_to_split': 0.005,  # 从0.003增加到0.005，更严格分裂\n",
    "    'max_depth': 10,             # 从12减少到10，控制复杂度\n",
    "    'force_row_wise': True,      \n",
    "    'n_jobs': -1, \n",
    "    'random_state': RANDOM_STATE, \n",
    "    'label_gain': [0, 1],\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model with v2 business-optimized parameters...\")\n",
    "print(\"V2 business feature optimizations:\")\n",
    "print(\"- Reduced leaves (140) and depth (10) to prevent overfitting\")\n",
    "print(\"- Lower learning rate (0.15) for better stability\")\n",
    "print(\"- Increased regularization (L1=0.006, L2=7.5) for generalization\")\n",
    "print(\"- Conservative sampling for better model robustness\")\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    final_lgb_params,\n",
    "    lgb_train,\n",
    "    num_boost_round=2000,        # Increased for complex features\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    callbacks=[lgb.early_stopping(130), lgb.log_evaluation(50)]  # Increased patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bcb999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:27:00.799719Z",
     "iopub.status.busy": "2025-07-11T07:27:00.799427Z",
     "iopub.status.idle": "2025-07-11T08:48:17.131151Z",
     "shell.execute_reply": "2025-07-11T08:48:17.125140Z"
    },
    "papermill": {
     "duration": 4876.343702,
     "end_time": "2025-07-11T08:48:17.133942",
     "exception": false,
     "start_time": "2025-07-11T07:27:00.790240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM DART Model ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RANDOM_STATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training LightGBM DART Model ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Keep original DART parameters but with slight optimization (v2)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dart_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambdarank\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_at\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2500\u001b[39m,      \u001b[38;5;66;03m# 保持训练轮数\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.045\u001b[39m,    \u001b[38;5;66;03m# 从0.042增加到0.045，更好学习能力\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m60\u001b[39m,          \u001b[38;5;66;03m# 从55增加到60，更多模型容量\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.075\u001b[39m,        \u001b[38;5;66;03m# 从0.08减少到0.075，更稳定dropout\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.85\u001b[39m,         \u001b[38;5;66;03m# 从0.82增加到0.85，更好采样\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_drop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.48\u001b[39m,         \u001b[38;5;66;03m# 从0.45增加到0.48，更平衡的保留\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m12\u001b[39m,           \u001b[38;5;66;03m# 添加深度限制，控制复杂度\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m45\u001b[39m,   \u001b[38;5;66;03m# 添加最小样本数，防止过拟合\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m,        \u001b[38;5;66;03m# 添加L1正则化\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2.5\u001b[39m,          \u001b[38;5;66;03m# 添加L2正则化\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mRANDOM_STATE\u001b[49m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_gain\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Enhanced LightGBM DART model (v2)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV2 DART optimizations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RANDOM_STATE' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training LightGBM DART Model ---\")\n",
    "\n",
    "# Keep original DART parameters but with slight optimization (v2)\n",
    "dart_params = {\n",
    "    'objective': 'lambdarank', \n",
    "    'metric': 'ndcg', \n",
    "    'eval_at': [3],\n",
    "    'boosting_type': 'dart', \n",
    "    'n_estimators': 2500,      # 保持训练轮数\n",
    "    'learning_rate': 0.045,    # 从0.042增加到0.045，更好学习能力\n",
    "    'num_leaves': 60,          # 从55增加到60，更多模型容量\n",
    "    'drop_rate': 0.075,        # 从0.08减少到0.075，更稳定dropout\n",
    "    'subsample': 0.85,         # 从0.82增加到0.85，更好采样\n",
    "    'skip_drop': 0.48,         # 从0.45增加到0.48，更平衡的保留\n",
    "    'max_depth': 12,           # 添加深度限制，控制复杂度\n",
    "    'min_child_samples': 45,   # 添加最小样本数，防止过拟合\n",
    "    'lambda_l1': 0.001,        # 添加L1正则化\n",
    "    'lambda_l2': 2.5,          # 添加L2正则化\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'label_gain': [0, 1]\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Enhanced LightGBM DART model (v2)...\")\n",
    "print(\"V2 DART optimizations:\")\n",
    "print(\"- Refined learning rate (0.045) for better convergence\")\n",
    "print(\"- Increased capacity (60 leaves) with depth control (12)\")\n",
    "print(\"- Optimized DART dropout balance (0.075 rate, 0.48 skip)\")\n",
    "print(\"- Added regularization for better generalization\")\n",
    "\n",
    "lgb_model_dart = lgb.train(\n",
    "    dart_params,\n",
    "    lgb_train, \n",
    "    num_boost_round=dart_params['n_estimators'], \n",
    "    valid_sets=[lgb_val],\n",
    "    callbacks=[lgb.early_stopping(110), lgb.log_evaluation(50)]  # Increased patience\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training CatBoost Model (After Validation) ---\")\n",
    "print(\"🔧 Now training CatBoost with full dataset in original sequence...\")\n",
    "\n",
    "# Prepare data for CatBoost (needs specific format)\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "# 🔧 FIXED: Create CatBoost data with proper type handling\n",
    "# CatBoost has specific requirements for categorical features - they must be integers\n",
    "# But when converting polars DataFrame to numpy, the entire array becomes float64\n",
    "# Solution: Use the ORIGINAL data (X) instead of rank-encoded data for CatBoost\n",
    "\n",
    "print(\"🔧 Preparing CatBoost data with original categorical features...\")\n",
    "\n",
    "# Use original data X (not rank-encoded data_xgb) for CatBoost\n",
    "# This preserves the original categorical values as intended\n",
    "X_catboost_tr = X[:n1]  # Same split as training data\n",
    "X_catboost_va = X[n1:n2]  # Same split as validation data  \n",
    "X_catboost_te = X[n2:]  # Same split as test data\n",
    "\n",
    "# Ensure all categorical features are properly encoded as integers\n",
    "print(f\"📊 Converting {len(cat_features_final)} categorical features to integers...\")\n",
    "for col in cat_features_final:\n",
    "    # Convert categorical columns to proper integer encoding for CatBoost\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(\n",
    "        pl.col(col).cast(pl.String).rank(\"dense\").cast(pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_va = X_catboost_va.with_columns(\n",
    "        pl.col(col).cast(pl.String).rank(\"dense\").cast(pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_te = X_catboost_te.with_columns(\n",
    "        pl.col(col).cast(pl.String).rank(\"dense\").cast(pl.Int32).alias(col)\n",
    "    )\n",
    "\n",
    "# Convert non-categorical features to float32 for efficiency\n",
    "non_cat_features = [col for col in feature_cols if col not in cat_features_final]\n",
    "print(f\"📊 Converting {len(non_cat_features)} numerical features to float32...\")\n",
    "for col in non_cat_features:\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_va = X_catboost_va.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_te = X_catboost_te.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "\n",
    "print(\"✅ CatBoost data preparation completed with proper data types\")\n",
    "\n",
    "# Create CatBoost pools with properly typed data\n",
    "print(\"🏗️ Creating CatBoost Pool objects...\")\n",
    "train_pool = Pool(\n",
    "    data=X_catboost_tr.to_numpy(),\n",
    "    label=y_tr.to_numpy().flatten(),\n",
    "    group_id=groups_tr.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_catboost_va.to_numpy(),\n",
    "    label=y_va.to_numpy().flatten(),\n",
    "    group_id=groups_va.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "print(\"✅ CatBoost Pool objects created successfully!\")\n",
    "print(f\"   Training pool: {train_pool.num_row()} rows x {train_pool.num_col()} cols\")\n",
    "print(f\"   Validation pool: {val_pool.num_row()} rows x {val_pool.num_col()} cols\")\n",
    "print(f\"   Categorical features: {len(catboost_cat_indices)} indices\")\n",
    "\n",
    "# CatBoost parameters optimized for business features\n",
    "catboost_params = {\n",
    "    'loss_function': 'YetiRank',\n",
    "    'custom_metric': ['NDCG:top=3'],\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.12,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 4.0,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'bagging_temperature': 0.8,\n",
    "    'subsample': 0.85,\n",
    "    'random_strength': 0.8,\n",
    "    'one_hot_max_size': 10,\n",
    "    'max_ctr_complexity': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'thread_count': -1,\n",
    "    'verbose': 50\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRanker(**catboost_params)\n",
    "catboost_model.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d042ef",
   "metadata": {
    "papermill": {
     "duration": 0.010298,
     "end_time": "2025-07-11T08:48:17.155336",
     "exception": false,
     "start_time": "2025-07-11T08:48:17.145038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Blending and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6ab7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:48:17.179705Z",
     "iopub.status.busy": "2025-07-11T08:48:17.179443Z",
     "iopub.status.idle": "2025-07-11T08:48:42.113109Z",
     "shell.execute_reply": "2025-07-11T08:48:42.106978Z"
    },
    "papermill": {
     "duration": 24.949952,
     "end_time": "2025-07-11T08:48:42.115554",
     "exception": false,
     "start_time": "2025-07-11T08:48:17.165602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all models on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "XGBoost HitRate@3:     0.5045\n",
      "LGBM GBDT HitRate@3:   0.4844\n",
      "LGBM DART HitRate@3:   0.4932\n",
      "------------------------------\n",
      "3-Model Blend HitRate@3: 0.5039\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training CatBoost Model (FIXED) ---\")\n",
    "print(\"Training CatBoost with CORRECTED data type handling...\")\n",
    "\n",
    "# Prepare data for CatBoost (needs specific format)\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "# CRITICAL FIX: Use unified categorical encoding approach\n",
    "# The problem: Polars DataFrame to numpy conversion always results in float64 arrays\n",
    "# Solution: Create separate data processing pipeline for CatBoost with explicit type control\n",
    "\n",
    "print(\"Preparing CatBoost data with FIXED categorical encoding...\")\n",
    "\n",
    "# Step 1: Get data splits using the same indices as other models\n",
    "X_catboost_tr = X[:n1].clone()  # Training split\n",
    "X_catboost_va = X[n1:n2].clone()  # Validation split \n",
    "X_catboost_te = X[n2:].clone()  # Test split\n",
    "\n",
    "print(f\"CatBoost data splits: Train={len(X_catboost_tr)}, Val={len(X_catboost_va)}, Test={len(X_catboost_te)}\")\n",
    "\n",
    "# Step 2: CRITICAL FIX - Apply proper categorical encoding\n",
    "# Convert categorical features to integers using a unified approach\n",
    "print(f\"Converting {len(cat_features_final)} categorical features to integers...\")\n",
    "\n",
    "# Create a unified string-to-integer mapping for each categorical feature\n",
    "for col in cat_features_final:\n",
    "    print(f\"   Processing categorical feature: {col}\")\n",
    "    \n",
    "    # Combine all data to create consistent encoding across splits\n",
    "    all_values = pl.concat([\n",
    "        X_catboost_tr.select(col),\n",
    "        X_catboost_va.select(col), \n",
    "        X_catboost_te.select(col)\n",
    "    ]).unique().sort(col)\n",
    "    \n",
    "    # Create mapping: unique values -> integers (0, 1, 2, ...)\n",
    "    mapping_dict = {\n",
    "        val: idx for idx, val in enumerate(all_values[col].to_list())\n",
    "    }\n",
    "    \n",
    "    # Apply mapping to all splits consistently\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_va = X_catboost_va.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_te = X_catboost_te.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "\n",
    "# Step 3: Convert numerical features to float32 for memory efficiency\n",
    "non_cat_features = [col for col in feature_cols if col not in cat_features_final]\n",
    "print(f\"Converting {len(non_cat_features)} numerical features to float32...\")\n",
    "\n",
    "for col in non_cat_features:\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_va = X_catboost_va.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_te = X_catboost_te.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "\n",
    "print(\"CatBoost data preparation completed with FIXED data types\")\n",
    "\n",
    "# Step 4: Verify data types before Pool creation\n",
    "print(\"Verifying data types before Pool creation...\")\n",
    "sample_data = X_catboost_tr.head(100).to_numpy()\n",
    "print(f\"   Sample data shape: {sample_data.shape}\")\n",
    "print(f\"   Sample data dtype: {sample_data.dtype}\")\n",
    "\n",
    "# Check categorical columns specifically\n",
    "for i, cat_idx in enumerate(catboost_cat_indices[:3]):\n",
    "    col_data = sample_data[:, cat_idx]\n",
    "    print(f\"   Cat feature {i} (col {cat_idx}): dtype={col_data.dtype}, sample={col_data[:3]}\")\n",
    "\n",
    "# Step 5: Create CatBoost pools with properly typed data\n",
    "print(\"Creating CatBoost Pool objects...\")\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_catboost_tr.to_numpy(),\n",
    "    label=y_tr.to_numpy().flatten(),\n",
    "    group_id=groups_tr.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_catboost_va.to_numpy(),\n",
    "    label=y_va.to_numpy().flatten(),\n",
    "    group_id=groups_va.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "print(\"CatBoost Pool objects created successfully!\")\n",
    "print(f\"   Training pool: {train_pool.num_row()} rows x {train_pool.num_col()} cols\")\n",
    "print(f\"   Validation pool: {val_pool.num_row()} rows x {val_pool.num_col()} cols\")\n",
    "print(f\"   Categorical features: {len(catboost_cat_indices)} indices\")\n",
    "\n",
    "# Step 6: Train CatBoost model with optimized parameters\n",
    "print(\"Training CatBoost model...\")\n",
    "\n",
    "catboost_params = {\n",
    "    'loss_function': 'YetiRank',\n",
    "    'custom_metric': ['NDCG:top=3'],\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.12,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 4.0,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'bagging_temperature': 0.8,\n",
    "    'subsample': 0.85,\n",
    "    'random_strength': 0.8,\n",
    "    'one_hot_max_size': 10,\n",
    "    'max_ctr_complexity': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'thread_count': -1,\n",
    "    'verbose': 50\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRanker(**catboost_params)\n",
    "catboost_model.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"CatBoost model trained successfully with FIXED data types!\")\n",
    "print(\"All models training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd6add",
   "metadata": {
    "papermill": {
     "duration": 0.010454,
     "end_time": "2025-07-11T08:48:42.137008",
     "exception": false,
     "start_time": "2025-07-11T08:48:42.126554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16480ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and Ensemble Optimization (Required Variables)\n",
    "print(\"Setting up ensemble strategies...\")\n",
    "\n",
    "# Get validation predictions for ensemble optimization\n",
    "xgb_val_preds = xgb_model.predict(dval)\n",
    "lgb_gbdt_val_preds = lgb_model.predict(data_xgb_va)\n",
    "lgb_dart_val_preds = lgb_model_dart.predict(data_xgb_va)\n",
    "\n",
    "# CatBoost validation predictions\n",
    "catboost_val_preds = catboost_model.predict(val_pool)\n",
    "\n",
    "# Calculate individual model performance on validation set\n",
    "val_hitrates = {}\n",
    "val_hitrates['XGBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), xgb_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['LightGBM_GBDT'] = hitrate_at_3(y_va.to_numpy().flatten(), lgb_gbdt_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['LightGBM_DART'] = hitrate_at_3(y_va.to_numpy().flatten(), lgb_dart_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['CatBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), catboost_val_preds, groups_va.to_numpy().flatten())\n",
    "\n",
    "print(\"Individual model validation performance:\")\n",
    "for model, hr in val_hitrates.items():\n",
    "    print(f\"  {model}: {hr:.4f}\")\n",
    "\n",
    "# Define ensemble strategies\n",
    "strategies = {\n",
    "    \"Static Balanced\": np.array([0.35, 0.25, 0.10, 0.30]),  # XGB, LGB-GBDT, LGB-DART, CatBoost\n",
    "    \"DART Focused\": np.array([0.25, 0.25, 0.35, 0.15]),\n",
    "    \"Performance Weighted\": np.array([0.4, 0.3, 0.1, 0.2]),\n",
    "    \"Static XGBoost Focused\": np.array([0.45, 0.15, 0.05, 0.35])\n",
    "}\n",
    "\n",
    "# Test strategies on validation data\n",
    "val_submission_df = data_xgb_va.select(['ranker_id']).with_columns([\n",
    "    pl.Series('xgb_score', xgb_val_preds),\n",
    "    pl.Series('lgb_gbdt_score', lgb_gbdt_val_preds),\n",
    "    pl.Series('lgb_dart_score', lgb_dart_val_preds),\n",
    "    pl.Series('catboost_score', catboost_val_preds)\n",
    "])\n",
    "\n",
    "# Convert scores to ranks\n",
    "val_submission_df = val_submission_df.with_columns([\n",
    "    pl.col(\"xgb_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"xgb_rank\"),\n",
    "    pl.col(\"lgb_gbdt_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_gbdt_rank\"),\n",
    "    pl.col(\"lgb_dart_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_dart_rank\"),\n",
    "    pl.col(\"catboost_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"catboost_rank\")\n",
    "])\n",
    "\n",
    "# Test ensemble strategies\n",
    "best_strategy_hr3 = 0\n",
    "best_strategy_name = \"Static Balanced\"\n",
    "strategy_results = {}\n",
    "\n",
    "for strategy_name, weights in strategies.items():\n",
    "    # Create ensemble\n",
    "    ensemble_score = (weights[0] * val_submission_df.get_column(\"xgb_rank\") + \n",
    "                     weights[1] * val_submission_df.get_column(\"lgb_gbdt_rank\") + \n",
    "                     weights[2] * val_submission_df.get_column(\"lgb_dart_rank\") + \n",
    "                     weights[3] * val_submission_df.get_column(\"catboost_rank\"))\n",
    "    \n",
    "    # Get best option per group (lowest ensemble rank)\n",
    "    val_ensemble_df = val_submission_df.with_columns([\n",
    "        pl.Series(\"ensemble_rank\", ensemble_score)\n",
    "    ])\n",
    "    \n",
    "    # Rank ensemble scores within each group\n",
    "    val_ensemble_df = val_ensemble_df.with_columns([\n",
    "        pl.col(\"ensemble_rank\").rank(method=\"ordinal\", descending=False).over(\"ranker_id\").alias(\"final_rank\")\n",
    "    ])\n",
    "    \n",
    "    # Select top option per group\n",
    "    selected = val_ensemble_df.filter(pl.col(\"final_rank\") == 1)\n",
    "    \n",
    "    # Create predictions (1 for selected, 0 for others)\n",
    "    val_preds = np.zeros(len(val_ensemble_df))\n",
    "    selected_indices = selected.select(pl.int_range(pl.len()).over(\"ranker_id\")).to_numpy().flatten()\n",
    "    # val_preds[selected_indices] = 1\n",
    "    \n",
    "    # Calculate hit rate using ensemble ranking\n",
    "    val_ensemble_preds = val_ensemble_df.get_column(\"ensemble_rank\").to_numpy()\n",
    "    strategy_hr3 = hitrate_at_3(y_va.to_numpy().flatten(), -val_ensemble_preds, groups_va.to_numpy().flatten())\n",
    "    \n",
    "    strategy_results[strategy_name] = strategy_hr3\n",
    "    print(f\"Strategy '{strategy_name}': {strategy_hr3:.4f}\")\n",
    "    \n",
    "    if strategy_hr3 > best_strategy_hr3:\n",
    "        best_strategy_hr3 = strategy_hr3\n",
    "        best_strategy_name = strategy_name\n",
    "\n",
    "# Set the best strategy weights\n",
    "if best_strategy_name in strategies:\n",
    "    optimized_weights = strategies[best_strategy_name]\n",
    "    dart_focused_weights = strategies.get(\"DART Focused\", strategies[\"Static Balanced\"])\n",
    "    performance_weights = strategies.get(\"Performance Weighted\", strategies[\"Static Balanced\"])\n",
    "    balanced_weights = strategies[\"Static Balanced\"]\n",
    "else:\n",
    "    optimized_weights = strategies[\"Static Balanced\"]\n",
    "    dart_focused_weights = strategies[\"Static Balanced\"]\n",
    "    performance_weights = strategies[\"Static Balanced\"]\n",
    "    balanced_weights = strategies[\"Static Balanced\"]\n",
    "\n",
    "print(f\"\\nBest strategy: {best_strategy_name} (HR@3: {best_strategy_hr3:.4f})\")\n",
    "print(\"Ensemble optimization completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fc692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:48:42.161517Z",
     "iopub.status.busy": "2025-07-11T08:48:42.161280Z",
     "iopub.status.idle": "2025-07-11T08:50:16.272461Z",
     "shell.execute_reply": "2025-07-11T08:50:16.265939Z"
    },
    "papermill": {
     "duration": 94.130711,
     "end_time": "2025-07-11T08:50:16.278051",
     "exception": false,
     "start_time": "2025-07-11T08:48:42.147340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for the test set with all three models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌──────────┬─────────────────────────────────┬──────────┐\n",
      "│ Id       ┆ ranker_id                       ┆ selected │\n",
      "│ ---      ┆ ---                             ┆ ---      │\n",
      "│ i64      ┆ str                             ┆ i32      │\n",
      "╞══════════╪═════════════════════════════════╪══════════╡\n",
      "│ 18144679 ┆ c9373e5f772e43d593dd6ad2fa90f6… ┆ 18       │\n",
      "│ 18144680 ┆ c9373e5f772e43d593dd6ad2fa90f6… ┆ 76       │\n",
      "│ 18144681 ┆ c9373e5f772e43d593dd6ad2fa90f6… ┆ 267      │\n",
      "│ 18144682 ┆ c9373e5f772e43d593dd6ad2fa90f6… ┆ 102      │\n",
      "│ 18144683 ┆ c9373e5f772e43d593dd6ad2fa90f6… ┆ 82       │\n",
      "└──────────┴─────────────────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "# XGBoost test predictions\n",
    "xgb_test_preds = xgb_model.predict(dtest)\n",
    "\n",
    "# LightGBM test predictions  \n",
    "lgb_gbdt_test_preds = lgb_model.predict(data_xgb_te)\n",
    "lgb_dart_test_preds = lgb_model_dart.predict(data_xgb_te)\n",
    "\n",
    "# CatBoost test predictions with SAME encoding as training\n",
    "print(\"Preparing CatBoost test predictions with FIXED encoding...\")\n",
    "\n",
    "# CatBoost test data is already properly encoded in the training cell\n",
    "# X_catboost_te was processed with the same categorical encoding pipeline\n",
    "# So we can directly create the test pool and predict\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_catboost_te.to_numpy(),\n",
    "    group_id=groups_te.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "print(\"CatBoost test pool created successfully\")\n",
    "catboost_test_preds = catboost_model.predict(test_pool)\n",
    "print(\"CatBoost test predictions generated successfully\")\n",
    "\n",
    "# Create comprehensive submission dataframe with all models and strategies\n",
    "submission_df = test.select(['Id', 'ranker_id']).with_columns([\n",
    "    pl.Series('xgb_score', xgb_test_preds),\n",
    "    pl.Series('lgb_gbdt_score', lgb_gbdt_test_preds),\n",
    "    pl.Series('lgb_dart_score', lgb_dart_test_preds),\n",
    "    pl.Series('catboost_score', catboost_test_preds)\n",
    "])\n",
    "\n",
    "# Convert scores to ranks within each group\n",
    "submission_df = submission_df.with_columns([\n",
    "    pl.col(\"xgb_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"xgb_rank\"),\n",
    "    pl.col(\"lgb_gbdt_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_gbdt_rank\"),\n",
    "    pl.col(\"lgb_dart_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_dart_rank\"),\n",
    "    pl.col(\"catboost_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"catboost_rank\")\n",
    "])\n",
    "\n",
    "print(\"All test predictions generated successfully!\")\n",
    "print(f\"Test predictions shape: {submission_df.shape}\")\n",
    "print(f\"XGBoost predictions range: {xgb_test_preds.min():.4f} - {xgb_test_preds.max():.4f}\")\n",
    "print(f\"LightGBM GBDT predictions range: {lgb_gbdt_test_preds.min():.4f} - {lgb_gbdt_test_preds.max():.4f}\")\n",
    "print(f\"LightGBM DART predictions range: {lgb_dart_test_preds.min():.4f} - {lgb_dart_test_preds.max():.4f}\")\n",
    "print(f\"CatBoost predictions range: {catboost_test_preds.min():.4f} - {catboost_test_preds.max():.4f}\")\n",
    "\n",
    "# STEP 1: STRATEGY SELECTION AND ENSEMBLE OPTIMIZATION\n",
    "print(\"\\nStep 1: Applying Best Strategy to Test Data...\")\n",
    "\n",
    "# Apply the best strategy from validation to test data\n",
    "if best_strategy_name == \"DART Focused\":\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (dart_focused_weights[0] * pl.col(\"xgb_rank\") + dart_focused_weights[1] * pl.col(\"lgb_gbdt_rank\") + \n",
    "         dart_focused_weights[2] * pl.col(\"lgb_dart_rank\") + dart_focused_weights[3] * pl.col(\"catboost_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = dart_focused_weights\n",
    "    \n",
    "elif best_strategy_name == \"Dynamic Optimized\":\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (optimized_weights[0] * pl.col(\"xgb_rank\") + optimized_weights[1] * pl.col(\"lgb_gbdt_rank\") + \n",
    "         optimized_weights[2] * pl.col(\"lgb_dart_rank\") + optimized_weights[3] * pl.col(\"catboost_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = optimized_weights\n",
    "    \n",
    "elif best_strategy_name == \"Performance Weighted\":\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (performance_weights[0] * pl.col(\"xgb_rank\") + performance_weights[1] * pl.col(\"lgb_gbdt_rank\") + \n",
    "         performance_weights[2] * pl.col(\"lgb_dart_rank\") + performance_weights[3] * pl.col(\"catboost_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = performance_weights\n",
    "    \n",
    "elif best_strategy_name == \"Adaptive Balanced\":\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (balanced_weights[0] * pl.col(\"xgb_rank\") + balanced_weights[1] * pl.col(\"lgb_gbdt_rank\") + \n",
    "         balanced_weights[2] * pl.col(\"lgb_dart_rank\") + balanced_weights[3] * pl.col(\"catboost_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = balanced_weights\n",
    "    \n",
    "elif best_strategy_name == \"Static XGBoost Focused\":\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (0.45 * pl.col(\"xgb_rank\") + 0.35 * pl.col(\"catboost_rank\") + \n",
    "         0.15 * pl.col(\"lgb_gbdt_rank\") + 0.05 * pl.col(\"lgb_dart_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = np.array([0.45, 0.15, 0.05, 0.35])  # XGB, LGB-GBDT, LGB-DART, CatBoost\n",
    "    \n",
    "else:  # Static Balanced as fallback\n",
    "    submission_df = submission_df.with_columns([\n",
    "        (0.35 * pl.col(\"xgb_rank\") + 0.3 * pl.col(\"catboost_rank\") + \n",
    "         0.25 * pl.col(\"lgb_gbdt_rank\") + 0.1 * pl.col(\"lgb_dart_rank\")).alias(\"best_ensemble\")\n",
    "    ])\n",
    "    strategy_weights = np.array([0.35, 0.25, 0.10, 0.30])  # XGB, LGB-GBDT, LGB-DART, CatBoost\n",
    "\n",
    "print(f\"Applied Strategy: {best_strategy_name}\")\n",
    "print(f\"Strategy Weights: XGB={strategy_weights[0]:.3f}, LGB-GBDT={strategy_weights[1]:.3f}, LGB-DART={strategy_weights[2]:.3f}, CatBoost={strategy_weights[3]:.3f}\")\n",
    "\n",
    "# STEP 2: FINAL SUBMISSION RANKING\n",
    "print(\"\\nStep 2: Creating Final Submission...\")\n",
    "\n",
    "# Convert ensemble scores to final ranks\n",
    "final_submission = submission_df.select(['Id']).with_columns([\n",
    "    pl.col('Id').alias('Id'),\n",
    "    pl.lit(1).alias('selected')  # All flights ranked as 1 initially\n",
    "])\n",
    "\n",
    "# Rank flights within each ranker group based on best ensemble score\n",
    "submission_with_ranks = submission_df.with_columns([\n",
    "    pl.col(\"best_ensemble\").rank(method=\"ordinal\", descending=False).over(\"ranker_id\").alias(\"final_rank\")\n",
    "])\n",
    "\n",
    "# Keep only the top-ranked flight for each ranker (rank 1)\n",
    "final_submission = submission_with_ranks.filter(pl.col(\"final_rank\") == 1).select(['Id']).with_columns([\n",
    "    pl.lit(1).alias('selected')\n",
    "])\n",
    "\n",
    "print(f\"Final submission created!\")\n",
    "print(f\"Selected flights: {len(final_submission)}\")\n",
    "print(f\"Expected rankers: ~{len(test.select('ranker_id').unique())}\")\n",
    "\n",
    "# Save submission file\n",
    "final_submission.write_csv('submission.csv')\n",
    "print(\"Submission saved to 'submission.csv'\")\n",
    "\n",
    "print(\"\\nENSEMBLE OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Strategy: {best_strategy_name}\")\n",
    "print(f\"Validation HitRate@3: {best_strategy_hr3:.4f}\")\n",
    "print(f\"Strategy Weights: XGB={strategy_weights[0]:.3f}, LGB-GBDT={strategy_weights[1]:.3f}, LGB-DART={strategy_weights[2]:.3f}, CatBoost={strategy_weights[3]:.3f}\")\n",
    "print(\"Ready for Kaggle submission!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 12733338,
     "sourceId": 105399,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6049.316815,
   "end_time": "2025-07-11T08:50:26.483574",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T07:09:37.166759",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
