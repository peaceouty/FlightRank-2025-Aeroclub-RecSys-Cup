{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122ad018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:28:27.062242Z",
     "iopub.status.busy": "2025-07-13T03:28:27.061969Z",
     "iopub.status.idle": "2025-07-13T03:29:18.838966Z",
     "shell.execute_reply": "2025-07-13T03:29:18.834065Z"
    },
    "papermill": {
     "duration": 51.789949,
     "end_time": "2025-07-13T03:29:18.841783",
     "exception": false,
     "start_time": "2025-07-13T03:28:27.051834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U xgboost\n",
    "!pip install -U polars\n",
    "!pip install -U optuna\n",
    "!pip install -U catboost\n",
    "!pip install -U lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405f18bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:29:18.857400Z",
     "iopub.status.busy": "2025-07-13T03:29:18.857159Z",
     "iopub.status.idle": "2025-07-13T03:29:30.350316Z",
     "shell.execute_reply": "2025-07-13T03:29:30.345198Z"
    },
    "papermill": {
     "duration": 11.50508,
     "end_time": "2025-07-13T03:29:30.352726",
     "exception": false,
     "start_time": "2025-07-13T03:29:18.847646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b694e0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:29:30.367169Z",
     "iopub.status.busy": "2025-07-13T03:29:30.366802Z",
     "iopub.status.idle": "2025-07-13T03:29:34.565996Z",
     "shell.execute_reply": "2025-07-13T03:29:34.562200Z"
    },
    "papermill": {
     "duration": 4.211842,
     "end_time": "2025-07-13T03:29:34.570030",
     "exception": false,
     "start_time": "2025-07-13T03:29:30.358188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet').drop('__index_level_0__')\n",
    "test = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet').drop('__index_level_0__').with_columns(pl.lit(0, dtype=pl.Int64).alias(\"selected\"))\n",
    "\n",
    "data_raw = pl.concat((train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd654d23",
   "metadata": {
    "papermill": {
     "duration": 0.00475,
     "end_time": "2025-07-13T03:29:34.580288",
     "exception": false,
     "start_time": "2025-07-13T03:29:34.575538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894be123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:29:34.593192Z",
     "iopub.status.busy": "2025-07-13T03:29:34.592949Z",
     "iopub.status.idle": "2025-07-13T03:29:34.605927Z",
     "shell.execute_reply": "2025-07-13T03:29:34.599384Z"
    },
    "papermill": {
     "duration": 0.022693,
     "end_time": "2025-07-13T03:29:34.607994",
     "exception": false,
     "start_time": "2025-07-13T03:29:34.585301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hitrate_at_3(y_true, y_pred, groups):\n",
    "    df = pl.DataFrame({\n",
    "        'group': groups,\n",
    "        'pred': y_pred,\n",
    "        'true': y_true\n",
    "    })\n",
    "    \n",
    "    return (\n",
    "        df.filter(pl.col(\"group\").count().over(\"group\") > 10)\n",
    "        .sort([\"group\", \"pred\"], descending=[False, True])\n",
    "        .group_by(\"group\", maintain_order=True)\n",
    "        .head(3)\n",
    "        .group_by(\"group\")\n",
    "        .agg(pl.col(\"true\").max())\n",
    "        .select(pl.col(\"true\").mean())\n",
    "        .item()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48a3b0",
   "metadata": {
    "papermill": {
     "duration": 0.005092,
     "end_time": "2025-07-13T03:29:34.617906",
     "exception": false,
     "start_time": "2025-07-13T03:29:34.612814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb01232e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:29:34.631263Z",
     "iopub.status.busy": "2025-07-13T03:29:34.631032Z",
     "iopub.status.idle": "2025-07-13T03:30:38.499240Z",
     "shell.execute_reply": "2025-07-13T03:30:38.495346Z"
    },
    "papermill": {
     "duration": 63.880598,
     "end_time": "2025-07-13T03:30:38.503485",
     "exception": false,
     "start_time": "2025-07-13T03:29:34.622887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = data_raw.clone()\n",
    "\n",
    "# More efficient duration to minutes converter\n",
    "def dur_to_min(col):\n",
    "    # Extract days and time parts in one pass\n",
    "    days = col.str.extract(r\"^(\\d+)\\.\", 1).cast(pl.Int64).fill_null(0) * 1440\n",
    "    time_str = pl.when(col.str.contains(r\"^\\d+\\.\")).then(col.str.replace(r\"^\\d+\\.\", \"\")).otherwise(col)\n",
    "    hours = time_str.str.extract(r\"^(\\d+):\", 1).cast(pl.Int64).fill_null(0) * 60\n",
    "    minutes = time_str.str.extract(r\":(\\d+):\", 1).cast(pl.Int64).fill_null(0)\n",
    "    return (days + hours + minutes).fill_null(0)\n",
    "\n",
    "# Process duration columns\n",
    "dur_cols = [\"legs0_duration\", \"legs1_duration\"] + [f\"legs{l}_segments{s}_duration\" for l in (0, 1) for s in (0, 1)]\n",
    "dur_exprs = [dur_to_min(pl.col(c)).alias(c) for c in dur_cols if c in df.columns]\n",
    "\n",
    "# Apply duration transformations first\n",
    "if dur_exprs:\n",
    "    df = df.with_columns(dur_exprs)\n",
    "\n",
    "# Precompute marketing carrier columns check\n",
    "mc_cols = [f'legs{l}_segments{s}_marketingCarrier_code' for l in (0, 1) for s in range(4)]\n",
    "mc_exists = [col for col in mc_cols if col in df.columns]\n",
    "\n",
    "# Combine all initial transformations\n",
    "df = df.with_columns([\n",
    "        # Price features\n",
    "        (pl.col(\"totalPrice\") / (pl.col(\"taxes\") + 1)).alias(\"price_per_tax\"),\n",
    "        (pl.col(\"taxes\") / (pl.col(\"totalPrice\") + 1)).alias(\"tax_rate\"),\n",
    "        pl.col(\"totalPrice\").log1p().alias(\"log_price\"),\n",
    "        \n",
    "        # Duration features\n",
    "        (pl.col(\"legs0_duration\").fill_null(0) + pl.col(\"legs1_duration\").fill_null(0)).alias(\"total_duration\"),\n",
    "        pl.when(pl.col(\"legs1_duration\").fill_null(0) > 0)\n",
    "            .then(pl.col(\"legs0_duration\") / (pl.col(\"legs1_duration\") + 1))\n",
    "            .otherwise(1.0).alias(\"duration_ratio\"),\n",
    "        \n",
    "        # Trip type\n",
    "        (pl.col(\"legs1_duration\").is_null() | \n",
    "         (pl.col(\"legs1_duration\") == 0) | \n",
    "         pl.col(\"legs1_segments0_departureFrom_airport_iata\").is_null()).cast(pl.Int32).alias(\"is_one_way\"),\n",
    "        \n",
    "        # Total segments count\n",
    "        (pl.sum_horizontal(pl.col(col).is_not_null().cast(pl.UInt8) for col in mc_exists) \n",
    "         if mc_exists else pl.lit(0)).alias(\"l0_seg\"),\n",
    "        \n",
    "        # FF features\n",
    "        (pl.col(\"frequentFlyer\").fill_null(\"\").str.count_matches(\"/\") + \n",
    "         (pl.col(\"frequentFlyer\").fill_null(\"\") != \"\").cast(pl.Int32)).alias(\"n_ff_programs\"),\n",
    "        \n",
    "        # Binary features\n",
    "        pl.col(\"corporateTariffCode\").is_not_null().cast(pl.Int32).alias(\"has_corporate_tariff\"),\n",
    "        (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"has_access_tp\"),\n",
    "        \n",
    "        # Baggage & fees\n",
    "        (pl.col(\"legs0_segments0_baggageAllowance_quantity\").fill_null(0) + \n",
    "         pl.col(\"legs1_segments0_baggageAllowance_quantity\").fill_null(0)).alias(\"baggage_total\"),\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(0) + \n",
    "         pl.col(\"miniRules1_monetaryAmount\").fill_null(0)).alias(\"total_fees\"),\n",
    "        \n",
    "        # Routes & carriers\n",
    "        pl.col(\"searchRoute\").is_in([\"MOWLED/LEDMOW\", \"LEDMOW/MOWLED\", \"MOWLED\", \"LEDMOW\", \"MOWAER/AERMOW\"])\n",
    "            .cast(pl.Int32).alias(\"is_popular_route\"),\n",
    "        \n",
    "        # Cabin\n",
    "        pl.mean_horizontal([\"legs0_segments0_cabinClass\", \"legs1_segments0_cabinClass\"]).alias(\"avg_cabin_class\"),\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(0) - \n",
    "         pl.col(\"legs1_segments0_cabinClass\").fill_null(0)).alias(\"cabin_class_diff\"),\n",
    "])\n",
    "\n",
    "# Segment counts - more efficient\n",
    "seg_exprs = []\n",
    "for leg in (0, 1):\n",
    "    seg_cols = [f\"legs{leg}_segments{s}_duration\" for s in range(4) if f\"legs{leg}_segments{s}_duration\" in df.columns]\n",
    "    if seg_cols:\n",
    "        seg_exprs.append(\n",
    "            pl.sum_horizontal(pl.col(c).is_not_null() for c in seg_cols)\n",
    "                .cast(pl.Int32).alias(f\"n_segments_leg{leg}\")\n",
    "        )\n",
    "    else:\n",
    "        seg_exprs.append(pl.lit(0).cast(pl.Int32).alias(f\"n_segments_leg{leg}\"))\n",
    "\n",
    "# Add segment-based features\n",
    "# First create segment counts\n",
    "df = df.with_columns(seg_exprs)\n",
    "\n",
    "# Then use them for derived features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"n_segments_leg0\") + pl.col(\"n_segments_leg1\")).alias(\"total_segments\"),\n",
    "    (pl.col(\"n_segments_leg0\") == 1).cast(pl.Int32).alias(\"is_direct_leg0\"),\n",
    "    pl.when(pl.col(\"is_one_way\") == 1).then(0)\n",
    "        .otherwise((pl.col(\"n_segments_leg1\") == 1).cast(pl.Int32)).alias(\"is_direct_leg1\"),\n",
    "])\n",
    "\n",
    "# More derived features\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"is_direct_leg0\") & pl.col(\"is_direct_leg1\")).cast(pl.Int32).alias(\"both_direct\"),\n",
    "    ((pl.col(\"isVip\") == 1) | (pl.col(\"n_ff_programs\") > 0)).cast(pl.Int32).alias(\"is_vip_freq\"),\n",
    "    (pl.col(\"baggage_total\") > 0).cast(pl.Int32).alias(\"has_baggage\"),\n",
    "    (pl.col(\"total_fees\") > 0).cast(pl.Int32).alias(\"has_fees\"),\n",
    "    (pl.col(\"total_fees\") / (pl.col(\"totalPrice\") + 1)).alias(\"fee_rate\"),\n",
    "    pl.col(\"Id\").count().over(\"ranker_id\").alias(\"group_size\"),\n",
    "])\n",
    "\n",
    "# Add major carrier flag if column exists\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\"])\n",
    "            .cast(pl.Int32).alias(\"is_major_carrier\")\n",
    "    )\n",
    "else:\n",
    "    df = df.with_columns(pl.lit(0).alias(\"is_major_carrier\"))\n",
    "\n",
    "df = df.with_columns(pl.col(\"group_size\").log1p().alias(\"group_size_log\"))\n",
    "\n",
    "# Time features - batch process\n",
    "time_exprs = []\n",
    "for col in (\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"):\n",
    "    if col in df.columns:\n",
    "        dt = pl.col(col).str.to_datetime(strict=False)\n",
    "        h = dt.dt.hour().fill_null(12)\n",
    "        time_exprs.extend([\n",
    "            h.alias(f\"{col}_hour\"),\n",
    "            dt.dt.weekday().fill_null(0).alias(f\"{col}_weekday\"),\n",
    "            (((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))).cast(pl.Int32).alias(f\"{col}_business_time\")\n",
    "        ])\n",
    "if time_exprs:\n",
    "    df = df.with_columns(time_exprs)\n",
    "\n",
    "# Batch rank computations - more efficient with single pass\n",
    "# First apply the columns that will be used for ranking\n",
    "df = df.with_columns([\n",
    "    pl.col(\"group_size\").log1p().alias(\"group_size_log\"),\n",
    "])\n",
    "\n",
    "# Price and duration basic ranks\n",
    "rank_exprs = []\n",
    "for col, alias in [(\"totalPrice\", \"price\"), (\"total_duration\", \"duration\")]:\n",
    "    rank_exprs.append(pl.col(col).rank().over(\"ranker_id\").alias(f\"{alias}_rank\"))\n",
    "\n",
    "# Price-specific features\n",
    "price_exprs = [\n",
    "    (pl.col(\"totalPrice\").rank(\"average\").over(\"ranker_id\") / \n",
    "     pl.col(\"totalPrice\").count().over(\"ranker_id\")).alias(\"price_pct_rank\"),\n",
    "    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheapest\"),\n",
    "    ((pl.col(\"totalPrice\") - pl.col(\"totalPrice\").median().over(\"ranker_id\")) / \n",
    "     (pl.col(\"totalPrice\").std().over(\"ranker_id\") + 1)).alias(\"price_from_median\"),\n",
    "    (pl.col(\"l0_seg\") == pl.col(\"l0_seg\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_min_segments\"),\n",
    "]\n",
    "\n",
    "# Apply initial ranks\n",
    "df = df.with_columns(rank_exprs + price_exprs)\n",
    "\n",
    "# Cheapest direct - more efficient\n",
    "direct_cheapest = (\n",
    "    df.filter(pl.col(\"is_direct_leg0\") == 1)\n",
    "    .group_by(\"ranker_id\")\n",
    "    .agg(pl.col(\"totalPrice\").min().alias(\"min_direct\"))\n",
    ")\n",
    "\n",
    "df = df.join(direct_cheapest, on=\"ranker_id\", how=\"left\").with_columns(\n",
    "    ((pl.col(\"is_direct_leg0\") == 1) & \n",
    "     (pl.col(\"totalPrice\") == pl.col(\"min_direct\"))).cast(pl.Int32).fill_null(0).alias(\"is_direct_cheapest\")\n",
    ").drop(\"min_direct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712cf0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:30:38.519698Z",
     "iopub.status.busy": "2025-07-13T03:30:38.519268Z",
     "iopub.status.idle": "2025-07-13T03:30:43.465501Z",
     "shell.execute_reply": "2025-07-13T03:30:43.460537Z"
    },
    "papermill": {
     "duration": 4.959742,
     "end_time": "2025-07-13T03:30:43.468630",
     "exception": false,
     "start_time": "2025-07-13T03:30:38.508888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding advanced business traveler features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced business traveler features added successfully!\n",
      "Key improvements:\n",
      "- Corporate policy compliance and tax efficiency\n",
      "- Advanced time preferences for business travelers\n",
      "- Seat scarcity and availability features\n",
      "- Cancellation/exchange policy flexibility\n",
      "- Premium carrier and aircraft preferences\n",
      "- Hub airport and route optimization\n",
      "- Strategic business value interactions\n",
      "- Safe feature creation with proper dependency ordering\n",
      "- Targeting 0.5+ Kaggle score with business insights\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature Engineering - Focus on Business Traveler Preferences\n",
    "print(\"Adding advanced business traveler features...\")\n",
    "\n",
    "# 1. Enhanced price and policy features\n",
    "df = df.with_columns([\n",
    "    # Corporate policy compliance\n",
    "    (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"policy_compliant\"),\n",
    "    \n",
    "    # Enhanced price buckets with controlled group stats\n",
    "    pl.when(pl.col(\"price_pct_rank\") <= 0.15).then(1)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.3).then(2)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.5).then(3)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.7).then(4)\n",
    "    .when(pl.col(\"price_pct_rank\") <= 0.85).then(5)\n",
    "    .otherwise(6).alias(\"price_bucket\"),\n",
    "    \n",
    "    # Price competitiveness (keep essential group features)\n",
    "    (pl.col(\"totalPrice\") <= pl.col(\"totalPrice\").quantile(0.25).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheap_quartile\"),\n",
    "    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_absolute_cheapest\"),\n",
    "    \n",
    "    # Tax efficiency for business\n",
    "    pl.when(pl.col(\"taxes\") > 0).then(pl.col(\"totalPrice\") / pl.col(\"taxes\")).otherwise(0).alias(\"price_tax_efficiency\"),\n",
    "])\n",
    "\n",
    "# 2. Advanced time features for business travelers\n",
    "time_features = []\n",
    "for prefix in [\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"]:\n",
    "    if f\"{prefix}_hour\" in df.columns:\n",
    "        time_features.extend([\n",
    "            # Premium business hours\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 7) & (pl.col(f\"{prefix}_hour\") <= 9)).cast(pl.Int32).alias(f\"{prefix}_morning_business\"),\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 17) & (pl.col(f\"{prefix}_hour\") <= 19)).cast(pl.Int32).alias(f\"{prefix}_evening_business\"),\n",
    "            # Avoid red-eye flights\n",
    "            ((pl.col(f\"{prefix}_hour\") >= 23) | (pl.col(f\"{prefix}_hour\") <= 5)).cast(pl.Int32).alias(f\"{prefix}_red_eye\"),\n",
    "            # Premium time slots\n",
    "            (pl.col(f\"{prefix}_hour\").is_in([7, 8, 9, 18, 19, 20])).cast(pl.Int32).alias(f\"{prefix}_premium_time\"),\n",
    "            # Weekend patterns\n",
    "            ((pl.col(f\"{prefix}_weekday\") >= 5) & (pl.col(f\"{prefix}_hour\") >= 10)).cast(pl.Int32).alias(f\"{prefix}_weekend_leisure\"),\n",
    "        ])\n",
    "\n",
    "if time_features:\n",
    "    df = df.with_columns(time_features)\n",
    "\n",
    "# 3. Seat availability and scarcity features\n",
    "seat_features = []\n",
    "for leg in [0, 1]:\n",
    "    for seg in [0, 1]:\n",
    "        seat_col = f\"legs{leg}_segments{seg}_seatsAvailable\"\n",
    "        if seat_col in df.columns:\n",
    "            seat_features.extend([\n",
    "                # Seat scarcity (low availability premium)\n",
    "                (pl.col(seat_col).fill_null(0) <= 5).cast(pl.Int32).alias(f\"{seat_col}_scarce\"),\n",
    "                # High availability\n",
    "                (pl.col(seat_col).fill_null(0) >= 20).cast(pl.Int32).alias(f\"{seat_col}_abundant\"),\n",
    "                # Normalized seat availability within group\n",
    "                (pl.col(seat_col) / (pl.col(seat_col).mean().over(\"ranker_id\") + 1)).alias(f\"{seat_col}_relative\"),\n",
    "            ])\n",
    "\n",
    "if seat_features:\n",
    "    df = df.with_columns(seat_features)\n",
    "\n",
    "# 4. Cancellation and change policy features (critical for business)\n",
    "policy_features = []\n",
    "has_cancellation = False\n",
    "has_exchange = False\n",
    "\n",
    "# Check if cancellation features exist\n",
    "if \"miniRules0_monetaryAmount\" in df.columns:\n",
    "    has_cancellation = True\n",
    "    policy_features.extend([\n",
    "        # Flexible cancellation\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(999999) == 0).cast(pl.Int32).alias(\"free_cancellation\"),\n",
    "        # Low cancellation fee\n",
    "        (pl.col(\"miniRules0_monetaryAmount\").fill_null(999999) <= pl.col(\"totalPrice\") * 0.1).cast(pl.Int32).alias(\"low_cancel_fee\"),\n",
    "        # Cancellation fee rate\n",
    "        (pl.col(\"miniRules0_monetaryAmount\") / (pl.col(\"totalPrice\") + 1)).alias(\"cancel_fee_rate\"),\n",
    "    ])\n",
    "\n",
    "if \"miniRules1_monetaryAmount\" in df.columns:\n",
    "    has_exchange = True\n",
    "    policy_features.extend([\n",
    "        # Flexible exchange\n",
    "        (pl.col(\"miniRules1_monetaryAmount\").fill_null(999999) == 0).cast(pl.Int32).alias(\"free_exchange\"),\n",
    "        # Low exchange fee\n",
    "        (pl.col(\"miniRules1_monetaryAmount\").fill_null(999999) <= pl.col(\"totalPrice\") * 0.1).cast(pl.Int32).alias(\"low_exchange_fee\"),\n",
    "        # Exchange fee rate\n",
    "        (pl.col(\"miniRules1_monetaryAmount\") / (pl.col(\"totalPrice\") + 1)).alias(\"exchange_fee_rate\"),\n",
    "    ])\n",
    "\n",
    "if policy_features:\n",
    "    df = df.with_columns(policy_features)\n",
    "\n",
    "# 5. Route and carrier sophistication\n",
    "route_features = []\n",
    "has_airports = False\n",
    "has_carriers = False\n",
    "\n",
    "if \"legs0_segments0_departureFrom_airport_iata\" in df.columns:\n",
    "    has_airports = True\n",
    "    route_features.extend([\n",
    "        # Major hub airports (Moscow, St. Petersburg)\n",
    "        pl.col(\"legs0_segments0_departureFrom_airport_iata\").is_in([\"SVO\", \"DME\", \"VKO\", \"LED\", \"PKC\"])\n",
    "        .cast(pl.Int32).alias(\"departs_from_major_hub\"),\n",
    "        \n",
    "        # International airports\n",
    "        pl.col(\"legs0_segments0_arrivalTo_airport_iata\").is_in([\"LED\", \"SVO\", \"DME\", \"VKO\"])\n",
    "        .cast(pl.Int32).alias(\"arrives_to_major_hub\"),\n",
    "        \n",
    "        # Airport consistency (same departure/arrival airports for round trips)\n",
    "        (pl.col(\"legs0_segments0_departureFrom_airport_iata\") == \n",
    "         pl.col(\"legs1_segments0_arrivalTo_airport_iata\").fill_null(\"\")).cast(pl.Int32).alias(\"consistent_airports\"),\n",
    "    ])\n",
    "\n",
    "if \"legs0_segments0_marketingCarrier_code\" in df.columns:\n",
    "    has_carriers = True\n",
    "    route_features.extend([\n",
    "        # Carrier consistency across legs\n",
    "        (pl.col(\"legs0_segments0_marketingCarrier_code\") == \n",
    "         pl.col(\"legs1_segments0_marketingCarrier_code\").fill_null(\"\")).cast(pl.Int32).alias(\"same_carrier_both_legs\"),\n",
    "        \n",
    "        # Premium carriers (Aeroflot, S7, etc.)\n",
    "        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\", \"DP\"])\n",
    "        .cast(pl.Int32).alias(\"is_premium_carrier\"),\n",
    "        \n",
    "        # Marketing vs Operating carrier alignment\n",
    "        (pl.col(\"legs0_segments0_marketingCarrier_code\") == \n",
    "         pl.col(\"legs0_segments0_operatingCarrier_code\").fill_null(\"\")).cast(pl.Int32).alias(\"aligned_carriers_leg0\"),\n",
    "    ])\n",
    "\n",
    "if route_features:\n",
    "    df = df.with_columns(route_features)\n",
    "\n",
    "# 6. Aircraft and service quality features\n",
    "aircraft_features = []\n",
    "has_aircraft = False\n",
    "\n",
    "if \"legs0_segments0_aircraft_code\" in df.columns:\n",
    "    has_aircraft = True\n",
    "    aircraft_features.extend([\n",
    "        # Wide-body aircraft (better for long flights)\n",
    "        pl.col(\"legs0_segments0_aircraft_code\").is_in([\"330\", \"777\", \"787\", \"320\", \"321\"])\n",
    "        .cast(pl.Int32).alias(\"wide_body_leg0\"),\n",
    "        \n",
    "        # Modern aircraft\n",
    "        pl.col(\"legs0_segments0_aircraft_code\").is_in([\"787\", \"350\", \"320\", \"321\", \"737\"])\n",
    "        .cast(pl.Int32).alias(\"modern_aircraft_leg0\"),\n",
    "    ])\n",
    "\n",
    "if aircraft_features:\n",
    "    df = df.with_columns(aircraft_features)\n",
    "\n",
    "# 7. Cabin class optimization\n",
    "cabin_features = []\n",
    "has_cabin = False\n",
    "\n",
    "if \"legs0_segments0_cabinClass\" in df.columns:\n",
    "    has_cabin = True\n",
    "    cabin_features.extend([\n",
    "        # Business class upgrade availability\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(1) >= 2).cast(pl.Int32).alias(\"business_class_leg0\"),\n",
    "        # Premium economy or better\n",
    "        (pl.col(\"legs0_segments0_cabinClass\").fill_null(1) >= 1.5).cast(pl.Int32).alias(\"premium_class_leg0\"),\n",
    "        # Cabin class consistency\n",
    "        (pl.col(\"legs0_segments0_cabinClass\") == pl.col(\"legs1_segments0_cabinClass\")).cast(pl.Int32).alias(\"consistent_cabin\"),\n",
    "    ])\n",
    "\n",
    "if cabin_features:\n",
    "    df = df.with_columns(cabin_features)\n",
    "\n",
    "# 8. Competitive analysis within group (basic features first)\n",
    "competitive_features = [\n",
    "    # Direct flight premium\n",
    "    pl.col(\"both_direct\").sum().over(\"ranker_id\").alias(\"n_direct_options\"),\n",
    "    \n",
    "    # Premium combinations\n",
    "    ((pl.col(\"both_direct\") == 1) & (pl.col(\"is_cheap_quartile\") == 1)).cast(pl.Int32).alias(\"is_cheap_direct\"),\n",
    "    ((pl.col(\"policy_compliant\") == 1) & (pl.col(\"is_cheap_quartile\") == 1)).cast(pl.Int32).alias(\"compliant_and_cheap\"),\n",
    "    \n",
    "    # Business convenience\n",
    "    ((pl.col(\"legs0_departureAt_hour\").fill_null(12) >= 7) & \n",
    "     (pl.col(\"legs0_departureAt_hour\").fill_null(12) <= 9)).cast(pl.Int32).alias(\"morning_departure_business\"),\n",
    "    \n",
    "    # Fast and convenient\n",
    "    (pl.col(\"total_duration\") <= pl.col(\"total_duration\").quantile(0.3).over(\"ranker_id\")).cast(pl.Int32).alias(\"is_fast_option\"),\n",
    "]\n",
    "\n",
    "df = df.with_columns(competitive_features)\n",
    "\n",
    "# 8b. Advanced competitive features (requires previous features to exist)\n",
    "advanced_competitive = []\n",
    "\n",
    "if has_carriers:\n",
    "    advanced_competitive.extend([\n",
    "        # VIP treatment with premium carriers\n",
    "        ((pl.col(\"is_vip_freq\") == 1) & (pl.col(\"is_premium_carrier\") == 1)).cast(pl.Int32).alias(\"vip_premium_carrier\"),\n",
    "    ])\n",
    "\n",
    "if has_cancellation and has_carriers:\n",
    "    advanced_competitive.extend([\n",
    "        # Flexible and premium\n",
    "        ((pl.col(\"free_cancellation\") == 1) & (pl.col(\"is_premium_carrier\") == 1)).cast(pl.Int32).alias(\"flexible_premium\"),\n",
    "    ])\n",
    "\n",
    "if advanced_competitive:\n",
    "    df = df.with_columns(advanced_competitive)\n",
    "\n",
    "# 9. High-impact interaction features (safe with all previous features defined)\n",
    "interaction_features = [\n",
    "    # Price-policy compliance\n",
    "    (pl.col(\"price_bucket\") * pl.col(\"policy_compliant\")).alias(\"price_policy_interaction\"),\n",
    "    \n",
    "    # Business convenience score\n",
    "    (pl.col(\"morning_departure_business\") * pl.col(\"is_cheap_quartile\") * pl.col(\"both_direct\")).alias(\"business_value_score\"),\n",
    "    \n",
    "    # Hub efficiency (basic version)\n",
    "    (pl.col(\"both_direct\") * pl.col(\"is_fast_option\")).alias(\"hub_efficiency_score\"),\n",
    "    \n",
    "    # Corporate optimization (basic version)\n",
    "    (pl.col(\"policy_compliant\") * pl.col(\"compliant_and_cheap\")).alias(\"corporate_optimization\"),\n",
    "]\n",
    "\n",
    "# Add premium service interaction if features exist\n",
    "if has_carriers and has_cabin:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"is_premium_carrier\") * pl.col(\"business_class_leg0\")).alias(\"premium_service_score\")\n",
    "    )\n",
    "\n",
    "# Add enhanced hub efficiency if airport features exist\n",
    "if has_airports:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"departs_from_major_hub\") * pl.col(\"both_direct\") * pl.col(\"is_fast_option\")).alias(\"enhanced_hub_efficiency_score\")\n",
    "    )\n",
    "\n",
    "# Add enhanced corporate optimization if carrier features exist\n",
    "if has_carriers:\n",
    "    interaction_features.append(\n",
    "        (pl.col(\"policy_compliant\") * pl.col(\"compliant_and_cheap\") * pl.col(\"same_carrier_both_legs\")).alias(\"enhanced_corporate_optimization\")\n",
    "    )\n",
    "\n",
    "df = df.with_columns(interaction_features)\n",
    "\n",
    "print(\"Advanced business traveler features added successfully!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"- Corporate policy compliance and tax efficiency\")\n",
    "print(\"- Advanced time preferences for business travelers\")\n",
    "print(\"- Seat scarcity and availability features\")\n",
    "print(\"- Cancellation/exchange policy flexibility\")\n",
    "print(\"- Premium carrier and aircraft preferences\")\n",
    "print(\"- Hub airport and route optimization\")\n",
    "print(\"- Strategic business value interactions\")\n",
    "print(\"- Safe feature creation with proper dependency ordering\")\n",
    "print(\"- Targeting 0.5+ Kaggle score with business insights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29372aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:30:43.483932Z",
     "iopub.status.busy": "2025-07-13T03:30:43.483660Z",
     "iopub.status.idle": "2025-07-13T03:30:47.579996Z",
     "shell.execute_reply": "2025-07-13T03:30:47.573152Z"
    },
    "papermill": {
     "duration": 4.108218,
     "end_time": "2025-07-13T03:30:47.582681",
     "exception": false,
     "start_time": "2025-07-13T03:30:43.474463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill nulls\n",
    "data = df.with_columns(\n",
    "    [pl.col(c).fill_null(0) for c in df.select(pl.selectors.numeric()).columns] +\n",
    "    [pl.col(c).fill_null(\"missing\") for c in df.select(pl.selectors.string()).columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30728afa",
   "metadata": {
    "papermill": {
     "duration": 0.00577,
     "end_time": "2025-07-13T03:30:47.594866",
     "exception": false,
     "start_time": "2025-07-13T03:30:47.589096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2376e0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:30:47.609638Z",
     "iopub.status.busy": "2025-07-13T03:30:47.609339Z",
     "iopub.status.idle": "2025-07-13T03:30:47.629113Z",
     "shell.execute_reply": "2025-07-13T03:30:47.624200Z"
    },
    "papermill": {
     "duration": 0.03118,
     "end_time": "2025-07-13T03:30:47.631613",
     "exception": false,
     "start_time": "2025-07-13T03:30:47.600433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 180 features (35 categorical)\n",
      "CatBoost categorical indices: 35 features\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "cat_features = [\n",
    "    'nationality', 'searchRoute', 'corporateTariffCode',\n",
    "    'bySelf', 'sex', 'companyID',\n",
    "    # Leg 0 segments 0-1\n",
    "    'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',\n",
    "    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',\n",
    "    'legs0_segments0_flightNumber',\n",
    "    'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',\n",
    "    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',\n",
    "    'legs0_segments1_flightNumber',\n",
    "    # Leg 1 segments 0-1\n",
    "    'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',\n",
    "    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',\n",
    "    'legs1_segments0_flightNumber',\n",
    "    'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',\n",
    "    'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',\n",
    "    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',\n",
    "    'legs1_segments1_flightNumber',\n",
    "    # Only add price_bucket as new categorical\n",
    "    'price_bucket'\n",
    "]\n",
    "\n",
    "# Columns to exclude (uninformative or problematic)\n",
    "exclude_cols = [\n",
    "    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n",
    "    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',\n",
    "    'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing\n",
    "    'frequentFlyer',  # Already processed\n",
    "    # Exclude constant columns\n",
    "    'pricingInfo_passengerCount'\n",
    "]\n",
    "\n",
    "# Exclude segment 2-3 columns (>98% missing)\n",
    "for leg in [0, 1]:\n",
    "    for seg in [2, 3]:\n",
    "        for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',\n",
    "                      'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',\n",
    "                      'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',\n",
    "                      'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:\n",
    "            exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "cat_features_final = [col for col in cat_features if col in feature_cols]\n",
    "\n",
    "# Create CatBoost categorical feature indices (column positions in feature_cols)\n",
    "catboost_cat_indices = [i for i, col in enumerate(feature_cols) if col in cat_features_final]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features ({len(cat_features_final)} categorical)\")\n",
    "print(f\"CatBoost categorical indices: {len(catboost_cat_indices)} features\")\n",
    "\n",
    "X = data.select(feature_cols)\n",
    "y = data.select('selected')\n",
    "groups = data.select('ranker_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab25e6",
   "metadata": {
    "papermill": {
     "duration": 0.005389,
     "end_time": "2025-07-13T03:30:47.642110",
     "exception": false,
     "start_time": "2025-07-13T03:30:47.636721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training and Tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48900e",
   "metadata": {
    "papermill": {
     "duration": 0.006987,
     "end_time": "2025-07-13T03:30:49.490121",
     "exception": false,
     "start_time": "2025-07-13T03:30:49.483134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9064fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:30:49.508087Z",
     "iopub.status.busy": "2025-07-13T03:30:49.507813Z",
     "iopub.status.idle": "2025-07-13T03:31:53.714008Z",
     "shell.execute_reply": "2025-07-13T03:31:53.706750Z"
    },
    "papermill": {
     "duration": 64.219604,
     "end_time": "2025-07-13T03:31:53.716600",
     "exception": false,
     "start_time": "2025-07-13T03:30:49.496996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_xgb = X.with_columns([(pl.col(c).rank(\"dense\") - 1).fill_null(-1).cast(pl.Int32) for c in cat_features_final])\n",
    "\n",
    "n1 = 16487352 # split train to train and val (10%) in time\n",
    "n2 = train.height\n",
    "data_xgb_tr, data_xgb_va, data_xgb_te = data_xgb[:n1], data_xgb[n1:n2], data_xgb[n2:]\n",
    "y_tr, y_va, y_te = y[:n1], y[n1:n2], y[n2:]\n",
    "groups_tr, groups_va, groups_te = groups[:n1], groups[n1:n2], groups[n2:]\n",
    "\n",
    "group_sizes_tr = groups_tr.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_va = groups_va.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "group_sizes_te = groups_te.group_by('ranker_id').agg(pl.len()).sort('ranker_id')['len'].to_numpy()\n",
    "dtrain = xgb.DMatrix(data_xgb_tr, label=y_tr, group=group_sizes_tr, feature_names=data_xgb.columns)\n",
    "dval   = xgb.DMatrix(data_xgb_va, label=y_va, group=group_sizes_va, feature_names=data_xgb.columns)\n",
    "dtest  = xgb.DMatrix(data_xgb_te, label=y_te, group=group_sizes_te, feature_names=data_xgb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d4245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:31:53.735961Z",
     "iopub.status.busy": "2025-07-13T03:31:53.735677Z",
     "iopub.status.idle": "2025-07-13T03:51:51.878660Z",
     "shell.execute_reply": "2025-07-13T03:51:51.872900Z"
    },
    "papermill": {
     "duration": 1198.15652,
     "end_time": "2025-07-13T03:51:51.881101",
     "exception": false,
     "start_time": "2025-07-13T03:31:53.724581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost with v2 business-optimized parameters...\n",
      "V2 optimizations for better convergence:\n",
      "- Reduced depth (8) to prevent slight overfitting\n",
      "- Improved sampling rates for better generalization\n",
      "- Fine-tuned regularization for optimal expressiveness\n",
      "- Adjusted learning rate for faster convergence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@3:0.78564\tval-ndcg@3:0.80987"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-ndcg@3:0.81576\tval-ndcg@3:0.83376"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-ndcg@3:0.82366\tval-ndcg@3:0.83657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttrain-ndcg@3:0.83067\tval-ndcg@3:0.83838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain-ndcg@3:0.83721\tval-ndcg@3:0.83973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttrain-ndcg@3:0.84342\tval-ndcg@3:0.84100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttrain-ndcg@3:0.84874\tval-ndcg@3:0.84202"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\ttrain-ndcg@3:0.85386\tval-ndcg@3:0.84297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttrain-ndcg@3:0.85807\tval-ndcg@3:0.84337"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\ttrain-ndcg@3:0.86272\tval-ndcg@3:0.84341"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain-ndcg@3:0.86614\tval-ndcg@3:0.84329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550]\ttrain-ndcg@3:0.86971\tval-ndcg@3:0.84374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttrain-ndcg@3:0.87300\tval-ndcg@3:0.84386"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650]\ttrain-ndcg@3:0.87562\tval-ndcg@3:0.84408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttrain-ndcg@3:0.87778\tval-ndcg@3:0.84422"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750]\ttrain-ndcg@3:0.88063\tval-ndcg@3:0.84426"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttrain-ndcg@3:0.88340\tval-ndcg@3:0.84430"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\ttrain-ndcg@3:0.88546\tval-ndcg@3:0.84451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\ttrain-ndcg@3:0.88773\tval-ndcg@3:0.84500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950]\ttrain-ndcg@3:0.89052\tval-ndcg@3:0.84522"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain-ndcg@3:0.89354\tval-ndcg@3:0.84484"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1042]\ttrain-ndcg@3:0.89669\tval-ndcg@3:0.84534"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized XGBoost parameters for business traveler features (v3 - Faster Training)\n",
    "final_xgb_params = {\n",
    "    'objective': 'rank:pairwise', \n",
    "    'eval_metric': 'ndcg@3', \n",
    "    'max_depth': 8,              # 保持深度8，平衡性能和速度\n",
    "    'min_child_weight': 10,      # 保持10，良好的泛化\n",
    "    'subsample': 0.92,           # 保持0.92，良好的泛化\n",
    "    'colsample_bytree': 0.9,     # 保持0.9，充分利用特征\n",
    "    'lambda': 3.0,              # 保持3.0，平衡正则化\n",
    "    'alpha': 0.12,              # 保持0.12，良好的稀疏性\n",
    "    'learning_rate': 0.065,     # 从0.055提升到0.065，更快收敛\n",
    "    'gamma': 0.06,              # 保持0.06，适度的分裂要求\n",
    "    'seed': RANDOM_STATE, \n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist',\n",
    "    'grow_policy': 'lossguide'\n",
    "}\n",
    "\n",
    "print(\"\\nTraining XGBoost with v3 efficiency-optimized parameters...\")\n",
    "print(\"V3 efficiency optimizations (保持性能，提升速度):\")\n",
    "print(\"- Increased learning rate (0.065) for faster convergence\")\n",
    "print(\"- Reduced max iterations based on log analysis\")\n",
    "print(\"- Tighter early stopping for efficiency\")\n",
    "print(\"- Targeting same performance with 30% less iterations\")\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    final_xgb_params, dtrain,\n",
    "    num_boost_round=1200,        # 从1600减少到1200 (基于日志1042轮停止)\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    early_stopping_rounds=80,    # 从110减少到80，更激进的停止\n",
    "    verbose_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0ed0e",
   "metadata": {
    "papermill": {
     "duration": 0.00891,
     "end_time": "2025-07-13T03:51:51.899090",
     "exception": false,
     "start_time": "2025-07-13T03:51:51.890180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7abe781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:51:51.920493Z",
     "iopub.status.busy": "2025-07-13T03:51:51.920231Z",
     "iopub.status.idle": "2025-07-13T03:51:51.983233Z",
     "shell.execute_reply": "2025-07-13T03:51:51.977874Z"
    },
    "papermill": {
     "duration": 0.077657,
     "end_time": "2025-07-13T03:51:51.985504",
     "exception": false,
     "start_time": "2025-07-13T03:51:51.907847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LightGBM Datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Datasets created successfully.\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL\n",
    "# LightGBM requires its own Dataset object. We can reuse the rank-encoded data from XGBoost.\n",
    "print(\"Creating LightGBM Datasets...\")\n",
    "lgb_train = lgb.Dataset(\n",
    "    data=data_xgb_tr, \n",
    "    label=y_tr.to_numpy().flatten(), \n",
    "    group=group_sizes_tr,\n",
    "    feature_name=feature_cols,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "lgb_val = lgb.Dataset(\n",
    "    data=data_xgb_va, \n",
    "    label=y_va.to_numpy().flatten(), \n",
    "    group=group_sizes_va,\n",
    "    feature_name=feature_cols,\n",
    "    reference=lgb_train,\n",
    "    free_raw_data=False\n",
    ")\n",
    "print(\"LightGBM Datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b79d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:51:52.007270Z",
     "iopub.status.busy": "2025-07-13T03:51:52.006990Z",
     "iopub.status.idle": "2025-07-13T03:59:47.199077Z",
     "shell.execute_reply": "2025-07-13T03:59:47.191787Z"
    },
    "papermill": {
     "duration": 475.207461,
     "end_time": "2025-07-13T03:59:47.201826",
     "exception": false,
     "start_time": "2025-07-13T03:51:51.994365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM model with v2 business-optimized parameters...\n",
      "V2 business feature optimizations:\n",
      "- Reduced leaves (140) and depth (10) to prevent overfitting\n",
      "- Lower learning rate (0.15) for better stability\n",
      "- Increased regularization (L1=0.006, L2=7.5) for generalization\n",
      "- Conservative sampling for better model robustness\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 130 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttraining's ndcg@3: 0.838298\tvalid_1's ndcg@3: 0.839591"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's ndcg@3: 0.858681\tvalid_1's ndcg@3: 0.841336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttraining's ndcg@3: 0.873883\tvalid_1's ndcg@3: 0.841788"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's ndcg@3: 0.88456\tvalid_1's ndcg@3: 0.842278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\ttraining's ndcg@3: 0.894915\tvalid_1's ndcg@3: 0.841154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's ndcg@3: 0.903706\tvalid_1's ndcg@3: 0.842088"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\ttraining's ndcg@3: 0.911544\tvalid_1's ndcg@3: 0.840891"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's ndcg@3: 0.918839\tvalid_1's ndcg@3: 0.840583"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's ndcg@3: 0.900754\tvalid_1's ndcg@3: 0.842607"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimized LightGBM parameters for business traveler features (v3 - Faster Training)\n",
    "final_lgb_params = {\n",
    "    'objective': 'lambdarank', \n",
    "    'metric': 'ndcg', \n",
    "    'boosting_type': 'gbdt',\n",
    "    'eval_at': [3],\n",
    "    'num_leaves': 140,           # 保持140，良好的模型容量\n",
    "    'learning_rate': 0.18,       # 从0.15提升到0.18，更快收敛\n",
    "    'min_child_samples': 60,     # 保持60，良好的泛化\n",
    "    'lambda_l1': 0.006,          # 保持0.006，适度L1正则化\n",
    "    'lambda_l2': 7.5,            # 保持7.5，良好的L2正则化\n",
    "    'feature_fraction': 0.75,    # 保持0.75，防止过拟合\n",
    "    'bagging_fraction': 0.84,    # 保持0.84，良好的采样\n",
    "    'bagging_freq': 5,           \n",
    "    'min_gain_to_split': 0.005,  # 保持0.005，适度的分裂要求\n",
    "    'max_depth': 10,             # 保持10，控制复杂度\n",
    "    'feature_pre_filter': False, # 修复LightGBM警告\n",
    "    'force_row_wise': True,      \n",
    "    'n_jobs': -1, \n",
    "    'random_state': RANDOM_STATE, \n",
    "    'label_gain': [0, 1],\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "print(\"\\nTraining LightGBM model with v3 efficiency-optimized parameters...\")\n",
    "print(\"V3 efficiency optimizations (保持性能，提升速度):\")\n",
    "print(\"- Increased learning rate (0.18) for faster convergence\")\n",
    "print(\"- Reduced max iterations based on log analysis (282轮停止)\")\n",
    "print(\"- Fixed feature_pre_filter warning\")\n",
    "print(\"- Tighter early stopping for efficiency\")\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    final_lgb_params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,        # 从2000减少到1000 (基于日志282轮停止)\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    callbacks=[lgb.early_stopping(80), lgb.log_evaluation(50)]  # 从130减少到80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa105c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T03:59:47.226974Z",
     "iopub.status.busy": "2025-07-13T03:59:47.226667Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-07-13T03:59:47.212093",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM DART Model ---\n",
      "\n",
      "Training Enhanced LightGBM DART model (v2)...\n",
      "V2 DART optimizations:\n",
      "- Refined learning rate (0.045) for better convergence\n",
      "- Increased capacity (60 leaves) with depth control (12)\n",
      "- Optimized DART dropout balance (0.075 rate, 0.48 skip)\n",
      "- Added regularization for better generalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.\n",
      "You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Reducing `min_data_in_leaf` with `feature_pre_filter=true` may cause unexpected behaviour for features that were pre-filtered by the larger `min_data_in_leaf`.\n",
      "You need to set `feature_pre_filter=false` to dynamically change the `min_data_in_leaf`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning(\"Early stopping is not available in dart mode\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's ndcg@3: 0.826599"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@3: 0.829285"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid_0's ndcg@3: 0.830514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's ndcg@3: 0.831999"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\tvalid_0's ndcg@3: 0.834289"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's ndcg@3: 0.834768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350]\tvalid_0's ndcg@3: 0.835424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\tvalid_0's ndcg@3: 0.836063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\tvalid_0's ndcg@3: 0.837309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's ndcg@3: 0.836952"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550]\tvalid_0's ndcg@3: 0.837339"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\tvalid_0's ndcg@3: 0.838046"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[650]\tvalid_0's ndcg@3: 0.838453"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\tvalid_0's ndcg@3: 0.839325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750]\tvalid_0's ndcg@3: 0.839923"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\tvalid_0's ndcg@3: 0.840273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[850]\tvalid_0's ndcg@3: 0.840311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900]\tvalid_0's ndcg@3: 0.840903"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950]\tvalid_0's ndcg@3: 0.841412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_0's ndcg@3: 0.841571"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050]\tvalid_0's ndcg@3: 0.841679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100]\tvalid_0's ndcg@3: 0.842276"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1150]\tvalid_0's ndcg@3: 0.842249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200]\tvalid_0's ndcg@3: 0.842248"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1250]\tvalid_0's ndcg@3: 0.842623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1300]\tvalid_0's ndcg@3: 0.842513"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1350]\tvalid_0's ndcg@3: 0.842651"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400]\tvalid_0's ndcg@3: 0.842489"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1450]\tvalid_0's ndcg@3: 0.842229"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500]\tvalid_0's ndcg@3: 0.842906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1550]\tvalid_0's ndcg@3: 0.843373"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600]\tvalid_0's ndcg@3: 0.842931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1650]\tvalid_0's ndcg@3: 0.843108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1700]\tvalid_0's ndcg@3: 0.843577"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1750]\tvalid_0's ndcg@3: 0.844182"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800]\tvalid_0's ndcg@3: 0.844068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1850]\tvalid_0's ndcg@3: 0.843944"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1900]\tvalid_0's ndcg@3: 0.843679"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1950]\tvalid_0's ndcg@3: 0.843263"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_0's ndcg@3: 0.843664"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2050]\tvalid_0's ndcg@3: 0.843839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2100]\tvalid_0's ndcg@3: 0.844111"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2150]\tvalid_0's ndcg@3: 0.844119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2200]\tvalid_0's ndcg@3: 0.844405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250]\tvalid_0's ndcg@3: 0.844383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2300]\tvalid_0's ndcg@3: 0.844701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2350]\tvalid_0's ndcg@3: 0.844149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2400]\tvalid_0's ndcg@3: 0.844231\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training LightGBM DART Model (v3 - Efficiency Optimized) ---\")\n",
    "\n",
    "# DART参数优化：基于日志分析大幅减少训练轮数\n",
    "dart_params = {\n",
    "    'objective': 'lambdarank', \n",
    "    'metric': 'ndcg', \n",
    "    'eval_at': [3],\n",
    "    'boosting_type': 'dart', \n",
    "    'n_estimators': 1200,      # 从2500大幅减少到1200 (基于日志分析)\n",
    "    'learning_rate': 0.055,    # 从0.045提升到0.055，更快收敛\n",
    "    'num_leaves': 65,          # 从60增加到65，稍微增加容量\n",
    "    'drop_rate': 0.08,         # 从0.075增加到0.08，更好的正则化\n",
    "    'subsample': 0.87,         # 从0.85增加到0.87，更好的采样\n",
    "    'skip_drop': 0.45,         # 从0.48减少到0.45，更多dropout\n",
    "    'max_depth': 12,           # 保持12，控制复杂度\n",
    "    'min_child_samples': 45,   # 保持45，防止过拟合\n",
    "    'lambda_l1': 0.002,        # 从0.001增加到0.002，更多L1正则化\n",
    "    'lambda_l2': 3.0,          # 从2.5增加到3.0，更多L2正则化\n",
    "    'feature_pre_filter': False, # 修复LightGBM警告\n",
    "    'n_jobs': -1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'label_gain': [0, 1]\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Enhanced LightGBM DART model (v3 - 效率优化)...\")\n",
    "print(\"V3 DART efficiency optimizations (保持性能，大幅提升速度):\")\n",
    "print(\"- Reduced iterations from 2500 to 1200 (基于日志2400+轮分析)\")\n",
    "print(\"- Increased learning rate (0.055) for faster convergence\") \n",
    "print(\"- Enhanced regularization to compensate for fewer iterations\")\n",
    "print(\"- Fixed feature_pre_filter warning\")\n",
    "print(\"- Target: Same performance with 50% less training time\")\n",
    "\n",
    "lgb_model_dart = lgb.train(\n",
    "    dart_params,\n",
    "    lgb_train, \n",
    "    num_boost_round=dart_params['n_estimators'], \n",
    "    valid_sets=[lgb_val],\n",
    "    callbacks=[lgb.log_evaluation(50)]  # DART不支持early stopping，但减少了总轮数\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 LightGBM models training completed successfully!\")\n",
    "print(\"Note: CatBoost training moved to dedicated section below for proper data handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ee08c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Blending and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366a0f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:48:17.179705Z",
     "iopub.status.busy": "2025-07-11T08:48:17.179443Z",
     "iopub.status.idle": "2025-07-11T08:48:42.113109Z",
     "shell.execute_reply": "2025-07-11T08:48:42.106978Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Training CatBoost Model (CORRECTED v3) ---\")\n",
    "print(\"Training CatBoost with FIXED DataFrame approach and v3 efficiency optimizations...\")\n",
    "\n",
    "# Prepare data for CatBoost (needs specific format)\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "# CRITICAL FIX: Use DataFrame approach instead of numpy arrays for mixed types\n",
    "# The problem: numpy arrays can't handle mixed int/float types properly for CatBoost\n",
    "# Solution: Use pandas DataFrames which preserve column-specific data types\n",
    "\n",
    "print(\"Preparing CatBoost data with FIXED DataFrame approach...\")\n",
    "\n",
    "# Step 1: Get data splits using the same indices as other models\n",
    "X_catboost_tr = X[:n1].clone()  # Training split\n",
    "X_catboost_va = X[n1:n2].clone()  # Validation split \n",
    "X_catboost_te = X[n2:].clone()  # Test split\n",
    "\n",
    "print(f\"CatBoost data splits: Train={len(X_catboost_tr)}, Val={len(X_catboost_va)}, Test={len(X_catboost_te)}\")\n",
    "\n",
    "# Step 2: CRITICAL FIX - Apply proper categorical encoding\n",
    "# Convert categorical features to integers using a unified approach\n",
    "print(f\"Converting {len(cat_features_final)} categorical features to integers...\")\n",
    "\n",
    "# Create a unified string-to-integer mapping for each categorical feature\n",
    "for col in cat_features_final:\n",
    "    print(f\"   Processing categorical feature: {col}\")\n",
    "    \n",
    "    # Combine all data to create consistent encoding across splits\n",
    "    all_values = pl.concat([\n",
    "        X_catboost_tr.select(col),\n",
    "        X_catboost_va.select(col), \n",
    "        X_catboost_te.select(col)\n",
    "    ]).unique().sort(col)\n",
    "    \n",
    "    # Create mapping: unique values -> integers (0, 1, 2, ...)\n",
    "    mapping_dict = {\n",
    "        val: idx for idx, val in enumerate(all_values[col].to_list())\n",
    "    }\n",
    "    \n",
    "    # Apply mapping to all splits consistently\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_va = X_catboost_va.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "    X_catboost_te = X_catboost_te.with_columns(\n",
    "        pl.col(col).map_elements(lambda x: mapping_dict.get(x, -1), return_dtype=pl.Int32).alias(col)\n",
    "    )\n",
    "\n",
    "# Step 3: Convert numerical features to float32 for memory efficiency\n",
    "non_cat_features = [col for col in feature_cols if col not in cat_features_final]\n",
    "print(f\"Converting {len(non_cat_features)} numerical features to float32...\")\n",
    "\n",
    "for col in non_cat_features:\n",
    "    X_catboost_tr = X_catboost_tr.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_va = X_catboost_va.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "    X_catboost_te = X_catboost_te.with_columns(pl.col(col).cast(pl.Float32).alias(col))\n",
    "\n",
    "print(\"CatBoost data preparation completed with FIXED data types\")\n",
    "\n",
    "# Step 4: Convert to pandas DataFrames (CRITICAL FIX for mixed types)\n",
    "print(\"Converting to pandas DataFrames for CatBoost compatibility...\")\n",
    "\n",
    "X_catboost_tr_df = X_catboost_tr.to_pandas()\n",
    "X_catboost_va_df = X_catboost_va.to_pandas()\n",
    "X_catboost_te_df = X_catboost_te.to_pandas()\n",
    "\n",
    "# Verify data types before Pool creation\n",
    "print(\"Verifying data types before Pool creation...\")\n",
    "print(f\"   Training data shape: {X_catboost_tr_df.shape}\")\n",
    "print(f\"   Training data memory usage: {X_catboost_tr_df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Check categorical columns specifically\n",
    "for i, cat_idx in enumerate(catboost_cat_indices[:3]):\n",
    "    col_name = X_catboost_tr_df.columns[cat_idx]\n",
    "    col_data = X_catboost_tr_df.iloc[:, cat_idx]\n",
    "    print(f\"   Cat feature {i} (col {cat_idx}, '{col_name}'): dtype={col_data.dtype}, sample={col_data.head(3).tolist()}\")\n",
    "\n",
    "# Step 5: Create CatBoost pools with properly typed DataFrames\n",
    "print(\"Creating CatBoost Pool objects with DataFrames...\")\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_catboost_tr_df,  # ✅ Using DataFrame instead of numpy array\n",
    "    label=y_tr.to_numpy().flatten(),\n",
    "    group_id=groups_tr.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=X_catboost_va_df,  # ✅ Using DataFrame instead of numpy array\n",
    "    label=y_va.to_numpy().flatten(),\n",
    "    group_id=groups_va.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "print(\"CatBoost Pool objects created successfully!\")\n",
    "print(f\"   Training pool: {train_pool.num_row()} rows x {train_pool.num_col()} cols\")\n",
    "print(f\"   Validation pool: {val_pool.num_row()} rows x {val_pool.num_col()} cols\")\n",
    "print(f\"   Categorical features: {len(catboost_cat_indices)} indices\")\n",
    "\n",
    "# Step 6: Train CatBoost model with v3 efficiency-optimized parameters\n",
    "print(\"Training CatBoost model with v3 efficiency optimizations...\")\n",
    "\n",
    "# CatBoost parameters optimized for business features (v3 - Efficiency)\n",
    "catboost_params = {\n",
    "    'loss_function': 'YetiRank',\n",
    "    'custom_metric': ['NDCG:top=3'],\n",
    "    'iterations': 1500,          # 从2000减少到1500，基于其他模型收敛分析\n",
    "    'learning_rate': 0.15,       # 从0.12提升到0.15，更快收敛\n",
    "    'depth': 8,                  # 保持8，良好的深度\n",
    "    'l2_leaf_reg': 4.0,          # 保持4.0，适度正则化\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'bagging_temperature': 0.8,\n",
    "    'subsample': 0.85,\n",
    "    'random_strength': 0.8,\n",
    "    'one_hot_max_size': 10,\n",
    "    'max_ctr_complexity': 3,\n",
    "    'random_seed': RANDOM_STATE,\n",
    "    'thread_count': -1,\n",
    "    'verbose': 50\n",
    "}\n",
    "\n",
    "print(\"V3 CatBoost efficiency optimizations:\")\n",
    "print(\"- Reduced iterations from 2000 to 1500\")\n",
    "print(\"- Increased learning rate (0.15) for faster convergence\")\n",
    "print(\"- Tighter early stopping for efficiency\")\n",
    "print(\"- FIXED DataFrame approach prevents data type errors\")\n",
    "print(\"- Target: Same performance with 25% less training time\")\n",
    "\n",
    "catboost_model = CatBoostRanker(**catboost_params)\n",
    "catboost_model.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    early_stopping_rounds=80,    # 从100减少到80，更激进的停止\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"CatBoost model trained successfully with FIXED DataFrame approach!\")\n",
    "print(\"All models training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9a36a",
   "metadata": {},
   "source": [
    "### 🔧 CatBoost Data Type Fix\n",
    "\n",
    "**问题**: CatBoost在使用numpy数组时，所有数据会被转换为float64，导致分类特征类型冲突错误：\n",
    "```\n",
    "CatBoostError: 'data' is numpy array of floating point numerical type, \n",
    "but 'cat_features' parameter specifies nonzero number of categorical features\n",
    "```\n",
    "\n",
    "**解决方案**: \n",
    "1. ✅ 使用pandas DataFrame而不是numpy数组\n",
    "2. ✅ 确保分类特征保持int32类型\n",
    "3. ✅ 数值特征使用float32类型\n",
    "4. ✅ DataFrame能正确处理混合数据类型\n",
    "\n",
    "**注意**: 这个修复确保CatBoost能正确识别和处理分类特征，避免训练时的类型错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a6b13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf1a61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation and Ensemble Optimization (Required Variables)\n",
    "print(\"Setting up ensemble strategies...\")\n",
    "\n",
    "# Get validation predictions for ensemble optimization\n",
    "xgb_val_preds = xgb_model.predict(dval)\n",
    "lgb_gbdt_val_preds = lgb_model.predict(data_xgb_va)\n",
    "lgb_dart_val_preds = lgb_model_dart.predict(data_xgb_va)\n",
    "\n",
    "# CatBoost validation predictions\n",
    "catboost_val_preds = catboost_model.predict(val_pool)\n",
    "\n",
    "# Calculate individual model performance on validation set\n",
    "val_hitrates = {}\n",
    "val_hitrates['XGBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), xgb_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['LightGBM_GBDT'] = hitrate_at_3(y_va.to_numpy().flatten(), lgb_gbdt_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['LightGBM_DART'] = hitrate_at_3(y_va.to_numpy().flatten(), lgb_dart_val_preds, groups_va.to_numpy().flatten())\n",
    "val_hitrates['CatBoost'] = hitrate_at_3(y_va.to_numpy().flatten(), catboost_val_preds, groups_va.to_numpy().flatten())\n",
    "\n",
    "print(\"Individual model validation performance:\")\n",
    "for model, hr in val_hitrates.items():\n",
    "    print(f\"  {model}: {hr:.4f}\")\n",
    "\n",
    "# Define ensemble strategies\n",
    "strategies = {\n",
    "    \"Static Balanced\": np.array([0.35, 0.25, 0.10, 0.30]),  # XGB, LGB-GBDT, LGB-DART, CatBoost\n",
    "    \"DART Focused\": np.array([0.25, 0.25, 0.35, 0.15]),\n",
    "    \"Performance Weighted\": np.array([0.4, 0.3, 0.1, 0.2]),\n",
    "    \"Static XGBoost Focused\": np.array([0.45, 0.15, 0.05, 0.35])\n",
    "}\n",
    "\n",
    "# Test strategies on validation data\n",
    "val_submission_df = data_xgb_va.select(['ranker_id']).with_columns([\n",
    "    pl.Series('xgb_score', xgb_val_preds),\n",
    "    pl.Series('lgb_gbdt_score', lgb_gbdt_val_preds),\n",
    "    pl.Series('lgb_dart_score', lgb_dart_val_preds),\n",
    "    pl.Series('catboost_score', catboost_val_preds)\n",
    "])\n",
    "\n",
    "# Convert scores to ranks\n",
    "val_submission_df = val_submission_df.with_columns([\n",
    "    pl.col(\"xgb_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"xgb_rank\"),\n",
    "    pl.col(\"lgb_gbdt_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_gbdt_rank\"),\n",
    "    pl.col(\"lgb_dart_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_dart_rank\"),\n",
    "    pl.col(\"catboost_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"catboost_rank\")\n",
    "])\n",
    "\n",
    "# Test ensemble strategies\n",
    "best_strategy_hr3 = 0\n",
    "best_strategy_name = \"Static Balanced\"\n",
    "strategy_results = {}\n",
    "\n",
    "for strategy_name, weights in strategies.items():\n",
    "    # Create ensemble\n",
    "    ensemble_score = (weights[0] * val_submission_df.get_column(\"xgb_rank\") + \n",
    "                     weights[1] * val_submission_df.get_column(\"lgb_gbdt_rank\") + \n",
    "                     weights[2] * val_submission_df.get_column(\"lgb_dart_rank\") + \n",
    "                     weights[3] * val_submission_df.get_column(\"catboost_rank\"))\n",
    "    \n",
    "    # Get best option per group (lowest ensemble rank)\n",
    "    val_ensemble_df = val_submission_df.with_columns([\n",
    "        pl.Series(\"ensemble_rank\", ensemble_score)\n",
    "    ])\n",
    "    \n",
    "    # Rank ensemble scores within each group\n",
    "    val_ensemble_df = val_ensemble_df.with_columns([\n",
    "        pl.col(\"ensemble_rank\").rank(method=\"ordinal\", descending=False).over(\"ranker_id\").alias(\"final_rank\")\n",
    "    ])\n",
    "    \n",
    "    # Select top option per group\n",
    "    selected = val_ensemble_df.filter(pl.col(\"final_rank\") == 1)\n",
    "    \n",
    "    # Create predictions (1 for selected, 0 for others)\n",
    "    val_preds = np.zeros(len(val_ensemble_df))\n",
    "    selected_indices = selected.select(pl.int_range(pl.len()).over(\"ranker_id\")).to_numpy().flatten()\n",
    "    # val_preds[selected_indices] = 1\n",
    "    \n",
    "    # Calculate hit rate using ensemble ranking\n",
    "    val_ensemble_preds = val_ensemble_df.get_column(\"ensemble_rank\").to_numpy()\n",
    "    strategy_hr3 = hitrate_at_3(y_va.to_numpy().flatten(), -val_ensemble_preds, groups_va.to_numpy().flatten())\n",
    "    \n",
    "    strategy_results[strategy_name] = strategy_hr3\n",
    "    print(f\"Strategy '{strategy_name}': {strategy_hr3:.4f}\")\n",
    "    \n",
    "    if strategy_hr3 > best_strategy_hr3:\n",
    "        best_strategy_hr3 = strategy_hr3\n",
    "        best_strategy_name = strategy_name\n",
    "\n",
    "# Set the best strategy weights\n",
    "if best_strategy_name in strategies:\n",
    "    optimized_weights = strategies[best_strategy_name]\n",
    "    dart_focused_weights = strategies.get(\"DART Focused\", strategies[\"Static Balanced\"])\n",
    "    performance_weights = strategies.get(\"Performance Weighted\", strategies[\"Static Balanced\"])\n",
    "    balanced_weights = strategies[\"Static Balanced\"]\n",
    "else:\n",
    "    optimized_weights = strategies[\"Static Balanced\"]\n",
    "    dart_focused_weights = strategies[\"Static Balanced\"]\n",
    "    performance_weights = strategies[\"Static Balanced\"]\n",
    "    balanced_weights = strategies[\"Static Balanced\"]\n",
    "\n",
    "print(f\"\\nBest strategy: {best_strategy_name} (HR@3: {best_strategy_hr3:.4f})\")\n",
    "print(\"Ensemble optimization completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a55a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T08:48:42.161517Z",
     "iopub.status.busy": "2025-07-11T08:48:42.161280Z",
     "iopub.status.idle": "2025-07-11T08:50:16.272461Z",
     "shell.execute_reply": "2025-07-11T08:50:16.265939Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "# XGBoost test predictions\n",
    "xgb_test_preds = xgb_model.predict(dtest)\n",
    "\n",
    "# LightGBM test predictions  \n",
    "lgb_gbdt_test_preds = lgb_model.predict(data_xgb_te)\n",
    "lgb_dart_test_preds = lgb_model_dart.predict(data_xgb_te)\n",
    "\n",
    "# CatBoost test predictions with FIXED DataFrame approach\n",
    "print(\"Preparing CatBoost test predictions with FIXED DataFrame approach...\")\n",
    "\n",
    "# CatBoost test data is already properly encoded in the training cell\n",
    "# X_catboost_te was processed with the same categorical encoding pipeline\n",
    "# CRITICAL FIX: Use DataFrame instead of numpy array for test pool\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_catboost_te_df,  # Use the DataFrame version from training cell\n",
    "    group_id=groups_te.to_numpy().flatten(),\n",
    "    cat_features=catboost_cat_indices\n",
    ")\n",
    "\n",
    "print(\"CatBoost test pool created successfully with DataFrame approach\")\n",
    "catboost_test_preds = catboost_model.predict(test_pool)\n",
    "print(\"CatBoost test predictions generated successfully\")\n",
    "\n",
    "# Create comprehensive submission dataframe with all models and strategies\n",
    "submission_df = test.select(['Id', 'ranker_id']).with_columns([\n",
    "    pl.Series('xgb_score', xgb_test_preds),\n",
    "    pl.Series('lgb_gbdt_score', lgb_gbdt_test_preds),\n",
    "    pl.Series('lgb_dart_score', lgb_dart_test_preds),\n",
    "    pl.Series('catboost_score', catboost_test_preds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281420ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model scores to ranks within each group (Required for submission)\n",
    "print(\"Converting model scores to ranks for ensemble strategies...\")\n",
    "\n",
    "submission_df = submission_df.with_columns([\n",
    "    pl.col(\"xgb_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"xgb_rank\"),\n",
    "    pl.col(\"lgb_gbdt_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_gbdt_rank\"),\n",
    "    pl.col(\"lgb_dart_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"lgb_dart_rank\"),\n",
    "    pl.col(\"catboost_score\").rank(method=\"average\", descending=True).over(\"ranker_id\").alias(\"catboost_rank\")\n",
    "])\n",
    "\n",
    "# Apply best ensemble strategy from validation\n",
    "print(f\"Applying best ensemble strategy: {best_strategy_name}\")\n",
    "ensemble_rank = (optimized_weights[0] * submission_df.get_column(\"xgb_rank\") + \n",
    "                optimized_weights[1] * submission_df.get_column(\"lgb_gbdt_rank\") + \n",
    "                optimized_weights[2] * submission_df.get_column(\"lgb_dart_rank\") + \n",
    "                optimized_weights[3] * submission_df.get_column(\"catboost_rank\"))\n",
    "\n",
    "# Create final ranking within each group (1 = best, 2 = second best, etc.)\n",
    "submission_df = submission_df.with_columns([\n",
    "    pl.Series(\"ensemble_rank\", ensemble_rank)\n",
    "]).with_columns([\n",
    "    pl.col(\"ensemble_rank\").rank(method=\"ordinal\", descending=False).over(\"ranker_id\").alias(\"selected\")\n",
    "])\n",
    "\n",
    "# Create final submission with required format: Id, ranker_id, selected\n",
    "final_submission = submission_df.select(['Id', 'ranker_id', 'selected'])\n",
    "\n",
    "print(\"Final submission format validation...\")\n",
    "print(f\"Submission shape: {final_submission.shape}\")\n",
    "print(f\"Unique ranker_ids: {final_submission['ranker_id'].n_unique()}\")\n",
    "print(f\"Rank range: {final_submission['selected'].min()} to {final_submission['selected'].max()}\")\n",
    "\n",
    "# Validate submission format (Critical for competition)\n",
    "validation_results = final_submission.group_by('ranker_id').agg([\n",
    "    pl.col('selected').min().alias('min_rank'),\n",
    "    pl.col('selected').max().alias('max_rank'),\n",
    "    pl.col('selected').n_unique().alias('unique_ranks'),\n",
    "    pl.col('selected').len().alias('group_size')\n",
    "])\n",
    "\n",
    "# Check if all groups have valid permutations (1, 2, 3, ..., N)\n",
    "invalid_groups = validation_results.filter(\n",
    "    (pl.col('min_rank') != 1) | \n",
    "    (pl.col('max_rank') != pl.col('group_size')) |\n",
    "    (pl.col('unique_ranks') != pl.col('group_size'))\n",
    ")\n",
    "\n",
    "if len(invalid_groups) > 0:\n",
    "    print(f\"⚠️  WARNING: Found {len(invalid_groups)} groups with invalid rankings!\")\n",
    "    print(\"Sample invalid groups:\")\n",
    "    print(invalid_groups.head())\n",
    "else:\n",
    "    print(\"✅ All groups have valid rank permutations (1, 2, 3, ..., N)\")\n",
    "\n",
    "# Display sample submission\n",
    "print(\"\\nSample submission format:\")\n",
    "print(final_submission.head(10))\n",
    "\n",
    "# Verify submission requirements\n",
    "print(\"\\n=== SUBMISSION FORMAT VALIDATION ===\")\n",
    "print(f\"✅ Required columns: {list(final_submission.columns) == ['Id', 'ranker_id', 'selected']}\")\n",
    "print(f\"✅ All ranks are integers: {final_submission['selected'].dtype in [pl.Int32, pl.Int64]}\")\n",
    "print(f\"✅ All ranks ≥ 1: {final_submission['selected'].min() >= 1}\")\n",
    "print(f\"✅ Total rows: {len(final_submission):,}\")\n",
    "print(f\"✅ Row order preserved: Test set order maintained\")\n",
    "\n",
    "# Save submission file\n",
    "submission_file = \"submission.csv\"\n",
    "final_submission.write_csv(submission_file)\n",
    "print(f\"\\n🎯 Submission saved to: {submission_file}\")\n",
    "print(\"Ready for Kaggle submission!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE SUBMISSION VALIDATION (Critical for competition success)\n",
    "print(\"=== COMPREHENSIVE SUBMISSION VALIDATION ===\")\n",
    "\n",
    "# 1. Load and verify test set order preservation\n",
    "print(\"1. Verifying row order preservation...\")\n",
    "test_ids = test.select('Id').to_numpy().flatten()\n",
    "submission_ids = final_submission.select('Id').to_numpy().flatten()\n",
    "order_preserved = np.array_equal(test_ids, submission_ids)\n",
    "print(f\"   ✅ Row order preserved: {order_preserved}\")\n",
    "if not order_preserved:\n",
    "    print(\"   ❌ CRITICAL ERROR: Row order not preserved!\")\n",
    "    print(f\"   Test IDs: {test_ids[:5]}...\")\n",
    "    print(f\"   Submission IDs: {submission_ids[:5]}...\")\n",
    "\n",
    "# 2. Detailed rank validation per group\n",
    "print(\"2. Validating rank permutations...\")\n",
    "rank_validation = final_submission.group_by('ranker_id').agg([\n",
    "    pl.col('selected').sort().alias('sorted_ranks'),\n",
    "    pl.col('selected').len().alias('n_options')\n",
    "]).with_columns([\n",
    "    pl.col('sorted_ranks').list.eval(pl.int_range(1, pl.col('n_options') + 1)).alias('expected_ranks'),\n",
    "    (pl.col('sorted_ranks').list.eval(pl.int_range(1, pl.col('n_options') + 1)) == pl.col('sorted_ranks')).alias('valid_permutation')\n",
    "])\n",
    "\n",
    "invalid_count = rank_validation.filter(~pl.col('valid_permutation')).height\n",
    "print(f\"   ✅ Valid permutations: {rank_validation.height - invalid_count}/{rank_validation.height}\")\n",
    "if invalid_count > 0:\n",
    "    print(f\"   ❌ Invalid permutations found: {invalid_count}\")\n",
    "    invalid_sample = rank_validation.filter(~pl.col('valid_permutation')).head(3)\n",
    "    print(\"   Sample invalid groups:\")\n",
    "    print(invalid_sample)\n",
    "\n",
    "# 3. Check for duplicate ranks within groups\n",
    "print(\"3. Checking for duplicate ranks...\")\n",
    "duplicate_check = final_submission.group_by('ranker_id').agg([\n",
    "    pl.col('selected').len().alias('total_ranks'),\n",
    "    pl.col('selected').n_unique().alias('unique_ranks')\n",
    "]).with_columns([\n",
    "    (pl.col('total_ranks') == pl.col('unique_ranks')).alias('no_duplicates')\n",
    "])\n",
    "\n",
    "duplicates_found = duplicate_check.filter(~pl.col('no_duplicates')).height\n",
    "print(f\"   ✅ No duplicate ranks: {duplicates_found == 0}\")\n",
    "if duplicates_found > 0:\n",
    "    print(f\"   ❌ Groups with duplicate ranks: {duplicates_found}\")\n",
    "\n",
    "# 4. Sample group examination\n",
    "print(\"4. Examining sample groups...\")\n",
    "sample_groups = final_submission.group_by('ranker_id').agg([\n",
    "    pl.col('Id').alias('ids'),\n",
    "    pl.col('selected').alias('ranks')\n",
    "]).head(3)\n",
    "\n",
    "for i, row in enumerate(sample_groups.iter_rows()):\n",
    "    ranker_id, ids, ranks = row\n",
    "    print(f\"   Group {i+1} (ranker_id: {ranker_id}):\")\n",
    "    print(f\"     IDs: {ids}\")\n",
    "    print(f\"     Ranks: {ranks}\")\n",
    "    print(f\"     Valid: {sorted(ranks) == list(range(1, len(ranks) + 1))}\")\n",
    "\n",
    "# 5. Final submission statistics\n",
    "print(\"5. Final submission statistics...\")\n",
    "submission_stats = final_submission.select([\n",
    "    pl.col('ranker_id').n_unique().alias('unique_groups'),\n",
    "    pl.col('Id').len().alias('total_rows'),\n",
    "    pl.col('selected').min().alias('min_rank'),\n",
    "    pl.col('selected').max().alias('max_rank'),\n",
    "    pl.col('selected').mean().alias('avg_rank')\n",
    "])\n",
    "\n",
    "print(f\"   Total groups: {submission_stats['unique_groups'].item():,}\")\n",
    "print(f\"   Total rows: {submission_stats['total_rows'].item():,}\")\n",
    "print(f\"   Rank range: {submission_stats['min_rank'].item()} to {submission_stats['max_rank'].item()}\")\n",
    "print(f\"   Average rank: {submission_stats['avg_rank'].item():.2f}\")\n",
    "\n",
    "# 6. Competition format compliance check\n",
    "print(\"6. Competition format compliance...\")\n",
    "required_columns = ['Id', 'ranker_id', 'selected']\n",
    "actual_columns = final_submission.columns\n",
    "format_compliant = (actual_columns == required_columns)\n",
    "print(f\"   ✅ Required columns {required_columns}: {format_compliant}\")\n",
    "print(f\"   ✅ Integer ranks: {final_submission['selected'].dtype in [pl.Int32, pl.Int64]}\")\n",
    "print(f\"   ✅ No missing values: {final_submission.null_count().sum_horizontal().item() == 0}\")\n",
    "\n",
    "# 7. Save verification\n",
    "import os\n",
    "if os.path.exists(submission_file):\n",
    "    file_size = os.path.getsize(submission_file) / 1024 / 1024  # MB\n",
    "    print(f\"   ✅ File saved: {submission_file} ({file_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"   ❌ File not found: {submission_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if order_preserved and invalid_count == 0 and duplicates_found == 0 and format_compliant:\n",
    "    print(\"🎉 SUBMISSION VALIDATION PASSED!\")\n",
    "    print(\"✅ Your submission meets all competition requirements\")\n",
    "    print(\"✅ Ready for Kaggle upload\")\n",
    "else:\n",
    "    print(\"⚠️  SUBMISSION VALIDATION FAILED!\")\n",
    "    print(\"❌ Please fix the issues above before submitting\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 12733338,
     "sourceId": 105399,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-13T03:28:23.905486",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
